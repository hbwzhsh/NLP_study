{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, concatenate\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# read data\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 单独保存comment_text\n",
    "train_text = train['comment_text'].str.lower()\n",
    "test_text = test['comment_text'].str.lower()\n",
    "# 获得y_train\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# 连接所有文字用于分词\n",
    "all_text = pd.concat([train_text, test_text], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0\n",
       "5      0             0        0       0       0              0\n",
       "6      1             1        1       0       1              0\n",
       "7      0             0        0       0       0              0\n",
       "8      0             0        0       0       0              0\n",
       "9      0             0        0       0       0              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据可视化分析\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入nltk预处理数据\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清理数据!\n",
      "完成清理数据!\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# all_text = all_text[:100]\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "def text_cleaned_process(text_raw):    \n",
    "    text_raw = str(text_raw)\n",
    "    text_raw = str(text_raw.lower())\n",
    "    text_raw = re.sub(r'[^a-zA-Z]', ' ', text_raw)\n",
    "    \n",
    "    words = text_raw.split()\n",
    "    \n",
    "    # 移除长度小于3的词语\n",
    "    words2 = []\n",
    "    for i in words:\n",
    "        if len(i) >= 0:\n",
    "            words2.append(i)\n",
    "    # 去停止词\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    result_text = []\n",
    "    result_text = \" \".join([w for w in words2 if not w in stops])\n",
    "    \n",
    "    return(\" \".join(lemmatize_all(result_text)))\n",
    "\n",
    "# 去掉数字\n",
    "all_text.replace({r'[^\\x00-\\x7F]+':''},regex=True,inplace=True)\n",
    "\n",
    "num_all_text = all_text.size\n",
    "\n",
    "# 输出清洗后的数据\n",
    "all_text_cleaned = []\n",
    "\n",
    "print(\"开始清理数据!\")\n",
    "for i in range(0, num_all_text):\n",
    "    all_text_cleaned.append(text_cleaned_process(all_text[i]))\n",
    "\n",
    "print(\"完成清理数据!\")\n",
    "# 构建pd形式\n",
    "all_text_cleaned = pd.Series(all_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of all_text_cleaned: 312735\n",
      "Text[0] before cleaned: \n",
      " explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n",
      "Text[0] after cleaned: \n",
      " explanation edits make username hardcore metallica fan revert vandalisms closure gas vote new york doll fac please remove template talk page since retire\n"
     ]
    }
   ],
   "source": [
    "# 原始数据可视化分析\n",
    "# all_text = all_text[:100]\n",
    "print(\"Len of all_text_cleaned:\", len(all_text_cleaned))\n",
    "# 数据清理前后对比\n",
    "print(\"Text[0] before cleaned: \\n\", all_text[0])\n",
    "print(\"Text[0] after cleaned: \\n\", all_text_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 278517\n"
     ]
    }
   ],
   "source": [
    "# 数据分词处理\n",
    "MAX_NUM_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# 分词\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(all_text_cleaned)\n",
    "sequences = tokenizer.texts_to_sequences(all_text_cleaned)\n",
    "\n",
    "# 分词完成\n",
    "# Pads sequences to the same length， return(len(sequence, maxlen))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# A dictionary of words and their uniquely assigned integers\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "\n",
    "# summarize what was learned\n",
    "# print(tokenizer.word_counts) # A dictionary of words and their counts\n",
    "# print(tokenizer.document_count) # A dictionary of words and how many documents each appeared in.\n",
    "# print(tokenizer.word_docs) # An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of x_train:  159571\n",
      "Len of y_train:  159571\n"
     ]
    }
   ],
   "source": [
    "# 重塑train与test数据\n",
    "x_train = data[:len(train_text)]\n",
    "x_test = data[len(train_text):]\n",
    "\n",
    "print(\"Len of x_train: \", len(x_train))\n",
    "print(\"Len of y_train: \", len(y_train))\n",
    "\n",
    "# 拆分train数据，降低验证集数据\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.05, random_state = 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1999996 word vectors in pretrain data.\n"
     ]
    }
   ],
   "source": [
    "# 加入预训练词\n",
    "EMBEDDING_FILE = 'words_vector/crawl-300d-2M.vec'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "print('Total %s word vectors in pretrain data.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 载入预训练词向量作为Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建CNN模型\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype = 'int32')\n",
    "# 加入词向量\n",
    "embedded_sequence = embedding_layer(sequence_input)\n",
    "\n",
    "x = SpatialDropout1D(0.2)(embedded_sequence)\n",
    "\n",
    "# model0\n",
    "x_0 = Bidirectional(LSTM(128, return_sequences = True, dropout=0.2, recurrent_dropout=0.1))(x)\n",
    "x_0 = GlobalAveragePooling1D()(x_0)\n",
    "\n",
    "# model1\n",
    "x_1 = Bidirectional(LSTM(128, return_sequences = True, dropout=0.3, recurrent_dropout=0.1))(x)\n",
    "x_1 = GlobalMaxPooling1D()(x_1)\n",
    "\n",
    "# 模型融合\n",
    "x_sum = concatenate([x_0, x_1], axis = 1)\n",
    "x_sum = Dense(50, activation=\"relu\")(x_sum)\n",
    "x_sum = Dropout(0.1)(x_sum)\n",
    "\n",
    "# 压缩成对应6个标签\n",
    "#conv_sum = Flatten()(conv_sum)\n",
    "preds = Dense(6, activation = \"sigmoid\")(x_sum)\n",
    "\n",
    "# 生成模型\n",
    "model = Model(inputs = sequence_input, outputs = preds)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "# model.save('LSTM_CNN_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/5\n",
      " - 858s - loss: 0.0712 - acc: 0.9771 - val_loss: 0.0512 - val_acc: 0.9804\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.979072\n",
      "Epoch 2/5\n",
      " - 851s - loss: 0.0485 - acc: 0.9819 - val_loss: 0.0478 - val_acc: 0.9808\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.984053\n",
      "Epoch 3/5\n",
      " - 851s - loss: 0.0450 - acc: 0.9827 - val_loss: 0.0464 - val_acc: 0.9810\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985836\n",
      "Epoch 4/5\n",
      " - 851s - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0458 - val_acc: 0.9816\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.986399\n",
      "Epoch 5/5\n",
      " - 851s - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0447 - val_acc: 0.9820\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.987048\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "# 模型加载\n",
    "# model = load_model('CNN_GRU_3.h5')\n",
    "# 回调函数查看训练得分\n",
    "ROC_AUC = RocAucEvaluation(validation_data = (x_val, y_val), interval = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size = 256, epochs = 5, validation_data = (x_val, y_val), verbose = 2, \n",
    "                   callbacks = [ROC_AUC])\n",
    "\n",
    "# 计算验证集数据得分\n",
    "# y_val_pred = model.predict(x_val, verbose = 2)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(\"Validation data ROC-AUC score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 134s 872us/step\n"
     ]
    }
   ],
   "source": [
    "# test数据预测值\n",
    "y_prediction = model.predict(x_test, batch_size =1024, verbose = 1)\n",
    "\n",
    "# 生成语言各分类概率\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_prediction\n",
    "\n",
    "# 输出submission.csv\n",
    "submission.to_csv('submission_lstm_fasttex.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.997645</td>\n",
       "      <td>4.509615e-01</td>\n",
       "      <td>0.983210</td>\n",
       "      <td>0.167452</td>\n",
       "      <td>0.914684</td>\n",
       "      <td>0.380847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>7.268186e-07</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>3.388999e-06</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>1.509362e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>5.976052e-06</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.015672e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>2.330638e-06</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.450074</td>\n",
       "      <td>1.516326e-03</td>\n",
       "      <td>0.026319</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.103240</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>0.259050</td>\n",
       "      <td>2.727079e-04</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.090293</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>3.525271e-07</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.997645  4.509615e-01  0.983210  0.167452  0.914684   \n",
       "1  0000247867823ef7  0.000671  7.268186e-07  0.000062  0.000008  0.000077   \n",
       "2  00013b17ad220c46  0.000904  3.388999e-06  0.000143  0.000036  0.000168   \n",
       "3  00017563c3f7919a  0.000123  1.509362e-07  0.000015  0.000004  0.000016   \n",
       "4  00017695ad8997eb  0.002842  5.976052e-06  0.000170  0.000049  0.000227   \n",
       "5  0001ea8717f6de06  0.000115  1.015672e-07  0.000009  0.000004  0.000015   \n",
       "6  00024115d4cbde0f  0.002264  2.330638e-06  0.000318  0.000023  0.000351   \n",
       "7  000247e83dcc1211  0.450074  1.516326e-03  0.026319  0.001539  0.103240   \n",
       "8  00025358d4737918  0.259050  2.727079e-04  0.009522  0.001149  0.090293   \n",
       "9  00026d1092fe71cc  0.000476  3.525271e-07  0.000042  0.000005  0.000045   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.380847  \n",
       "1       0.000006  \n",
       "2       0.000041  \n",
       "3       0.000002  \n",
       "4       0.000030  \n",
       "5       0.000002  \n",
       "6       0.000016  \n",
       "7       0.002929  \n",
       "8       0.000637  \n",
       "9       0.000004  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初步检查输出结果\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
