{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_union\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# read data\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 单独保存comment_text\n",
    "train_text = train['comment_text'].str.lower()\n",
    "test_text = test['comment_text'].str.lower()\n",
    "# 获得y_train\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# 连接所有文字用于分词\n",
    "train_text = train_text\n",
    "test_text = test_text\n",
    "all_text = pd.concat([train_text, test_text], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0\n",
       "5      0             0        0       0       0              0\n",
       "6      1             1        1       0       1              0\n",
       "7      0             0        0       0       0              0\n",
       "8      0             0        0       0       0              0\n",
       "9      0             0        0       0       0              0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据可视化分析\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# all_text = all_text[:100]\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "def text_cleaned_process(text_raw):    \n",
    "    text_raw = str(text_raw)\n",
    "    text_raw = str(text_raw.lower())\n",
    "    text_raw = re.sub(r'[^a-zA-Z]', ' ', text_raw)\n",
    "    \n",
    "    words = text_raw.split()\n",
    "    \n",
    "    # 移除长度小于3的词语\n",
    "    words2 = []\n",
    "    for i in words:\n",
    "        if len(i) >= 0:\n",
    "            words2.append(i)\n",
    "    # 去停止词\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    result_text = []\n",
    "    result_text = \" \".join([w for w in words2 if not w in stops])\n",
    "    \n",
    "    return(\" \".join(lemmatize_all(result_text)))\n",
    "\n",
    "# 去掉数字\n",
    "all_text.replace({r'[^\\x00-\\x7F]+':''},regex=True,inplace=True)\n",
    "\n",
    "num_all_text = all_text.size\n",
    "\n",
    "# 输出清洗后的数据\n",
    "all_text_cleaned = []\n",
    "\n",
    "for i in range(0, num_all_text):\n",
    "    all_text_cleaned.append(text_cleaned_process(all_text[i]))\n",
    "\n",
    "# 构建pd形式\n",
    "all_text_cleaned = pd.Series(all_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of all_text_cleaned: 312735\n",
      "Text[0] before cleaned: \n",
      " explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n",
      "Text[0] after cleaned: \n",
      " explanation edits make username hardcore metallica fan revert vandalisms closure gas vote new york doll fac please remove template talk page since retire\n"
     ]
    }
   ],
   "source": [
    "# 原始数据可视化分析\n",
    "# all_text = all_text[:100]\n",
    "print(\"Len of all_text_cleaned:\", len(all_text_cleaned))\n",
    "# 数据清理前后对比\n",
    "print(\"Text[0] before cleaned: \\n\", all_text[0])\n",
    "print(\"Text[0] after cleaned: \\n\", all_text_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分词处理\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), \n",
    "    analyzer = 'word',           \n",
    "    min_df=3, \n",
    "    max_df=0.9, \n",
    "    strip_accents='unicode', \n",
    "    use_idf=1,\n",
    "    smooth_idf=1, \n",
    "    sublinear_tf=1 )\n",
    "\n",
    "# 获取向量模型\n",
    "# vectorizer = make_union(word_vectorizer, word_vectorizer)\n",
    "\n",
    "# 训练tf-idf模型\n",
    "vectorizer.fit(all_text_cleaned)\n",
    "\n",
    "# 获取词向量\n",
    "train_features = vectorizer.transform(train_text)\n",
    "test_features = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: 151592 544855\n",
      "y_train shape: (151592, 6)\n"
     ]
    }
   ],
   "source": [
    "# 拆分train数据，用于验证模型好坏。\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_features, y_train, test_size = 0.05, random_state = 255)\n",
    "print(\"x_train shape:\", x_train.shape[0], x_train.shape[1])\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于NBSVM模型修改为NB+LR模型\n",
    "# 贝叶斯特征方程\n",
    "def pr(x, y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "# 模型建立\n",
    "def get_model(x,y):\n",
    "    y = y.values\n",
    "    # 贝叶斯概率\n",
    "    r = np.log(pr(x,1,y) / pr(x,0,y))\n",
    "    # 机器学习模型导入\n",
    "    classifier = LogisticRegression(C=4, dual=True)\n",
    "    # svm 速度太慢\n",
    "    # classifier = SVC(kernel = 'linear', random_state = 255, C = 4, probability=True)\n",
    "    # 向量相乘\n",
    "    x_nb = x.multiply(r)\n",
    "    # 返回训练后模型\n",
    "    return classifier.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "Training starts!  toxic\n",
      "ROC-AUC Score:  0.9745194732890752\n",
      "Training starts!  severe_toxic\n",
      "ROC-AUC Score:  0.9806229520986113\n",
      "Training starts!  obscene\n",
      "ROC-AUC Score:  0.9893626238529425\n",
      "Training starts!  threat\n",
      "ROC-AUC Score:  0.9877854598784831\n",
      "Training starts!  insult\n",
      "ROC-AUC Score:  0.9737127417804029\n",
      "Training starts!  identity_hate\n",
      "ROC-AUC Score:  0.9857800538068858\n",
      "Total mean ROC-AUC score is 0.9826305507844001\n"
     ]
    }
   ],
   "source": [
    "# 保存得分\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "scores = []\n",
    "\n",
    "print(\"Training starts!\")\n",
    "for class_name in class_names:\n",
    "    # 依次读取训练目标数据\n",
    "    train_target = y_train[class_name]\n",
    "    print(\"Training starts! \", class_name)\n",
    "    # 导入并测试模型\n",
    "    classifier, r = get_model(x_train, train_target)\n",
    "    \n",
    "    # 模型验证roc-auc score\n",
    "    preds_val = classifier.predict_proba(x_val.multiply(r))[:, 1]\n",
    "    # 计算roc-auc得分\n",
    "    roc_auc = roc_auc_score(y_val[class_name], preds_val)\n",
    "    # 保存并输出得分\n",
    "    scores.append(roc_auc)\n",
    "    print(\"ROC-AUC Score: \", roc_auc)\n",
    "    \n",
    "    # 输出test数据预测概率\n",
    "    submission[class_name] = classifier.predict_proba(test_features.multiply(r))[:, 1]\n",
    "\n",
    "# 计算所有类型的平均得分\n",
    "print('Total mean ROC-AUC score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果文件 \n",
    "submission.to_csv('submission_tf-idf_nblr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.972567</td>\n",
       "      <td>0.159247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.000345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.019914</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.000597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.025718</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.544172</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>0.073006</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.017635</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.999954      0.022472  0.999854  0.002673  0.972567   \n",
       "1  0000247867823ef7  0.002022      0.000495  0.000883  0.000056  0.002245   \n",
       "2  00013b17ad220c46  0.019914      0.000875  0.003614  0.000112  0.004860   \n",
       "3  00017563c3f7919a  0.001712      0.000734  0.001881  0.000299  0.002172   \n",
       "4  00017695ad8997eb  0.025718      0.000776  0.004733  0.000150  0.006023   \n",
       "5  0001ea8717f6de06  0.001980      0.000222  0.001759  0.000127  0.002114   \n",
       "6  00024115d4cbde0f  0.000860      0.000042  0.000865  0.000043  0.001213   \n",
       "7  000247e83dcc1211  0.544172      0.000839  0.004133  0.000104  0.013233   \n",
       "8  00025358d4737918  0.073006      0.000593  0.006754  0.000920  0.017635   \n",
       "9  00026d1092fe71cc  0.001725      0.000234  0.000997  0.000066  0.002036   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.159247  \n",
       "1       0.000345  \n",
       "2       0.000597  \n",
       "3       0.000397  \n",
       "4       0.000362  \n",
       "5       0.000433  \n",
       "6       0.000766  \n",
       "7       0.000438  \n",
       "8       0.001338  \n",
       "9       0.000347  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初步检查输出结果\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
