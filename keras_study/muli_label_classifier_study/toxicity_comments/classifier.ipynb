{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classifier.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["I-YH0loqgtG5","Hckz23Y9tmC2","9Q-8jo2MCrZt","TeDrob-w6nYX","RulHCu9ZOqiG","u_ta0f2VS0VC"],"toc_visible":true,"authorship_tag":"ABX9TyMmsDcQL27jdjjR58hLbcdP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"i2iar67NfID4","colab_type":"text"},"source":["## 连接 colab drive"]},{"cell_type":"code","metadata":{"id":"Bz7e_p_pfB5E","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Nset-CIfsUC","colab_type":"code","colab":{}},"source":["cd /content/drive/My Drive/NLP_study/classifier_study/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUtsiOmxfxpw","colab_type":"code","colab":{}},"source":["!ls keras_layers/transformer_utils/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWIHKvX4V8TF","colab_type":"text"},"source":["# 模型训练模块"]},{"cell_type":"markdown","metadata":{"id":"3gHGRJJYf7Sj","colab_type":"text"},"source":["## 导入相关库"]},{"cell_type":"code","metadata":{"id":"GJDFc7bBgNeo","colab_type":"code","colab":{}},"source":["# 导入相应的库\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import CountVectorizer\n","import datetime\n","import numpy as np\n","import io\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from keras.models import Model\n","from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,LSTM,Bidirectional,Activation,Conv1D,GRU\n","from keras.layers import Reshape, Flatten, Concatenate,concatenate,Dropout, SpatialDropout1D, CuDNNLSTM, CuDNNGRU,Layer\n","from keras.layers import GlobalMaxPooling1D, MaxPooling1D, Add,GlobalAveragePooling1D,ZeroPadding1D,Lambda\n","from keras.preprocessing import text, sequence\n","from keras.callbacks import Callback\n","import os\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n","from keras.optimizers import Adam\n","from keras.models import load_model\n","import keras\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sj7KK5rmgcTw","colab_type":"text"},"source":["## 数据导入"]},{"cell_type":"code","metadata":{"id":"NZoh0gfwgSUI","colab_type":"code","colab":{}},"source":["# 计时开始\n","starttime = datetime.datetime.now()\n","\n","# 加载训练集和测试集数据\n","train = pd.read_csv('train_clean.csv').fillna(' ')[0:5000]\n","class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","text_name = 'clean'\n","# 保存结果到csv文件中\n","max_features = 10000 # 最大特征数，现有数据中所有不同单词的种数\n","maxlen = 100 # 一条评论的词种类数的最大限制\n","embed_size = 50 # 预训练词向量的维度\n","batch_size = 128\n","epochs = 100\n","num_filters = 32 # 过滤器数\n","rnn_type = 'GRU'\n","tokenizer_path = 'tokenizer.pkl'\n","train_size = 0.7  # 训练集 和 测试集 分割比例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2O8ljKhAggVq","colab_type":"text"},"source":["## 函数"]},{"cell_type":"code","metadata":{"id":"CCwR7H68gfWS","colab_type":"code","colab":{}},"source":["def open_file(fname):\n","  embeddings_index = {}\n","  with open(fname,encoding='utf8') as f:\n","      for line in f:\n","          values = line.rstrip().rsplit(' ')\n","          word = values[0]\n","          coefs = np.asarray(values[1:], dtype='float32')\n","          embeddings_index[word] = coefs\n","  return embeddings_index\n","\n","def getTrainTokenizer(train,text_name,max_features,maxlen,tokenizer_path):\n","  '''\n","    功能：词向量加载\n","    :param train: dataframe 训练集\n","    :param text_name: string 训练集中 文本数据 的 列名\n","    :param maxlen: int， 评论中的词种类数的最大限制\n","    :param max_features: int  最大特征数，现有数据中所有不同单词的种数\n","    :param tokenizer_path: string tokenizer 保存地址\n","    :return \n","      X_train:  matrix      训练数据序列化\n","      tokenizer: object tokenizer \n","  '''\n","  X_train = train[text_name].values\n","  ## 将评论数据，转转成sequences形式，评论中英文单词类别数最大为200 \n","  # 分词器\n","  tokenizer = text.Tokenizer(num_words=max_features)\n","  tokenizer.fit_on_texts(list(X_train))\n","  # 将评论数据转换成sequences，[1,2,3]\n","  X_train = tokenizer.texts_to_sequences(X_train)\n","  # 统一长度\n","  X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n","  with open(tokenizer_path,'wb') as f:\n","    pickle.dump(tokenizer, f)  #模型保存\n","  return X_train,tokenizer\n","\n","\n","def getEmbeddingMatrix(fname,tokenizer,embed_size):\n","  '''\n","    功能：词向量加载\n","    :param fname: string 词向量路径\n","    :param embed_size: int 预训练词向量的维度\n","    :param tokenizer: object tokenizer \n","    :return \n","      embedding_matrix: matrix  词向量加载\n","  '''\n","  embeddings_index = open_file(fname)\n","  word_index = tokenizer.word_index\n","  num_words = min(max_features, len(word_index))\n","  embedding_matrix = np.zeros((num_words, embed_size))\n","  for word, i in word_index.items():\n","      if i >= max_features:continue\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:embedding_matrix[i] = embedding_vector\n","  return embedding_matrix\n","\n","\n","class RocAucEvaluation(Callback):\n","  \"\"\" Callback子类，用于打印ROC-AUC分数 \"\"\"\n","  def __init__(self, validation_data=(), interval=1):\n","      super(Callback, self).__init__()\n","      self.interval = interval\n","      self.X_val, self.y_val = validation_data\n","\n","  def on_epoch_end(self, epoch, logs={}):\n","      if epoch % self.interval == 0:\n","          y_pred = self.model.predict(self.X_val, verbose=0)\n","          score = roc_auc_score(self.y_val, y_pred)\n","          print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4obFCkEottB2","colab_type":"text"},"source":["## 数据加载"]},{"cell_type":"code","metadata":{"id":"I8iSWACmg_Wq","colab_type":"code","colab":{}},"source":["print('star CNN...')\n","# 数据\n","Y_train = train[class_names].values\n","X_train,tokenizer = getTrainTokenizer(train,text_name,max_features,maxlen,tokenizer_path)\n","embedding_matrix = getEmbeddingMatrix(\"glove.6B.50d.txt\",tokenizer,embed_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pF1N8yXierRA","colab_type":"text"},"source":["## Basemodel 模型"]},{"cell_type":"code","metadata":{"id":"p4-B3Zgw6gkK","colab_type":"code","colab":{}},"source":["class Basemodel():\n","  '''\n","    所有模型父类，该模型用于保存其他模型共性方法\n","  '''\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    '''\n","      功能：模型初始化\n","      :param maxlen: int， 评论中的词种类数的最大限制\n","      :param max_features: int，  最大特征数，现有数据中所有不同单词的种数\n","      :param embed_size: int， 预训练词向量的维度\n","      :param embedding_matrix: matrix， 词向量矩阵\n","      :param model_save_path: string， 模型存储路径\n","      :param num_filters: string， 过滤器数\n","    '''\n","    self.maxlen = maxlen            # 文本最大长度\n","    self.max_features = max_features      # 核数\n","    self.embed_size = embed_size        # 嵌入层尺寸  \n","    self.embedding_matrix = embedding_matrix  # 词向量矩阵\n","    self.model_save_path = model_save_path   # 模型存储路径\n","    self.dropout = 0.5             # dropout层系数，舍弃\n","    self.optimizer = 'adam'           # 优化函数\n","    self.label = 6               # 输出数量\n","    self.num_filters = 32            # 过滤器数\n","    self.lr = 1  #e-3               # 学习率\n","    self.l2 = 1e-6               # l2正则化系数\n","    self.activate_classify = 'softmax'     # 分类激活函数,softmax或者signod\n","    self.filters = [3, 4, 5]         # 卷积核大小\n","    self.decay_rate = 0.9            # 衰减系数\n","    self.decay_step = 100            # 衰减步数\n","    self.patience = 3              # 早停, 2-3就可以了\n","    self.rnn_units = 256            # rnn 单元数\n","    self.len_max_sen = 100 \n","    self.droupout_spatial = 0.2\n","    self.attention_units = self.rnn_units*2\n","    self.rnn_type = 'Bidirectional-LSTM'\n","    self.trainable = True           # 是否微调, 例如静态词向量、动态词向量、微调bert层等, random也可以\n","    self.loss = 'binary_crossentropy'      # 损失函数, mse, categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy等\n","    self.metrics = 'accuracy'          # acc, binary_accuracy, categorical_accuracy, sparse_categorical_accuracy, sparse_top_k_categorical_accuracy\n","\n","  def build_model(self):\n","    '''\n","      功能：模型构建，每个模型都不一样\n","    '''\n","    pass\n","  \n","  def train(self,X_train,Y_train,train_size,batch_size,epochs):\n","    '''\n","      功能：模型训练\n","      :param X_train: matrix， 训练集 X\n","      :param Y_train: matrix， 训练集 标签\n","      :param train_size: float， 训练集比例\n","      :param batch_size: int， batch 大小\n","      :param epochs: int 循环次数\n","    '''\n","    self.batch_size = batch_size\n","    self.epochs = epochs\n","    # 拆分训练集和验证集\n","    self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(X_train, Y_train, train_size=train_size)  \n","    self.create_compile()\n","    # 训练\n","    # verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n","    hist = self.model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=epochs,\n","                validation_data=(self.x_val, self.y_val),\n","                callbacks=self.callback(), verbose=2)\n","    self.model.save(self.model_save_path) \n","\n","  def create_compile(self):\n","    \"\"\"\n","      构建优化器、损失函数和评价函数\n","    :return: \n","    \"\"\"\n","    if self.optimizer == \"adam\":\n","      self.model.compile(optimizer=Adam(lr=self.lr, beta_1=0.9, beta_2=0.999, decay=0.0),\n","                          loss=self.loss,\n","                          metrics=[self.metrics]) # Any optimize\n","    elif self.optimizer == \"radam\":\n","      self.model.compile(optimizer=RAdam(lr=self.lr, beta_1=0.9, beta_2=0.999, decay=0.0),\n","                          loss=self.loss,\n","                          metrics=[self.metrics]) # Any optimize\n","    else:\n","      self.model.compile(optimizer=RAdam(lr=self.lr, beta_1=0.9, beta_2=0.999, decay=0.0),\n","                          loss=self.loss,\n","                          metrics=[self.metrics]) # Any optimize\n","      lookahead = Lookahead(k=5, alpha=0.5)  # Initialize Lookahead\n","      lookahead.inject(self.model)  # add into model\n","\n","  def callback(self):\n","    \"\"\"\n","      评价函数、早停\n","    :return: \n","    \"\"\"\n","    cb_em = [ TensorBoard(log_dir=os.path.join(self.path_model_dir, \"logs\"), \n","                 batch_size=self.batch_size, update_freq='batch'),\n","          EarlyStopping(monitor='val_loss', \n","                  mode='min', \n","                  min_delta=1e-8, \n","                  patience=self.patience,\n","                  baseline=None, \n","                  restore_best_weights=True),\n","          ModelCheckpoint(monitor='val_loss', \n","                  mode='min', \n","                  filepath=self.model_save_path, \n","                  verbose=1,\n","                  save_best_only=True, \n","                  save_weights_only=True),\n","          RocAucEvaluation(validation_data=(self.x_val, self.y_val), \n","                  interval=1)]\n","    return cb_em\n","\n","  def predict(self,X_test):\n","    '''\n","      功能：模型预测\n","    '''\n","    model = load_model(self.model_save_path)\n","    y_pred = model.predict(X_test)\n","    return y_pred\n","  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-YH0loqgtG5","colab_type":"text"},"source":["## Text-CNN 方法"]},{"cell_type":"markdown","metadata":{"id":"fqYy6mz5hJ_5","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"5OSdRQ0dhIac","colab_type":"code","colab":{}},"source":["class TextCNN(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/cnn/\"\n","  def build_model(self):\n","    inp = Input(shape=(self.maxlen, ))\n","    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n","    x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","    # 随机丢弃词，提高训练速度，提高词的独立性\n","    x = SpatialDropout1D(0.2)(x)\n","    # 转换维度，添加第三维，维度是1\n","    x = Reshape((self.maxlen, self.embed_size, 1))(x)\n","    # 卷积层，过滤器32,大小1*300\n","    conv_1 = Conv2D(self.num_filters, kernel_size=(1, self.embed_size), kernel_initializer='normal',activation='elu')(x)\n","    conv_2 = Conv2D(self.num_filters, kernel_size=(2, self.embed_size), kernel_initializer='normal',activation='elu')(x)\n","    conv_3 = Conv2D(self.num_filters, kernel_size=(3, self.embed_size), kernel_initializer='normal',activation='elu')(x)\n","    conv_5 = Conv2D(self.num_filters, kernel_size=(5, self.embed_size), kernel_initializer='normal',activation='elu')(x)\n","    # 最大池化层\n","    maxpool_1 = MaxPool2D(pool_size=(self.maxlen, 1))(conv_1)\n","    maxpool_2 = MaxPool2D(pool_size=(self.maxlen - 1, 1))(conv_2)\n","    maxpool_3 = MaxPool2D(pool_size=(self.maxlen - 2, 1))(conv_3)\n","    maxpool_5 = MaxPool2D(pool_size=(self.maxlen - 4, 1))(conv_5)\n","    # 连接最大池化层\n","    z = Concatenate(axis=1)([maxpool_1, maxpool_2,maxpool_3,maxpool_5])   \n","    # 压平\n","    z = Flatten()(z)\n","    # 随机丢弃，提高最后训练速度，防止因全连接层导致过拟合\n","    z = Dropout(self.dropout)(z)\n","    # 全连接层，输入六个值\n","    outp = Dense(self.label, activation=\"sigmoid\")(z)\n","    self.model = Model(inputs=inp, outputs=outp)\n","    self.model.summary(120)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q1PxgshhZxc","colab_type":"text"},"source":["### 模型训练"]},{"cell_type":"code","metadata":{"id":"ZTptfLtm4Wui","colab_type":"code","colab":{}},"source":["text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,'cnn_model.h5')\n","text_cnn_model.build_model()\n","text_cnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hckz23Y9tmC2","colab_type":"text"},"source":["## Text-RNN 方法"]},{"cell_type":"markdown","metadata":{"id":"ScFB_oTSt_GG","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"U69_BF1It2pv","colab_type":"code","colab":{}},"source":["class TextRNN(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/rnn/\"\n","  def build_model(self,rnn_type):\n","    rnn_type_dict = {\n","      'LSTM': LSTM,\n","      'GRU': GRU,\n","      'CuDNNLSTM': CuDNNLSTM,\n","      'CuDNNGRU': CuDNNGRU,\n","    }\n","    if rnn_type in rnn_type_dict:\n","      layer_cell = rnn_type_dict[rnn_type]\n","    else:\n","      layer_cell = rnn_type_dict['GRU']\n","    inp = Input(shape=(self.maxlen, ))\n","    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n","    x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","    # 随机丢弃词，提高训练速度，提高词的独立性\n","    x = SpatialDropout1D(0.2)(x)\n","    x = Bidirectional(layer_cell(128, return_sequences=True,dropout=self.dropout,recurrent_dropout=0.1))(x)\n","    x = Dropout(self.dropout)(x)\n","    x = Flatten()(x)\n","    preds = Dense(self.label, activation=\"sigmoid\")(x)\n","    self.model = Model(inp, preds)\n","    self.model.summary(120)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdbkzNDyulgn","colab_type":"text"},"source":["### 模型训练"]},{"cell_type":"code","metadata":{"id":"a9U3r_wAupIn","colab_type":"code","colab":{}},"source":["# X_train,Y_train,train_size,maxlen,max_features,embed_size,embedding_matrix,model_save_path,batch_size,epochs\n","text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,'rnn_model.h5')\n","text_rnn_model.build_model('GRU')\n","text_rnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q-8jo2MCrZt","colab_type":"text"},"source":["## Text-RCNN 方法"]},{"cell_type":"markdown","metadata":{"id":"p_YWbG3Zu3sv","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"PfHdwGqLu5zV","colab_type":"code","colab":{}},"source":["class TextRCNN(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/rcnn/\"\n","  def build_model(self,rnn_type):\n","    rnn_type_dict = {\n","      'LSTM': LSTM,\n","      'GRU': GRU,\n","      'CuDNNLSTM': CuDNNLSTM,\n","      'CuDNNGRU': CuDNNGRU,\n","    }\n","    if rnn_type in rnn_type_dict:\n","      layer_cell = rnn_type_dict[rnn_type]\n","    else:\n","      layer_cell = rnn_type_dict['GRU']\n","    inp = Input(shape=(self.maxlen, ))\n","    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n","    x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","    # 随机丢弃词，提高训练速度，提高词的独立性\n","    x = SpatialDropout1D(0.2)(x)\n","    x = Bidirectional(layer_cell(128, return_sequences=True,dropout=self.dropout,recurrent_dropout=0.1))(x)\n","    x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n","    avg_pool = GlobalAveragePooling1D()(x)\n","    max_pool = GlobalMaxPooling1D()(x)\n","    x = concatenate([avg_pool, max_pool]) \n","    preds = Dense(self.label, activation=\"sigmoid\")(x)\n","    self.model = Model(inp, preds)\n","    self.model.summary(120)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeLX8bfZPVuA","colab_type":"text"},"source":["### 模型训练"]},{"cell_type":"code","metadata":{"id":"nkXvi61LPWU0","colab_type":"code","colab":{}},"source":["text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,'text_rcnn_model.h5')\n","text_rcnn_model.build_model('GRU')\n","text_rcnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TeDrob-w6nYX","colab_type":"text"},"source":["## self-Attention 方法"]},{"cell_type":"markdown","metadata":{"id":"A_fh-j_y61j2","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"ibRDVcvX6vgP","colab_type":"code","colab":{}},"source":["from keras.regularizers import L1L2, Regularizer\n","from keras.engine.topology import Layer\n","from keras import backend as K\n","class AttentionSelf(Layer):\n","  \"\"\"\n","    self attention,\n","    codes from:  https://mp.weixin.qq.com/s/qmJnyFMkXVjYBwoR_AQLVA\n","  \"\"\"\n","  def __init__(self, output_dim, **kwargs):\n","    self.output_dim = output_dim\n","    super().__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    # W、K and V\n","    self.kernel = self.add_weight(name='WKV',\n","                    shape=(3, input_shape[2], self.output_dim),\n","                    initializer='uniform',\n","                    regularizer=L1L2(0.0000032),\n","                    trainable=True)\n","    super().build(input_shape)\n","\n","  def call(self, x):\n","    WQ = K.dot(x, self.kernel[0])\n","    WK = K.dot(x, self.kernel[1])\n","    WV = K.dot(x, self.kernel[2])\n","    print(\"WQ.shape\",WQ.shape)\n","    print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n","    QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n","    QK = QK / (64**0.5)\n","    QK = K.softmax(QK)\n","    print(\"QK.shape\",QK.shape)\n","    V = K.batch_dot(QK,WV)\n","    return V\n","\n","  def compute_output_shape(self, input_shape):\n","      return (input_shape[0],input_shape[1],self.output_dim)\n","\n","class SelfAttention(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/selfAttention/\"\n","  def build_model(self):\n","    inp = Input(shape=(self.maxlen, ))\n","    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n","    x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","    # 随机丢弃词，提高训练速度，提高词的独立性\n","    x = SpatialDropout1D(0.2)(x)\n","    x = AttentionSelf(self.embed_size)(x)\n","    x = GlobalMaxPooling1D()(x)\n","    x = Dropout(self.dropout)(x)\n","    # 全连接层，输入六个值\n","    outp = Dense(self.label, activation=\"sigmoid\")(x)\n","    self.model = Model(inputs=inp, outputs=outp)\n","    self.model.summary(120)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPjeF1eN86Kf","colab_type":"text"},"source":["### 模型训练 "]},{"cell_type":"code","metadata":{"id":"xK029qnQ7yLh","colab_type":"code","colab":{}},"source":["self_attention_model = SelfAttention(maxlen,max_features,embed_size,embedding_matrix,'self_attention_model.h5')\n","self_attention_model.build_model()\n","self_attention_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RulHCu9ZOqiG","colab_type":"text"},"source":["## HAN 方法"]},{"cell_type":"markdown","metadata":{"id":"TPfeDHDsOvG9","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"t-Vtar0qOpg3","colab_type":"code","colab":{}},"source":["from keras.layers import Dense, Dropout, SpatialDropout1D, Flatten, Input\n","from keras.layers import Bidirectional, LSTM, GRU, TimeDistributed\n","from keras import regularizers\n","from keras.models import Model\n","import keras.backend as K\n","class HAN(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/han/\"\n","\n","  def build_model(self):\n","    inp = Input(shape=(self.maxlen, ))\n","    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n","    self.word_embedding = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","    # 随机丢弃词，提高训练速度，提高词的独立性\n","    x_word = self.word_level()(self.word_embedding)\n","    x_word_to_sen = Dropout(self.dropout)(x_word)\n","\n","    # sentence or doc\n","    x_sen = self.sentence_level()(x_word_to_sen)\n","    x_sen = Dropout(self.dropout)(x_sen)\n","\n","    x_sen = Flatten()(x_sen)\n","    # 全连接层，输入六个值\n","    outp = Dense(self.label, activation=\"sigmoid\")(x_sen)\n","    self.model = Model(inputs=inp, outputs=outp)\n","    self.model.summary(120)\n","\n","  def word_level(self):\n","    x_input_word = Input(shape=(self.maxlen, self.embed_size))\n","    # x = SpatialDropout1D(self.dropout_spatial)(x_input_word)\n","    x = Bidirectional(GRU(units=self.rnn_units,\n","                  return_sequences=True,\n","                  activation='relu',\n","                  kernel_regularizer=regularizers.l2(self.l2),\n","                  recurrent_regularizer=regularizers.l2(self.l2)))(x_input_word)\n","    out_sent = AttentionSelf(self.rnn_units*2)(x)\n","    model = Model(x_input_word, out_sent)\n","    return model\n","\n","  def sentence_level(self):\n","    x_input_sen = Input(shape=(self.maxlen, self.rnn_units*2))\n","    # x = SpatialDropout1D(self.dropout_spatial)(x_input_sen)\n","    output_doc = Bidirectional(GRU(units=self.rnn_units*2,\n","                    return_sequences=True,\n","                    activation='relu',\n","                    kernel_regularizer=regularizers.l2(self.l2),\n","                    recurrent_regularizer=regularizers.l2(self.l2)))(x_input_sen)\n","    output_doc_att = AttentionSelf(self.embed_size)(output_doc)\n","    model = Model(x_input_sen, output_doc_att)\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_ta0f2VS0VC","colab_type":"text"},"source":["### 模型训练"]},{"cell_type":"code","metadata":{"id":"iyMeC0w6S0-o","colab_type":"code","colab":{}},"source":["han_model = HAN(maxlen,max_features,embed_size,embedding_matrix,'han_model.h5')\n","han_model.build_model()\n","han_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lXWn2S5keDU","colab_type":"text"},"source":["## Transformer"]},{"cell_type":"markdown","metadata":{"id":"wg-LbKWnmaJ9","colab_type":"text"},"source":["### 函数"]},{"cell_type":"code","metadata":{"id":"_VS2YgdwmCum","colab_type":"code","colab":{}},"source":["from keras_layers.transformer_utils.triangle_position_embedding import TriglePositiomEmbedding\n","from keras_layers.transformer_utils.embedding import EmbeddingRet\n","from keras_layers.transformer import build_encoders\n","\n","from keras_layers.non_mask_layer import NonMaskingLayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwO_52Mzkfhb","colab_type":"text"},"source":["### 模型构建"]},{"cell_type":"code","metadata":{"id":"zbvfyHQCkf-U","colab_type":"code","colab":{}},"source":["from keras import regularizers\n","from keras.models import Model\n","import keras.backend as K\n","import numpy as np\n","\n","class Transformer(Basemodel):\n","  def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n","    super().__init__(maxlen,max_features,embed_size,embedding_matrix,model_save_path)\n","    self.path_model_dir =  \"./model/capsuleNet/\"\n","    self.encoder_num = 1\n","    self.head_num = 12\n","    self.hidden_dim = 3072\n","    self.attention_activation =  'relu'\n","    self.feed_forward_activation =  'relu'\n","    self.use_adapter = False\n","    self.adapter_units = 768\n","    self.adapter_activation  = 'relu'\n","    self.embed_size = 768 # 预训练词向量的维度\n","    self.trainable = True           # 是否微调, 例如静态词向量、动态词向量、微调bert层等, \n","    self.loss = 'categorical_crossentropy'    # 损失函数, mse, categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy等\n","\n","\n","\n","  def build_model(self):\n","    self.word_embedding = Embedding( self.max_features, \n","                      self.embed_size, \n","                      input_length=self.maxlen,\n","                      trainable=True,)\n","    encoder_input = keras.layers.Input(shape=(self.maxlen,), name='Encoder-Input')\n","\n","    # self.word_embedding = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n","\n","    encoder_embed_layer = EmbeddingRet(input_dim=self.max_features,\n","                      output_dim=self.embed_size,\n","                      mask_zero=False,\n","                      weights=None,\n","                      trainable=True,\n","                      name='Token-Embedding',)\n","    encoder_embedding = encoder_embed_layer(encoder_input)\n","    encoder_embed = TriglePositiomEmbedding(mode=TriglePositiomEmbedding.MODE_ADD,\n","                          name='Encoder-Embedding',)(encoder_embedding[0])\n","    encoded_layer = build_encoders(encoder_num=self.encoder_num,\n","                          input_layer=encoder_embed,\n","                          head_num=self.head_num,\n","                          hidden_dim=self.hidden_dim,\n","                          attention_activation=self.activate_classify,\n","                          feed_forward_activation=self.activate_classify,\n","                          dropout_rate=self.dropout,\n","                          trainable=True,\n","                          use_adapter=self.use_adapter,\n","                          adapter_units=self.adapter_units,\n","                          adapter_activation=self.adapter_activation,\n","                          )\n","    encoded_layer = NonMaskingLayer()(encoded_layer)\n","    encoded_layer_flat = Flatten()(encoded_layer)\n","    encoded_layer_drop = Dropout(self.dropout)(encoded_layer_flat)\n","    output = Dense(self.label, activation=self.activate_classify)(encoded_layer_drop)\n","    self.model = Model(inputs=encoder_input, outputs=output)\n","    self.model.summary(120)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4BEesbMlkZt","colab_type":"text"},"source":["### 模型训练"]},{"cell_type":"code","metadata":{"id":"TMlyRpy9lk11","colab_type":"code","colab":{}},"source":["transformer_model = Transformer(maxlen,max_features,embed_size,embedding_matrix,'Transformer_model.h5')\n","transformer_model.build_model()\n","transformer_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZjVLxGRu6IN","colab_type":"text"},"source":["### 模型预测"]},{"cell_type":"code","metadata":{"id":"MCS6Yl0Hu739","colab_type":"code","colab":{}},"source":["# X_train,Y_train,train_size,maxlen,max_features,embed_size,embedding_matrix,model_save_path,batch_size,epochs\n","text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,'rcnn_model.h5')\n","text_rcnn_model.build_model('GRU')\n","text_rcnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8W0zgz0oecpi","colab_type":"text"},"source":["## 预测"]},{"cell_type":"markdown","metadata":{"id":"_iB4gcjAefBP","colab_type":"text"},"source":["### 函数"]},{"cell_type":"code","metadata":{"id":"ssYbch8JehPw","colab_type":"code","colab":{}},"source":["def text_process(text):\n","  '''\n","    功能：文本预处理\n","    :param text: string 文本\n","    :return\n","     cleaned_text:  string 处理后文本\n","     cleaned_text_len: int 文本长度\n","  '''\n","  # 大小写转换\n","  lower = text.lower()\n","  # 标点符号的处理\n","  #string.punctuation中包含英文的标点，我们将其放在待去除变量remove中\n","  #函数需要三个参数，前两个表示字符的映射，我们是不需要的。\n","  remove = str.maketrans('','',string.punctuation) \n","  without_punctuation = lower.translate(remove)\n","  # 分词 \n","  tokens = nltk.word_tokenize(without_punctuation)\n","  # 去除停用词\n","  without_stopwords = [w for w in tokens if not w in stopwords.words('english')]\n","  # 提取词干\n","  s = nltk.stem.SnowballStemmer('english')  #参数是选择的语言\n","  cleaned_text = [s.stem(ws) for ws in without_stopwords]\n","  cleaned_text_len = len(cleaned_text)\n","  cleaned_text = ' '.join(cleaned_text)\n","  return cleaned_text,cleaned_text_len\n","\n","def getTestTokenizer(test,maxlen,tokenizer_path):\n","  '''\n","    功能：词向量加载\n","    :param test: string test 测试集\n","    :param maxlen: int， 评论中的词种类数的最大限制\n","    :param tokenizer_path: string tokenizer 保存地址\n","    :return \n","      X_test:  matrix      测试数据序列化\n","      tokenizer: object tokenizer \n","  '''\n","  # 分词器\n","  with open(tokenizer_path,'rb') as f:\n","    tokenizer = pickle.load(f) #模型载入\n","  # 将评论数据转换成sequences，[1,2,3]\n","  X_test = tokenizer.texts_to_sequences(test)\n","  # 统一长度\n","  X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n","  return X_test,tokenizer\n","\n","threshold = 0.8 # 阙值 设定\n","def model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names):\n","  result = text_rnn_y_pred.copy()\n","  for c in class_names:\n","    result[c] = (3* text_cnn_y_pred[c] + 2 * text_rnn_y_pred[c] + 5 * text_rcnn_y_pred[c]) / 10\n","    result.loc[result[c] > threshold, c] = 1\n","    result.loc[result[c] <= threshold, c] = 0\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chUPdRe8Mnjn","colab_type":"text"},"source":["### 行预测"]},{"cell_type":"code","metadata":{"id":"lrZn2o2DeiUI","colab_type":"code","colab":{}},"source":["test = \"Fuck you, block me, you faggot pussy!\"\n","test,test_len = text_process(test)\n","test = [test]\n","X_test,tokenizer = getTestTokenizer(test,maxlen,tokenizer_path)\n","embedding_matrix = getEmbeddingMatrix(\"glove.6B.50d.txt\",tokenizer,embed_size)\n","\n","text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,'cnn_model.h5')\n","text_cnn_y_pred = pd.DataFrame(text_cnn_model.pred_line(X_test))\n","text_cnn_y_pred.columns = class_names\n","print(text_cnn_y_pred)\n","\n","text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,'rnn_model.h5')\n","text_rnn_y_pred = pd.DataFrame(text_rnn_model.pred_line(X_test))\n","text_rnn_y_pred.columns = class_names\n","print(text_rnn_y_pred)\n","\n","text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,'rcnn_model.h5')\n","text_rcnn_y_pred = pd.DataFrame(text_rcnn_model.pred_line(X_test))\n","text_rcnn_y_pred.columns = class_names\n","print(text_rcnn_y_pred)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkYaLEOFNfYe","colab_type":"code","colab":{}},"source":["result = model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names)\n","result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MW8rZJ2x60Vg","colab_type":"text"},"source":["### 批量预测"]},{"cell_type":"code","metadata":{"id":"RZyl_1FP6gCO","colab_type":"code","colab":{}},"source":["# 加载训练集和测试集数据\n","test = pd.read_csv('train.csv').fillna(' ')[0:50]\n","class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","text_name = 'comment_text'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx_HfxvCOrAb","colab_type":"code","colab":{}},"source":["len(test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4gwR_YuOYO-","colab_type":"code","colab":{}},"source":["test['clean'],test['sent_len'] = zip(*test[text_name].apply(text_process)) \n","test = test['clean']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDwefexvOHHw","colab_type":"code","colab":{}},"source":["X_test,tokenizer = getTestTokenizer(test,maxlen,tokenizer_path)\n","embedding_matrix = getEmbeddingMatrix(\"glove.6B.50d.txt\",tokenizer,embed_size)\n","\n","text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,'cnn_model.h5')\n","text_cnn_y_pred = pd.DataFrame(text_cnn_model.predict(X_test))\n","text_cnn_y_pred.columns = class_names\n","# print(text_cnn_y_pred)\n","\n","text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,'rnn_model.h5')\n","text_rnn_y_pred = pd.DataFrame(text_rnn_model.predict(X_test))\n","text_rnn_y_pred.columns = class_names\n","# print(text_rnn_y_pred)\n","\n","text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,'rcnn_model.h5')\n","text_rcnn_y_pred = pd.DataFrame(text_rcnn_model.predict(X_test))\n","text_rcnn_y_pred.columns = class_names\n","# print(text_rcnn_y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtdnD02NO-fQ","colab_type":"code","colab":{}},"source":["result = model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names)\n","result"],"execution_count":0,"outputs":[]}]}