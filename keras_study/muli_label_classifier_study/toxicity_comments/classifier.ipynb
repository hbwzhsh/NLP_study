{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2iar67NfID4"
   },
   "source": [
    "## 连接 colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bz7e_p_pfB5E"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Nset-CIfsUC"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/My Drive/NLP_study/classifier_study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUtsiOmxfxpw"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWIHKvX4V8TF"
   },
   "source": [
    "# 模型训练模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gHGRJJYf7Sj"
   },
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJDFc7bBgNeo"
   },
   "outputs": [],
   "source": [
    "# 导入相应的库\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.layers import Reshape, Flatten, Concatenate, concatenate,Dropout, SpatialDropout1D, CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, Add, Flatten,GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj7KK5rmgcTw"
   },
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZoh0gfwgSUI"
   },
   "outputs": [],
   "source": [
    "# 计时开始\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "data_path = 'data/'\n",
    "train_path = data_path+'train_clean.csv'\n",
    "emb_path = data_path + 'glove.6B.50d.txt'\n",
    "model_path = 'model/'\n",
    "\n",
    "# 加载训练集和测试集数据\n",
    "train = pd.read_csv(train_path).fillna(' ')#[0:5000]\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "text_name = 'clean'\n",
    "# 保存结果到csv文件中\n",
    "max_features = 100000 # 最大特征数，现有数据中所有不同单词的种数\n",
    "maxlen = 100 # 一条评论的词种类数的最大限制\n",
    "embed_size = 50 # 预训练词向量的维度\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "num_filters = 32 # 过滤器数\n",
    "rnn_type = 'GRU'\n",
    "tokenizer_path = model_path+'tokenizer.pkl'\n",
    "train_size = 0.7  # 训练集 和 测试集 分割比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2O8ljKhAggVq"
   },
   "source": [
    "## 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCwR7H68gfWS"
   },
   "outputs": [],
   "source": [
    "def open_file(fname):\n",
    "    embeddings_index = {}\n",
    "    with open(fname,encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def getTrainTokenizer(train,text_name,max_features,maxlen,tokenizer_path):\n",
    "    '''\n",
    "        功能：词向量加载\n",
    "        :param train: dataframe 训练集\n",
    "        :param text_name: string 训练集中 文本数据 的 列名\n",
    "        :param maxlen: int， 评论中的词种类数的最大限制\n",
    "        :param max_features: int  最大特征数，现有数据中所有不同单词的种数\n",
    "        :param tokenizer_path: string tokenizer 保存地址\n",
    "        :return \n",
    "          X_train:  matrix      训练数据序列化\n",
    "          tokenizer: object tokenizer \n",
    "    '''\n",
    "    X_train = train[text_name].values\n",
    "    ## 将评论数据，转转成sequences形式，评论中英文单词类别数最大为200 \n",
    "    # 分词器\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    # 将评论数据转换成sequences，[1,2,3]\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    # 统一长度\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    with open(tokenizer_path,'wb') as f:\n",
    "    pickle.dump(tokenizer, f)  #模型保存\n",
    "    return X_train,tokenizer\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(fname,tokenizer,embed_size):\n",
    "    '''\n",
    "    功能：词向量加载\n",
    "    :param fname: string 词向量路径\n",
    "    :param embed_size: int 预训练词向量的维度\n",
    "    :param tokenizer: object tokenizer \n",
    "    :return \n",
    "      embedding_matrix: matrix  词向量加载\n",
    "    '''\n",
    "    embeddings_index = open_file(fname)\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    \"\"\"\n",
    "       功能： Callback子类，用于打印ROC-AUC分数 \n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4obFCkEottB2"
   },
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8iSWACmg_Wq"
   },
   "outputs": [],
   "source": [
    "print('star CNN...')\n",
    "# 数据\n",
    "Y_train = train[class_names].values\n",
    "X_train,tokenizer = getTrainTokenizer(train,text_name,max_features,maxlen,tokenizer_path)\n",
    "embedding_matrix = getEmbeddingMatrix(emb_path,tokenizer,embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pF1N8yXierRA"
   },
   "source": [
    "## Basemodel 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4-B3Zgw6gkK"
   },
   "outputs": [],
   "source": [
    "class Basemodel():\n",
    "    '''\n",
    "    所有模型父类，该模型用于保存其他模型共性方法\n",
    "    '''\n",
    "    def __init__(self,maxlen,max_features,embed_size,embedding_matrix,model_save_path):\n",
    "        '''\n",
    "          功能：模型初始化\n",
    "          :param maxlen: int， 评论中的词种类数的最大限制\n",
    "          :param max_features: int，  最大特征数，现有数据中所有不同单词的种数\n",
    "          :param embed_size: int， 预训练词向量的维度\n",
    "          :param embedding_matrix: matrix， 词向量矩阵\n",
    "          :param model_save_path: string， 模型存储路径\n",
    "        '''\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.model_save_path = model_save_path\n",
    "        self.dropout = 0.5\n",
    "        self.optimizer = 'adam'\n",
    "        self.ouyput = 6\n",
    "\n",
    "    def build_model(self):\n",
    "        '''\n",
    "          功能：模型构建，每个模型都不一样\n",
    "        '''\n",
    "        pass\n",
    "  \n",
    "    def train(self,X_train,Y_train,train_size,batch_size,epochs):\n",
    "        '''\n",
    "          功能：模型训练\n",
    "          :param X_train: matrix， 训练集 X\n",
    "          :param Y_train: matrix， 训练集 标签\n",
    "          :param train_size: float， 训练集比例\n",
    "          :param batch_size: int， batch 大小\n",
    "          :param epochs: int 循环次数\n",
    "        '''\n",
    "        # 拆分训练集和验证集\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, train_size=train_size)  \n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=self.optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "        # 模型评估\n",
    "        RocAuc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "        early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                                  patience=0, verbose=0, mode='auto',\n",
    "                                  baseline=None, restore_best_weights=False)\n",
    "        # 训练\n",
    "        # verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "        hist = self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[RocAuc,early_stopping], verbose=2)\n",
    "        self.model.save(self.model_save_path) \n",
    "\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        '''\n",
    "          功能：模型预测\n",
    "        '''\n",
    "        model = load_model(self.model_save_path)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return y_pred\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-YH0loqgtG5"
   },
   "source": [
    "## Text-CNN 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqYy6mz5hJ_5"
   },
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OSdRQ0dhIac"
   },
   "outputs": [],
   "source": [
    "class TextCNN(Basemodel):\n",
    "    def build_model(self):\n",
    "        inp = Input(shape=(self.maxlen, ))\n",
    "        # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n",
    "        x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n",
    "        # 随机丢弃词，提高训练速度，提高词的独立性\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        # 转换维度，添加第三维，维度是1\n",
    "        x = Reshape((maxlen, embed_size, 1))(x)\n",
    "        # 卷积层，过滤器32,大小1*300\n",
    "        conv_1 = Conv2D(num_filters, kernel_size=(1, embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "        conv_2 = Conv2D(num_filters, kernel_size=(2, embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "        conv_3 = Conv2D(num_filters, kernel_size=(3, embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "        conv_5 = Conv2D(num_filters, kernel_size=(5, embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "        # 最大池化层\n",
    "        maxpool_1 = MaxPool2D(pool_size=(maxlen, 1))(conv_1)\n",
    "        maxpool_2 = MaxPool2D(pool_size=(maxlen - 1, 1))(conv_2)\n",
    "        maxpool_3 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_3)\n",
    "        maxpool_5 = MaxPool2D(pool_size=(maxlen - 4, 1))(conv_5)\n",
    "        # 连接最大池化层\n",
    "        z = Concatenate(axis=1)([maxpool_1, maxpool_2,maxpool_3,maxpool_5])   \n",
    "        # 压平\n",
    "        z = Flatten()(z)\n",
    "        # 随机丢弃，提高最后训练速度，防止因全连接层导致过拟合\n",
    "        z = Dropout(self.dropout)(z)\n",
    "        # 全连接层，输入六个值\n",
    "        outp = Dense(self.ouyput, activation=\"sigmoid\")(z)\n",
    "        self.model = Model(inputs=inp, outputs=outp)\n",
    "        self.model.summary(120)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Q1PxgshhZxc"
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTptfLtm4Wui"
   },
   "outputs": [],
   "source": [
    "text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'cnn_model.h5')\n",
    "text_cnn_model.build_model()\n",
    "text_cnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hckz23Y9tmC2"
   },
   "source": [
    "## Text-RNN 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "ScFB_oTSt_GG"
   },
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "U69_BF1It2pv"
   },
   "outputs": [],
   "source": [
    "class TextRNN(Basemodel):\n",
    "    def build_model(self,rnn_type):\n",
    "        rnn_type_dict = {\n",
    "          'LSTM': LSTM,\n",
    "          'GRU': GRU,\n",
    "          'CuDNNLSTM': CuDNNLSTM,\n",
    "          'CuDNNGRU': CuDNNGRU,\n",
    "        }\n",
    "        if rnn_type in rnn_type_dict:\n",
    "            layer_cell = rnn_type_dict[rnn_type]\n",
    "        else:\n",
    "            layer_cell = rnn_type_dict['GRU']\n",
    "        inp = Input(shape=(self.maxlen, ))\n",
    "        # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n",
    "        x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n",
    "        # 随机丢弃词，提高训练速度，提高词的独立性\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Bidirectional(layer_cell(128, return_sequences=True,dropout=self.dropout,recurrent_dropout=0.1))(x)\n",
    "        x = Dropout(self.dropout)(x)\n",
    "        x = Flatten()(x)\n",
    "        preds = Dense(self.ouyput, activation=\"sigmoid\")(x)\n",
    "        self.model = Model(inp, preds)\n",
    "        self.model.summary(120)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdbkzNDyulgn"
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9U3r_wAupIn"
   },
   "outputs": [],
   "source": [
    "# X_train,Y_train,train_size,maxlen,max_features,embed_size,embedding_matrix,model_save_path,batch_size,epochs\n",
    "text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rnn_model.h5')\n",
    "text_rnn_model.build_model('GRU')\n",
    "text_rnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Q-8jo2MCrZt"
   },
   "source": [
    "## Text-RCNN 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_YWbG3Zu3sv"
   },
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfHdwGqLu5zV"
   },
   "outputs": [],
   "source": [
    "class TextRCNN(Basemodel):\n",
    "    def build_model(self,rnn_type):\n",
    "        rnn_type_dict = {\n",
    "          'LSTM': LSTM,\n",
    "          'GRU': GRU,\n",
    "          'CuDNNLSTM': CuDNNLSTM,\n",
    "          'CuDNNGRU': CuDNNGRU,\n",
    "        }\n",
    "        if rnn_type in rnn_type_dict:\n",
    "            layer_cell = rnn_type_dict[rnn_type]\n",
    "        else:\n",
    "            layer_cell = rnn_type_dict['GRU']\n",
    "        inp = Input(shape=(self.maxlen, ))\n",
    "        # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n",
    "        x = Embedding(self.max_features, self.embed_size, weights=[self.embedding_matrix])(inp)\n",
    "        # 随机丢弃词，提高训练速度，提高词的独立性\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Bidirectional(layer_cell(128, return_sequences=True,dropout=self.dropout,recurrent_dropout=0.1))(x)\n",
    "        x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([avg_pool, max_pool]) \n",
    "        preds = Dense(self.ouyput, activation=\"sigmoid\")(x)\n",
    "        self.model = Model(inp, preds)\n",
    "        self.model.summary(120)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZjVLxGRu6IN"
   },
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCS6Yl0Hu739"
   },
   "outputs": [],
   "source": [
    "# X_train,Y_train,train_size,maxlen,max_features,embed_size,embedding_matrix,model_save_path,batch_size,epochs\n",
    "text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rcnn_model.h5')\n",
    "text_rcnn_model.build_model('GRU')\n",
    "text_rcnn_model.train(X_train,Y_train,train_size,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8W0zgz0oecpi"
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iB4gcjAefBP"
   },
   "source": [
    "### 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssYbch8JehPw"
   },
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "        功能：文本预处理\n",
    "        :param text: string 文本\n",
    "        :return\n",
    "         cleaned_text:  string 处理后文本\n",
    "         cleaned_text_len: int 文本长度\n",
    "    '''\n",
    "    # 大小写转换\n",
    "    lower = text.lower()\n",
    "    # 标点符号的处理\n",
    "    #string.punctuation中包含英文的标点，我们将其放在待去除变量remove中\n",
    "    #函数需要三个参数，前两个表示字符的映射，我们是不需要的。\n",
    "    remove = str.maketrans('','',string.punctuation) \n",
    "    without_punctuation = lower.translate(remove)\n",
    "    # 分词 \n",
    "    tokens = nltk.word_tokenize(without_punctuation)\n",
    "    # 去除停用词\n",
    "    without_stopwords = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    # 提取词干\n",
    "    s = nltk.stem.SnowballStemmer('english')  #参数是选择的语言\n",
    "    cleaned_text = [s.stem(ws) for ws in without_stopwords]\n",
    "    cleaned_text_len = len(cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text)\n",
    "    return cleaned_text,cleaned_text_len\n",
    "\n",
    "def getTestTokenizer(test,maxlen,tokenizer_path):\n",
    "    '''\n",
    "        功能：词向量加载\n",
    "        :param test: string test 测试集\n",
    "        :param maxlen: int， 评论中的词种类数的最大限制\n",
    "        :param tokenizer_path: string tokenizer 保存地址\n",
    "        :return \n",
    "          X_test:  matrix      测试数据序列化\n",
    "          tokenizer: object tokenizer \n",
    "    '''\n",
    "    # 分词器\n",
    "    with open(tokenizer_path,'rb') as f:\n",
    "        tokenizer = pickle.load(f) #模型载入\n",
    "    # 将评论数据转换成sequences，[1,2,3]\n",
    "    X_test = tokenizer.texts_to_sequences(test)\n",
    "    # 统一长度\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    return X_test,tokenizer\n",
    "\n",
    "threshold = 0.8 # 阙值 设定\n",
    "def model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names):\n",
    "    '''\n",
    "        功能：模型融合\n",
    "        :param threshold: float 阙值 设定\n",
    "        :param text_cnn_y_pred: list  text_cnn 预测结果\n",
    "        :param text_rnn_y_pred: list  text_rnn 预测结果\n",
    "        :param text_rcnn_y_pred: list text_rcnn 预测结果\n",
    "        :param class_names: list 类别列表\n",
    "        :return \n",
    "              result:  list      模型融合后的结果\n",
    "    '''\n",
    "    result = text_rnn_y_pred.copy()\n",
    "    for c in class_names:\n",
    "        result[c] = (3* text_cnn_y_pred[c] + 3 * text_rnn_y_pred[c] + 4 * text_rcnn_y_pred[c]) / 10\n",
    "        result.loc[result[c] > threshold, c] = 1\n",
    "        result.loc[result[c] <= threshold, c] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chUPdRe8Mnjn"
   },
   "source": [
    "### 行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrZn2o2DeiUI"
   },
   "outputs": [],
   "source": [
    "test = \"Fuck you, block me, you faggot pussy!\"\n",
    "test,test_len = text_process(test)\n",
    "test = [test]\n",
    "X_test,tokenizer = getTestTokenizer(test,maxlen,tokenizer_path)\n",
    "embedding_matrix = getEmbeddingMatrix(emb_path,tokenizer,embed_size)\n",
    "\n",
    "text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'cnn_model.h5')\n",
    "text_cnn_y_pred = pd.DataFrame(text_cnn_model.pred_line(X_test))\n",
    "text_cnn_y_pred.columns = class_names\n",
    "print(text_cnn_y_pred)\n",
    "\n",
    "text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rnn_model.h5')\n",
    "text_rnn_y_pred = pd.DataFrame(text_rnn_model.pred_line(X_test))\n",
    "text_rnn_y_pred.columns = class_names\n",
    "print(text_rnn_y_pred)\n",
    "\n",
    "text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rcnn_model.h5')\n",
    "text_rcnn_y_pred = pd.DataFrame(text_rcnn_model.pred_line(X_test))\n",
    "text_rcnn_y_pred.columns = class_names\n",
    "print(text_rcnn_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkYaLEOFNfYe"
   },
   "outputs": [],
   "source": [
    "result = model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MW8rZJ2x60Vg"
   },
   "source": [
    "### 批量预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZyl_1FP6gCO"
   },
   "outputs": [],
   "source": [
    "# 加载训练集和测试集数据\n",
    "test = pd.read_csv(data_path+'train.csv').fillna(' ')[0:50]\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "text_name = 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xx_HfxvCOrAb"
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4gwR_YuOYO-"
   },
   "outputs": [],
   "source": [
    "test['clean'],test['sent_len'] = zip(*test[text_name].apply(text_process)) \n",
    "test = test['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDwefexvOHHw"
   },
   "outputs": [],
   "source": [
    "X_test,tokenizer = getTestTokenizer(test,maxlen,tokenizer_path)\n",
    "embedding_matrix = getEmbeddingMatrix(emb_path,tokenizer,embed_size)\n",
    "\n",
    "text_cnn_model = TextCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'cnn_model.h5')\n",
    "text_cnn_y_pred = pd.DataFrame(text_cnn_model.predict(X_test))\n",
    "text_cnn_y_pred.columns = class_names\n",
    "# print(text_cnn_y_pred)\n",
    "\n",
    "text_rnn_model = TextRNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rnn_model.h5')\n",
    "text_rnn_y_pred = pd.DataFrame(text_rnn_model.predict(X_test))\n",
    "text_rnn_y_pred.columns = class_names\n",
    "# print(text_rnn_y_pred)\n",
    "\n",
    "text_rcnn_model = TextRCNN(maxlen,max_features,embed_size,embedding_matrix,model_path+'rcnn_model.h5')\n",
    "text_rcnn_y_pred = pd.DataFrame(text_rcnn_model.predict(X_test))\n",
    "text_rcnn_y_pred.columns = class_names\n",
    "# print(text_rcnn_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtdnD02NO-fQ"
   },
   "outputs": [],
   "source": [
    "result = model_fusion(threshold,text_cnn_y_pred,text_rnn_y_pred,text_rcnn_y_pred,class_names)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnJvwuJwFV2RMqGmuVvsS2",
   "collapsed_sections": [
    "i2iar67NfID4",
    "3gHGRJJYf7Sj",
    "sj7KK5rmgcTw",
    "2O8ljKhAggVq",
    "4obFCkEottB2",
    "I-YH0loqgtG5",
    "Hckz23Y9tmC2",
    "9Q-8jo2MCrZt"
   ],
   "name": "classifier.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
