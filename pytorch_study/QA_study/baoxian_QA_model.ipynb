{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @Author: King\\n    @Date: 2019.03.20\\n    @Purpose: 使用PyTorch实现Chatbot\\n    @Introduction:  下面介绍\\n    @Datasets: 百度的中文问答数据集WebQA 数据\\n    @Link : https://github.com/fancyerii/blog-codes\\n    @Reference : https://fancyerii.github.io/2019/02/14/chatbot/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.03.20\n",
    "    @Purpose: 使用PyTorch实现Chatbot\n",
    "    @Introduction:  下面介绍\n",
    "    @Datasets: 百度的保险问答数据集WebQA 数据\n",
    "    @Link : https://github.com/fancyerii/blog-codes\n",
    "    @Reference : https://fancyerii.github.io/2019/02/14/chatbot/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 下载地址： [百度网盘](https://pan.baidu.com/s/1cgYeIrJHAgb8D33H09Zc5w)\n",
    "2. 数据介绍： [github](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/baoxianzhidao/intro.ipynb)\n",
    "3. 数据概览： 8000 多条保险行业问答数据\n",
    "4. 推荐实验： FAQ 问答系统\n",
    "5. 数据来源： 百度知道\n",
    "6. 加工处理：\n",
    "    - 过滤了id、url、qid、reply_t、user字段\n",
    "    - 对question、reply做了脱敏处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包导入\n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍\n",
    "\n",
    "本教程会介绍使用seq2seq模型实现一个chatbot，训练数据来自Cornell电影对话语料库。对话系统是目前的研究热点，它在客服、可穿戴设备和智能家居等场景有广泛应用。\n",
    "\n",
    "传统的对话系统要么基于检索的方法——提前准备一个问答库，根据用户的输入寻找类似的问题和答案。这更像一个问答系统，它很难进行多轮的交互，而且答案是固定不变的。要么基于预先设置的对话流程，这主要用于slot-filling(Task-Oriented)的任务，比如查询机票需要用户提供日期，达到城市等信息。这种方法的缺点是比较死板，如果用户的意图在设计的流程之外，那么就无法处理，而且对话的流程也一般比较固定，要支持用户随意的话题内跳转和话题间切换比较困难。\n",
    "\n",
    "因此目前学术界的研究热点是根据大量的对话数据，自动的End-to-End的使用Seq2Seq模型学习对话模型。它的好处是不需要人来设计这个对话流程，完全是数据驱动的方法。它的缺点是流程不受人(开发者)控制，在严肃的场景(比如客服)下使用会有比较大的风险，而且需要大量的对话数据，这在很多实际应用中是很难得到的。因此目前seq2seq模型的对话系统更多的是用于类似小冰的闲聊机器人上，最近也有不少论文研究把这种方法用于task-oriented的任务，但还不是太成熟，在业界还很少被使用。\n",
    "\n",
    "## 数据介绍\n",
    "\n",
    "数据名称：百度的中文问答数据集保险问答 数据\n",
    "数据路径：F:\\document\\datasets\\nlpData\\conversation_dataset\\baidu_cn_WebQA\n",
    "格式：json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    工具包 end\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    工具包 begin\n",
    "'''\n",
    "import sys\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "'''\n",
    "    工具包 end\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 百度的中文问答数据集WebQA 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:最近在安邦长青树中看到什么豁免，这个是什么意思？\tanswer:您好，这个是重疾险中给予投保者的一项权利，安*长青树保障责任规定，投保者可以享受多次赔付，豁免等权益。也就是说不同轻症累计5次赔付，理赔1次轻症豁免后期所交保费，人性化的设计，无需加保费。\n",
      "question:和老婆利用假期去澳*探亲，但是第一次去不大熟悉，有没有相关保险呢？\tanswer:您好，HUTS保险中的乐游全球（探亲版）-慧择旅游保险澳新计划是澳*新西兰探亲专属保障，承保年龄可达90周岁，含有50万高额医疗保障，完全满足境外医疗保障需求，需要注意的是这款产品仅承保出行目的为境外探亲的人群，理赔时需提供相关签证或亲属关系证明等\n",
      "question:HUTS中有没有适合帆船比赛的保险，我男朋友这周就要开始了\tanswer:您好，水上运动比赛，尤其是带有奖金的比赛一般承保的公司比较少。不过，HUTS保险中的众行天下-水上运动保险赛事版B就是适合帆船等水上比赛的产品，含户外溺水保障，是水上运动专属定制的保障，意外住院有津贴，保障期限灵活可选，还可以投保有奖金的赛事，您可以根据情况看看。\n",
      "question:计划端午节和男朋友自驾去九*山，买保险三天要多少钱？\tanswer:您好，端午出行的人比较多，而且自驾存在一定风险，所以有保险意识还是很好的。考虑到价格以及保障内容等相关因素，您可以看看HUTS保险中的畅玩神州-慧择旅游保险计划三，适合驾驶私家车走南闯北国内旅游，自驾意外累计赔付,承保的范围也较为广泛，适合带家人出游，保障全面，三天仅需75元，性价比还是蛮高的。\n",
      "question:计划端午节和男朋友自驾去九*山，买保险三天要多少钱？\tanswer:不到10块钱………………\n",
      "question:端午我们准备要举行赛龙舟，说是要份保险，什么好\tanswer:您好，赛龙舟是一项比较传统的活动，很有意义。不过由于是在水上活动，建议安全保障还要做足，HUTS保险中有针对水上运动风险特别定制的众行天下-水上运动保险，可以针对这种赛事进行保障，含有意外住院有津贴以及一系列保障，性价比较高，关键是费用也比较实惠。\n",
      "question:老婆买了安*长*树，她在网上投保的，以后缴费怎么办\tanswer:您好，这点是不用担心的。投保后保险公司会在约定的保险费交纳日从消费者购买时填写的银行账号中划扣当期应交的安*长青树重疾险的保险费，所以您老婆是不用亲自去保险公司缴费的。\n",
      "question:听说huts可以申请医疗垫付的，什么情况下可以批呢？\tanswer:您好，是可以的。该医疗垫付目的便是解决投保者的切实困难，可以说是很人性化的一项保障了。在以下几个情况下可以申请医疗垫付：①发生重大意外或突发急性病；②所投保保险涵盖该医疗费用补偿责任；③医疗费用高客户无法承担\n",
      "question:和团队去北极探险，有没有针对这方面的HUTS保险呢\tanswer:您好，去北极探险本身就存在一定的风险，建议选择专业的装备以及在专业人士的陪同下进行。至于保险，市面上关于此类的保险并不多，不过HUTS保险中却有一款专门针对南北极旅游的定制产品，保障内容充足，户外伤害、医疗保障甚至的紧急救援都具备，详情可以多了解下。\n",
      "question:去欧*出差，随身的物品比较多，有没有保障这个的保险\tanswer:您好，去一个人生地不熟的地方最怕的就是行李丢失，很麻烦也很苦恼。目前关于境外的保险比较多，但是您可以看看HUTS保险中针对欧*商务出差必备，高额随身财产保障定制的乐游全球（境外直付版）全球顶级款。对出差人士的随身物品保障很充足，不仅仅提供全球医疗直付，含有高额医疗补偿保障以及智能手机及电脑保障，还对行程阻碍、意外伤害等均有保障。\n",
      "question:老公喜欢抽烟，但是听说这样就投不了安*长青树了，是真的吗，什么意思？\tanswer:您好，由于重疾险的投保要求比较严格，会对投保人的一些行为习惯有所规定。保险公司一般会限制抽烟酗酒的人投保重疾险的。如果您老公每天抽烟的数量较多，是不建议投保安邦长青树的，因为不符合产品健康告知中的条件，保险公司核保通过不了，出现拒保情况。建议您老公的情况可以选择意外险。\n",
      "question:老公喜欢抽烟，但是听说这样就投不了安*长青树了，是真的吗，什么意思？\tanswer:依哥们儿的经历来看，应该说这是一个循续渐进的过程。上学时，看到别人抽烟吐烟圈貌似蛮酷，于是就开始跟着学习抽烟……时间一常，想戒也难了。另外，不可否认的是，特定情况下对于某些人，吸烟有助于思考，确实可以适当缓解紧张情绪。\n",
      "question:这个周末天气不错，看着心情舒坦，准备和朋友户外拓展，什么保险好\tanswer:您好，HUTS保险中的众行天下-拓展训练保险针对各类户外活动定制的产品。它的承保范围比较官，33种热门拓展运动均在列，并且对投保者的年龄规定也比较松，适合1--65周岁的人群投保，性价比高，必要时候还可以提供第一现场救援。\n",
      "question:老婆已经买了其他的重疾险，还要投康惠保，能行吗\tanswer:您好，这个是没有冲突的，还是可以投保的。不过您需要注意的是，但康惠保有最高保额限制。被保险人在百年人寿持有保单重疾风险保额，出生满28天-40周岁不超过50万，41-50周岁不超过30万，51-55周岁不超过10万。\n",
      "question:老婆已经买了其他的重疾险，还要投康惠保，能行吗\tanswer:这种问题根本没法回答的。问题好比你说你爱人已经吃了一碗饭了，还想再吃一碗能行吗？你没说你爱人的饭量，我怎么知道她还能不能再吃一碗。也就是我无从知道你的家庭保险需求中你爱人的保险需求。买保险是大事，买不好，祸害你一生。\n",
      "question:我和老婆单位都买了社保，现在她又要买什么哆啦A保，有必要吗\tanswer:您好，社保是属于最为基础的保障，而它和重疾险两种保险的理赔方式不同，功能相互补充。这款产品是确诊患重大疾病就可以申请理赔，经核定属于保险责任的，保险公司会按照合同的约定进行保险金的一次性赔付，不管您实际治疗费用是多少，也不需要提供发票和费用清单，所获得的保险金可以用于治疗或弥补患病后的收入损失。另外，如果您还有社保或者其他公司的医疗保险，还可以凭医疗单据进行费用报销。\n",
      "question:我和老婆单位都买了社保，现在她又要买什么哆啦A保，有必要吗\tanswer:社保只是保险基础，哆啦a宝属于商业保险，很有必要购买。至于买什么类型商业保险，建议先买重疾、医疗、意外等等\n",
      "question:我和老婆单位都买了社保，现在她又要买什么哆啦A保，有必要吗\tanswer:景生产线屯阶上部狼\n",
      "question:小*买了份哆啦A保，说是没有纸质保单，怎么回事\tanswer:您好，这个不用担心的，投保哆啦A保完成后，电子保单会直接发送至您小叔的邮箱的。如果担心其他问题，保证合法权益，可通过访问弘康人寿保险网址或拨打弘康人寿客户服务热线对所投保保单的相关信息进行查询。\n",
      "question:huts保险中有没有关于赛事的？怎么好多保险户外赛事都不保障\tanswer:您好，是有的。HUTS保险中有专为部分高风险赛事进行定制的“赛事安心”保险计划综合运动会计划高风险运动计划，是业余高风险赛事专享保障，特含紧急医疗运送保障，可承保有奖金的赛事，保障全面。至于目前很多保险不保户外赛事，主要是因为风险系数比较高，而且多数保险是不承保含有奖金的赛事的，但是以上的那款保险是可以保障的。\n",
      "question:请问下，乙肝能不能买投康惠保？会被拒绝吗？\tanswer:会的，买不了\n",
      "question:请问下，乙肝能不能买投康惠保？会被拒绝吗？\tanswer:您好，乙肝病毒携带者客户可申请核保。如果核保通过就可以投保，若是不符合健康告知，可以加费投保或者选择康惠保以外的产品。\n",
      "question:我空闲时间不多，老婆带着儿子两个人去周边玩一天，有什么保险呢?\tanswer:您好，一般情况下周边游玩风险并不大，不过买份保险也是很有必要的。由于您妻子和孩子的游玩时间并不长，只有一天，可以看看HUTS保险中一日游-慧择旅游保险，主要针对1日游出行人群，拥有10万意外伤害保障，保费很便宜仅需1元。另外，天气逐渐转热，注意防暑。\n",
      "question:我空闲时间不多，老婆带着儿子两个人去周边玩一天，有什么保险呢?\tanswer:有出行险的啊旅行险\n",
      "question:这个周末儿子有场拳击比赛，很担心安全，什么保险好\tanswer:您好，拳击比赛存在一定风险的，所以对此类运动承保的并不多，不过您可以看看HUTS保险有专为部分高风险赛事进行定制的“赛事安心”保险计划，是业余高风险赛事专享保障，特含紧急医疗运送保障，可承保有奖金的赛事。建议可以根据您儿子的情况，参考参考。\n",
      "question:我女朋友比较马虎，她这次一个人去美*玩，想给买份保险\tanswer:您好，意外风险无处不在，买份保险防止意外发生很重要。您女朋友一个人去美*，路途比较遥远，建议可以看看HUTS保险中的乐游全球。主要是，可以承保部分高风险运动，对于旅途中比较常见的行李丢失、意外伤害医疗、航班延误等等都有保障。\n",
      "question:为什么要选择职业，我儿子还在上中学，怎么填安邦长青树的职业？\tanswer:您好，选择职业，是因为重疾险对职业有一定要求的，一般情况下重疾会允许1-4类的职业投保，安*长*树也是。超过1-4类的职业一般会限制投保的。不过您孩子还在上学，可以按其实际上学情况选择学龄前儿童或学生。\n",
      "question:哆啦A保对年龄有没有限制，10岁的小女孩可以买吗\tanswer:您好，这个重疾险对年龄是有一定的限制的，保险规定，哆啦A保适合出生满30天-55周岁(含30天、55周岁)投保，所以在这个年龄段的是可以买，10岁的小女孩也是能投保的。另外，哆啦A保保障160种疾病，不怕健康无守护，覆盖全周期的隐患，并且分组多次赔付无忧虑，而且智能核保更方便，功能完善人性化，很适合投保。\n",
      "question:是不是我在犹豫期退保了安邦长青树，就不给我保障了？\tanswer:您好，您在犹豫期申请解除保险合同，需要填写解除合同申请书并向保险公司提供您的有效身份证件。自保险公司收到解除合同申请书时起，保险合同终止。因为是在犹豫期，所以保险公司会无息退还您的保费，但是合同解除了，您自然也就不能享受保险的权利了，所以不会再对您给予保障的。\n",
      "question:是不是我在犹豫期退保了安邦长青树，就不给我保障了？\tanswer:是的\n",
      "question:是不是我在犹豫期退保了安邦长青树，就不给我保障了？\tanswer:当然\n",
      "question:想问下，如果重疾理赔后身故，哆啦A保还有身故赔偿金吗\tanswer:您好，不赔的。因为已经进行了重大疾病理赔，保险责任规定，重大疾病理赔后身故的，不给付身故保险金，所以是没有的。\n",
      "question:想问下，如果重疾理赔后身故，哆啦A保还有身故赔偿金吗\tanswer:没有\n",
      "question:重疾险的缴费期限有什么用处，康惠保的缴费期限好不好\tanswer:一般来说缴费期限越长越好的，因为一般重疾都有轻症重疾豁免权，那么如果在缴费期限内出险，就可以豁免后面的保费，那当然是缴费期限越长，经济压力越小嘛\n",
      "question:重疾险的缴费期限有什么用处，康惠保的缴费期限好不好\tanswer:您好，正常来说，缴费期限是越长越好的，因为缴费期限越长，杠杆率越高，性价比也就越高。康惠保能做到性价比最高的30年缴费，还可以保障终身，在同类产品中是比较出色的。\n",
      "question:安*长*树*保*不*扣费，怎么做呢\tanswer:会扣，只能退给你保单的现金价值，和你已交的保费比，差很多。\n",
      "question:怎么儿子去留学前三个月没有医疗保障，怎么办？\tanswer:可以买国际型的医保啊\n",
      "question:怎么儿子去留学前三个月没有医疗保障，怎么办？\tanswer:您好，留学生前期没有医疗保障是一个比较常见的问题，因为学校还没有给学生办理相关医疗保障，需要学生提前补充，加上国外的医疗费用一般比较高，所以这个时候的商业保险显得尤为重要了。建议考虑下HUTS保险中的前程似锦（基础版）-慧择留学保险，这款产品是海外游学专属保障，价格不贵，保障全面，1--90天任意投保，可以满足前三个月学校医疗保障的空白，针对性很强。\n",
      "question:谁知道,在海外务工,有什么保险可以保障?\tanswer:您好，HUTS保险中的海外务工（1-4类）-慧择旅游保险计划C，是海外工作派遣的专属选择，保险期间1至12月可选。另外可以承保年龄在18-65周岁，保障较为全面。\n",
      "question:去年下半年大叔买了哆啦A保，现在原位癌住院了，能不能理赔？\tanswer:您好，这属于哆啦a保55种轻症的其中一种，保险公司给付轻症疾病保险金，保险合同继续有效。也就是如后期罹患其他54种轻症和105种重疾仍可获得赔付的，所以您并不用担心。\n",
      "question:当前有没有可以针对第一现场救助的旅游保险呢\tanswer:您好，虽然目前市面上很多旅行险中都表示可以进行紧急救援，但是基本上提供的都是第二现场救援，而第一现场救援并不多。不过目前慧择就针对这部分人群专门定制这类保险，实现及时援助远比事后补偿更重要的切入点，让人们旅行无忧，真正做到陪同服务和必须赶往现场的救援服务。\n",
      "question:为什么我投保的弘康哆啦A保没有发票呢？\tanswer:您好，并不用担心，它是提供电子发票的。电子发票获取路径：关注弘康人寿官方微信：弘康人寿—注册登录—我的保单—点开保单详情页面—点击左下小机器人按钮—选择“电子发票”—确认您的电子邮箱无误—下载电子发票到邮箱中即可。（注：电子发票自动生成条件为保单状态有效、已过犹豫期。）\n",
      "question:康惠保是选择终身的好还是到70岁保障期好，为什么？\tanswer:您好，康惠保的保障周期可以选择保障到70岁或者终身，如果投保人经济压力大的话可以选择定期70岁的投保方案，价格更低，杠杆率更高。考虑到我国人均寿命也就70多岁，因此家庭不富裕的投保人完全可以选择这样的投保方式来获取更高的杠杆收益。保障到70岁，如果70岁前没有出险，之后合同终止；现金价值归零，且70岁之后没有重疾保障；保障到终身，终身有重疾保障，如果一辈子没有身患重疾的话，在身故之后可以拿到保单现金价值，现金价值比较可观。\n",
      "question:康惠保是选择终身的好还是到70岁保障期好，为什么？\tanswer:您好，康惠保重大疾病保险的产品形态，包括主险和附加险。主险（康惠保重大疾病保险）；附加险（康惠保特定疾病保险）。主险（必选）包含重症责任，附加险（可选）包括轻症及轻症保费豁免责任。这款重疾险是一款典型的消费型重疾险：保费较低，杠杆率较高，但无返还功能，身故责任保障给付现金价值。\n",
      "question:周末赔闺女体验下野外运动，有没有亲子保险\tanswer:您好，是有的，HUTS保险中父母孩子共同参与亲子类活动可以参考慧游游-慧择旅游保险计划一，保障全面，价格不高，可以说是性价比很高的一款产品了。\n",
      "question:康惠保在北*买的，现在人在山*，出险了是否可以理赔？\tanswer:您好，常住地在大*、湖*、河*、辽*、**、河*、黑**、安*、山*、**、四*、福*、陕*、内*古、吉*、**、浙*、山*、广*、重*这些地区方可投保，不影响理赔。如果迁入地有分支公司的支持保单迁移的保全操作，到柜面申请即可。如果没有迁移，您只是暂时在山*，理赔的时候会安排就近的分支公司提供服务。\n",
      "question:康惠保在北*买的，现在人在山*，出险了是否可以理赔？\tanswer:只要达到保险理赔要求就没问题的。\n",
      "question:出国玩比较担心护照等随身物品安全，什么保险可以投？\tanswer:您好，意外风险无处不在，防患于未然很有必要，在这种情况下建议可以了解下HUTS保险中的乐游全球（境外直付版）-慧择旅游保险钻石计划承保部分高风险运动，保额也很高，仅需100元，性价比很高。另外，关于随身物品、证件行李等都在保障范围内。\n",
      "question:出国玩比较担心护照等随身物品安全，什么保险可以投？\tanswer:对于中*游客而言，出*旅游固然可以欣赏异*风情，但是不同的语言和不熟悉的环境会加大自身安全风险，因此，您在出*之前给自己上一份合适的出*旅游保险是必要的。出*旅游买什么保险好建议您选择针对性强的出*旅游保险，投保时可以根据自己具体前往的*家以及实际保障需求来选购。一般来说，购买出*归属地的出*旅游保险性价比更高。投保前您需要注意以*几点：1.根据出行目的*的消费水平选择合适的保额。2.购买的境外旅游保险需要覆盖您的全部行程。使得自己的旅行全程在保障之中。3.在购买出境旅游保险时，要注意是否包括*际紧急救援服务。由于*内保险公司的网点很难铺到*外，所以出境旅游险产品通常是与*际紧急救援公司合作。无论是游客在外遗失钱包，还是护照丢失等，都可以致电救援热线，一些大的*际保险公司还专门提供汉语服务。4.选择合适的投保平台，在慧择网上购买，您可以对比多家保险公司的产品，进而选择最合适自己保障和出行需要的产品。投保也非常简单方便，在慧择网首页，免费注册会员，登陆后选择中意的产品，选择保障期限，即可立即购买。价格便宜，整个投保过程清晰透明，免去了中介代理的层层麻烦，保费更低，自主性更强。出*旅游买保险，您首先要要了解出*旅游保险的种类，然后再按需选购，慧择网是提供专业境外旅游保险的投保平台，欢迎您前来选购。美*“乐悠游”境外旅行保障计划二华泰“安*天*”*际旅游保障计划经济款展开\n",
      "question:出国玩比较担心护照等随身物品安全，什么保险可以投？\tanswer:可以了解一下平安的\n",
      "question:投了安*长*树几个月后，发现得了脑膜炎，这个可以报销吗\tanswer:您好，这个是可以报销的。因为脑膜炎属于安邦长青树保障的38种轻症的其中一种，保险公司给付轻症疾病保险金，并且保险合同继续有效。意思就是，如果后期罹患其他37种轻症和80种重疾仍可获得赔付。\n",
      "question:什么是小额快赔，在huts保险中看到的\tanswer:您好，小额快赔服务是HUTS保险针对3000元金额以内小额理赔的绿色快速通道，目的是为了让消费者更加快捷地获得理赔。优势主要体现在以下三个方面：①比非闪赔产品更少的材料；②绿色通道闪电般审核速度；③全程线上操作更便捷。\n",
      "question:请问下澳*利*公差一个星期，有什么保险保障全\tanswer:您好，当前境外商旅人士增多，也相应地对保险安全有了要求。澳*利*出差的话，建议参考下澳*利*商务出行必备，财务保障充足的乐游全球（境外直付版）-慧择旅游保险。这是一款针对商旅人士的保险，可以提供境外医疗直付，承保部分高风险运动，含有既往症急救补偿，保障1--5天仅需100元。\n",
      "question:请问下澳*利*公差一个星期，有什么保险保障全\tanswer:有个叫什么境外旅行什么的，1星期不贵；也有便宜的，几元。\n",
      "question:想要去欧*游玩，不晓得什么样的保险比较合适\tanswer:您好，欧*相对而言比较遥远，出行买份保险保障安全是很有必要的。首先，安全第一，可以看看HUTS保险的安*天下-慧择旅游保险。30万高额医疗保障，随身财产有保障，特含紧急医疗运送保障，承保年龄也较为广泛，基本上大多数人都是可以投保的。\n",
      "question:想要去欧*游玩，不晓得什么样的保险比较合适\tanswer:将赴吴*登乐游原一绝(杜*)\n",
      "question:什么是轻症豁免，在康惠保中看到的？\tanswer:您好，相对于重疾，其实轻症大病更为常见和多发，同时它往往是重疾的潜伏阶段，需要早期发现和积极治疗，才能防止往重疾转变。轻症是和重大疾病相对应的，简单来说，就是重大疾病前期较轻的疾病，对人影响不大，通过接受治疗恢复健康。所谓的保费豁免是指在保险合同规定的缴费期内，投保人或被保人达到某些特定的情况（如身故、残疾、重疾或轻症疾病等），由保险公司获准，同意投保人可以不再缴纳后续保费，保险合同仍然有效。\n",
      "question:安*长*树对各个地区可以投保的最高保额有限制吗？\tanswer:您好，安*长*树目前对A类地区及B类地区设有不同的保额限制。针对A类地区（**、上*、广*、深*、浙*、**），18-40岁保额最高可选50万，41-50岁最高可选25万，51-55岁最高可选8万；B类地区（河*、河*、山*、吉*、黑**、辽*、湖*、湖*、四*、**、山*、安*、天*），18-40岁保额最高可选33万，41-50岁最高可选16万，51-55岁最高可选4万。\n",
      "question:安*长*树对各个地区可以投保的最高保额有限制吗？\tanswer:在允许销售的地区是一样的\n",
      "question:本周公司团建，去户外玩，huts保险有什么比较好\tanswer:您好，一般公司团建安全系数还是挺高的，但是由于人员比较多，还是有份保险比较好。比如说HUTS保险中的众行天下-拓展训练保险，很符合团建素拓等各类团队活动定制的产品。另外，可以承保33种热门拓展运动，保障的年龄范围也广泛，适合1--65周岁的人群投保，其保障内容一般满足公司团建需求。\n",
      "question:哆啦A保享有多次赔付，但患重疾理赔后，轻症和身故责任还有吗？\tanswer:您好，在条款中有明确说明，首次给付重大疾病保险金后，身故保险金的保险责任与轻症疾病保险金的保险责任均终止。简言之，重疾理赔和身故理赔只赔付其中一种。\n",
      "question:哆啦A保享有多次赔付，但患重疾理赔后，轻症和身故责任还有吗？\tanswer:重疾赔付后，轻症，高残，身故责任终止，只能等下次大病理赔了\n",
      "question:和老婆去德*旅游，办签证要买保险，什么比较好？\tanswer:您好，HUTS保险中有一款产品在满足签证的基础上提供全面保障，那就是安*天下-慧择旅游保险欧*顶级款，30万高额医疗保障，随身财产有保障，特含紧急医疗运送保障，承保年龄也较为广泛，适合1--80周岁的人群投保，服务的人群比较广泛，可以看看。\n",
      "question:有谁知道，安*长*树出险了需要提供哪些医院证明？\tanswer:您好，该保险对医院是指定的，需要到合同约定的医院(境内一般是二级或二级以上公立医院)进行就诊治疗。需要提醒投保者的是，提醒主治医生使用医保范围内用药、诊疗项目及服务设施。另外，就诊的同时请妥善保存病历(包含首诊病历)、原始收费凭证、处方、诊断证明、检查化验报告、住院证明等就医相关材料，方便向保险公司办理索赔申请。\n",
      "question:姐姐投了安*长*树，为啥只有电子保单？\tanswer:您好，网上投保保险公司是只提供电子保单的，不过您不用担心，电子保单同样有效的。根据《中华人民共和*合同法》相关规定，数据电文是合法的合同表现形式，电子保单与纸质保单具有同等法律效力。另外，根据中*保*会《人身保险业务基本服务规定》第十五条规定，安邦人寿会在保单生效后对投保人进行电话回访，回访只是对于保险产品的风险进行提示确认的。\n",
      "question:女儿去欧*留学，刚开始几个月医保卡没有下来，什么保险可以补充？\tanswer:您好，欧*留学的话，刚开始几个月可能对留学生的保障是不够全面的，所以选择一款合适的保险进行补充是很有必要的。建议您可以看看HUTS的前程似锦（留学）-慧择旅游保险。主要考虑到下面几点：1、高额医疗补偿费用，专为发达国家留学定制，对于欧*这样的高消费国家而言，显得很有必要；2、涵盖个人责任及留学中断方方面面；3、含紧急医疗运送服务，孤身在外有照应。另外，对于一些证件的遗失等问题，也是提供理赔的。\n",
      "question:心肌桥可以投健康保险吗\tanswer:您好，大多数保险公司对于心肌桥患者都是不予投保健康险的，而且类似于康惠保这样的重疾险，在消费者购买保险之前都会有比较详细且严格的核保程序。所以建议您可以考虑意外险等相关保险的基础保障，健康险的话一般会拒保的，除非保险合同有规定，除外情况可以投保。\n",
      "question:我女朋友是个导游，比较担心安全，请问什么领队责任险可以考虑？\tanswer:您好，导游一般肩负着整个团队的安全责任，买份保险还是很有必要的。HUTS保险中充足的领队第三者责任保障的产品有众行天下-领队责任保险，高额领队责任保障，承保部分高风险运动，完全出于对于领队用户的考虑量身定做的，可以参考参考。\n",
      "question:请问下，安*长青树有没有等待期，长不长？\tanswer:您好，一般重疾险都有一个等待期，安*长*树也不例外，是有的。它的等待期为90天。如果等待期内发生合同约定的重大疾病、达到疾病终末期阶段、身故、全残，将无息返还已交保费，合同终止。因意外伤害导致上述保险事故的无等待期限制。\n",
      "question:请问下，安*长青树有没有等待期，长不长？\tanswer:沈*病重去世。臧姑亦悔不当初，决定重新做人。\n",
      "question:去韩*游玩，有什么HUTS保险比较可靠\tanswer:您好，可以考虑HUTS保险中的乐*日*-慧择旅*保险计划，40万医疗运送保障以及花粉过敏急性发作，产品设计可以说相当人性化了。5天仅需60元，性价比也是相当高的了，可以作为一个不错的参考产品了。\n",
      "question:打算周末和老公带儿子来场亲子游，有什么HUTS保险比较可靠？\tanswer:您好，是有的。慧游游-慧择旅游保险就是专门针对亲子游的保险。它的承保范围涵盖市内外，是亲子游首选；另外，还提供紧急医疗运送服务，大人小孩均有保障，它的最大特色，便是包括协助儿童送返费用补偿的了，您可以多了解下。\n",
      "question:消费型重疾险适合哪些人群\tanswer:（1）您好，所谓消费型重疾是相对于返还型重疾而言的一类重疾险种，其主要特点是到期后不会返还保费。比如说比较常见的康惠保、哆啦A保等都是这两年比较受欢迎的消费型重疾险。（2）另外，消费型重疾险的特点是性价比高，较低的保费就能获得很高的保额。（3）故而，消费型重疾险适合的人群：A、希望把预算用在风险保障上，而不是和理财、储蓄等功能混为一谈的人群身上。B、保险预算较少的家庭或个人。\n",
      "question:儿子去外国之前买了HUTS，这个在国外可以提供紧急救援吗？\tanswer:b\n",
      "question:儿子去外国之前买了HUTS，这个在国外可以提供紧急救援吗？\tanswer:您好，同样有的。HUTS境内旅行紧急援助服务已在国内覆盖各省市，全球救援服务覆盖达200个国家。目前慧择已与优普救援、美亚Tr#######rd、SOS等知名救援机构达成合作。\n",
      "question:下周儿子会有场小学生足球比赛，比较担心安全，可以买什么保险？\tanswer:不用买，踢足球很安全的，哪有那么容易出意外。其实学校给每个学生都买了意外险的。\n",
      "question:儿子计划去美*游学三个月，其他都准备好了，现在想找款合适的保险\tanswer:您好，去美*游学的话保险还是很有必要的。您可以考虑下HUTS是我境外游学-慧择旅游保险，这款产品特别适合医疗额度充足，对于外出游学的朋友保障力度十分强。1、全球留学的高性价比之选；2、三个月仅需190，适合短期留学出行；3、紧急医疗运送服务，保障学子安全。另外，它的主要特色有：海外游学专属、高额意外身故保障、意外医疗保障、突发急性病医疗保障。\n",
      "question:儿子计划去美*游学三个月，其他都准备好了，现在想找款合适的保险\tanswer:就是中*平安和中*人寿，比较合适\n",
      "question:儿子计划去美*游学三个月，其他都准备好了，现在想找款合适的保险\tanswer:买个极短险搞个3个月即可几百块\n",
      "question:请问huts可以承保30人的野外跑步赛事吗？\tanswer:您好，是可以承保的。HUTS中有适合跑步运动，包含猝死保障的产品--众行天下-跑步运动保险运动版C，是跑步运动专属保障，有猝死/急性病身故保障，特含突发急性病医疗，专门针对野外跑步比赛的。\n",
      "question:请问下康惠保的缴费年限有哪些？\tanswer:您好，百年康惠保保障年限可以选择至70岁和终身的，缴费年限有10年、15年、20年以及30四种选择。\n",
      "question:邻居大叔买了安*长*树，半年后查出有原位癌，让我查查赔吗？\tanswer:您好，不用担心的，原位癌属于38种轻症的其中一种，保险公司将按保额的20%也就是6万元给付轻症疾病保险金，保险合同继续有效。也就是如后期罹患其他37种轻症和80种重疾仍可获得赔付。\n",
      "question:因为工作原因，乘坐飞机比较多，有哪些HUTS保险产品是适合坐飞机投保的?\tanswer:您好，像飞机这种交通工具，一旦出事，后果将不堪设想。所以提前购买HUTS保险还是很有必要的，以防万一嘛。至于产品的选择，建议保额越高越好，最好附加购买航班延误保障。具体的HUTS产品可以参考下“慧飞行”航空意外保障全年计划：1、高性价比：全年五百万保障仅需50元；2、高额保障：保额叠加，最多可购买2份；3、适用人群：适合经常乘飞机出行的商务人士和旅行人士。大家也可以去网站上了解下。\n",
      "question:最近看到HUTS的小额快赔，有什么好处呢？\tanswer:您好，小额快赔服务是针对3000元金额以内小额理赔的绿色快速通道，非常快速便捷。优势主要体现在以下三个方面：①比非闪赔产品更少的材料；②绿色通道闪电般审核速度；③全程线上操作更便捷。\n",
      "question:爱好喝点小酒，可以投长青树吗\tanswer:您好，如果饮酒较为频繁，不建议投保安邦长青树的，因为不符合产品健康告知中的条件，保险公司核保通过不了，而且大部分重疾险一般对爱好抽烟、饮酒的人士都是拒保的。不过，您可以优先考虑意外险做基础的保障。\n",
      "question:老板安排我去美*出差一个月，什么保险比较安全\tanswer:您好，具体可以考虑HUTS保险中针对美*展会和商务必备，高额随身财产保障的产品---乐游全球（商旅无忧版）-慧择旅游保险全球顶级款，提供全球医疗直付，含有高额医疗补偿保障以及智能手机及电脑保障。\n",
      "question:请问下什么是轻症，为什么原位癌却按照安邦长青树的轻症理赔？\tanswer:您好，保险行业协会没有对轻症有统一的标准，也造成了不同公司对轻症的数量、种类甚至定义都有一定的不同。给大家列举比较常见的几种轻症吧：轻微脑中风、不典型的心肌梗、较小面积的III度烧伤、视力严重受损、冠状动脉介入手术、主动脉内手术、脑垂体瘤、脑囊肿、脑动脉瘤及脑血管瘤等等。原位癌虽然是癌症的早期阶段，但是治疗起来相对简单，一般手术切除病灶即可，5年生存率高，所以安*长*树将原位癌纳入轻症范围，按照轻症理赔。\n",
      "question:最近俱乐部有个羽毛球比赛，给参赛者选择什么huts保险好？\tanswer:您好，现在户外爱好者、学校、相关单位、团体组织的竞赛越来越多，竞赛过程中发生意外风险的机率也随之增加。另外，目前大多数保险并不承保竞赛类，特别是带奖金、奖品的竞赛性质活动。因此，如果参加相关赛事活动，一定要选择专门的HUTS业余赛事保险，来承保业余普通体育竞赛(包括田径、游泳、球类运动)中的意外事故。可根据具体比赛项目风险特点，有针对性的选择承保该项比赛的赛事保险，以策万全。需要大家注意的是，只有赛事保险才能保有奖金的赛事活动，普通意外险是不保的。如果投保普通意外险，理赔的时候被发现，可是会被拒赔的。\n",
      "question:请问下攀岩徒步等户外运动，买什么保险较好？\tanswer:您好，HUTS保险中承保攀岩、徒步溯溪过程中各类风险的众行天下-综合户外运动保险计划二，适合1-65周岁的人群投保，承保高风险活动，可以保障溯溪过程中的各类风险，保障期限灵活可选，是一款很不错的产品。\n",
      "question:有谁知道康惠保申请得严不严格？\tanswer:您好，对于重疾险产品来说核保相对还是很严格的，但是现在很多重疾险产品都是智能核保，很方便，操作也简单，大大节省了用户的时间。不过需要提醒您的是，虽然康惠保审核比较严格，但是您既可以线上核保，也可以人工核保的，比较便利。\n",
      "question:刚投保的康*保，说还在等待期，什么意思\tanswer:您好，所谓的等待期就是观察期，防止投保人逆选择投保。一般是90天或者180天，如果在这个期限内发生风险，保险公司一般是退还康惠保的保费的。\n",
      "question:我是个导游，近期带队很频繁，有什么保险可以投的？\tanswer:您好，根据您的职业角色，您可以了解下H*T*保险中的众行天下-领*责任保险计划一，含有高额领*责任保障，承保部分高风险运动。另外，购买这款产品后即可免费开通H*T*领*会员，自动获取专属服务卡，这么高额的真的不多见，不妨参考下。\n",
      "question:我是个导游，近期带队很频繁，有什么保险可以投的？\tanswer:1年期的意外险、医疗险都可以投。需要出国的话，记得选择在国外也能保障的产品。\n",
      "question:请问下HUTS旅游险和一般的旅游保险有什么不一样的？\tanswer:您好，HUTS保险关于旅游类的保险产品，它针对不同细分场景的风险特点，设计出了侧重点不同的保障责任，为用户提供相关场景下最需要的专属保障。HUTS将旅行户外保险分为7个大类，含户外运动、观光旅游、体育运动、出境旅游、探亲、商务出行、留学等，并细分出46个场景，包括一日游、自驾游、欧*旅游、户外运动、高风险运动等。HUTS针对不同细分场景的风险特点，设计出了侧重点不同的保障责任，为用户提供相关场景下最需要的专属保障。和一般的旅游类产品最大的不同在于，HUTS保险可以提供第一现场紧急救援。\n",
      "question:这个周末去周边玩，只有一天的时间，要不要买保险的\tanswer:您好，进行周边游玩的话，可以选择一份一天的保险，既划算也比较合理，保障安全。目前HUTS保险一日游，就专门针对一日游的朋友设计的，价格很实惠，一天一份只需要1元，但是保障却很足，高达10万。\n",
      "question:这个周末去周边玩，只有一天的时间，要不要买保险的\tanswer:如果是旅行社组织的短途游，一般合同里含保险如果是自驾游的话，建议平时可以购买个长期生效的意外险什么的。\n",
      "question:这个周末去周边玩，只有一天的时间，要不要买保险的\tanswer:我觉得没有自杀倾向，应该不需要？\n",
      "question:请问下哆啦A保重疾险,买多久的比较好?\tanswer:您好，哆啦A保规定，年龄在30天-55周岁均支持30年交费期间，年交保费。且带有豁免功能的保障型产品交费期间越长，豁免的作用就发挥得越大。所以，建议按照30年缴费计算。\n",
      "question:小孩太小没来的及买医疗保险怎么办\tanswer:您好，孩子的抵抗力比较弱些，建议可以先办理社区保险，作为基础。另外，如果医疗保险没有来得及办的话，可以先给孩子投保重疾险，目前百年人寿公司推出的康惠保，比较适合孩子，保障涵盖100种重疾以及30种轻症，孩子的年龄只要满了28周天就可以办理了。\n",
      "question:小孩太小没来的及买医疗保险怎么办\tanswer:我可以帮您\n"
     ]
    }
   ],
   "source": [
    "baoxian_file = 'F:/document/datasets/nlpData/conversation_dataset/baoxianzhidao_filter/baoxianzhidao_filter.csv'\n",
    "\n",
    "baoxian_df = pd.read_csv(baoxian_file)\n",
    "\n",
    "baoxian_df = baoxian_df[0:100]\n",
    "question_list = baoxian_df['title']\n",
    "answer_list = baoxian_df['reply']\n",
    "\n",
    "for (question,answer) in zip(question_list,answer_list):\n",
    "    print(\"question:{0}\\tanswer:{1}\".format(question,answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ASUS\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.047 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_cut_list[0:2]:['最近 在 安邦 长青树 中 看到 什么 豁免 ， 这个 是 什么 意思 ？', '和 老婆 利用 假期 去 澳 * 探亲 ， 但是 第一次 去 不大 熟悉 ， 有没有 相关 保险 呢 ？']\n",
      "answer_cut_list[0:2]:['您好 ， 这个 是 重疾险 中 给予 投保 者 的 一项 权利 ， 安 * 长青树 保障 责任 规定 ， 投保 者 可以 享受 多次 赔付 ， 豁免 等 权益 。 也就是说 不同 轻症 累计 5 次 赔付 ， 理赔 1 次轻症 豁免 后期 所 交 保费 ， 人性化 的 设计 ， 无需 加 保费 。', '您好 ， HUTS 保险 中 的 乐游 全球 （ 探亲 版 ） - 慧择 旅游 保险 澳新 计划 是 澳 * 新西兰 探亲 专属 保障 ， 承保 年龄 可达 90 周岁 ， 含有 50 万 高额 医疗保障 ， 完全 满足 境外 医疗保障 需求 ， 需要 注意 的 是 这款 产品 仅 承保 出行 目的 为 境外 探亲 的 人群 ， 理赔 时需 提供 相关 签证 或 亲属关系 证明 等']\n"
     ]
    }
   ],
   "source": [
    "# 利用jieba分词对 question 句子进行分词，并储存到 qa_df['question_cut'] 中\n",
    "import jieba\n",
    "def stopwords_file_load(stopword_path=\"\"):\n",
    "    # 1.读取停用词文件\n",
    "    with open_file(stopword_path) as f_stop:\n",
    "        try:\n",
    "            f_stop_text = f_stop.read()\n",
    "        finally:\n",
    "            f_stop.close()\n",
    "    # 停用词清除\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "    return f_stop_seg_list\n",
    "\n",
    "# f_stop_seg_list = stopwords_file_load(stopword_path=\"F:/document/datasets/jieba停用词and词典/stopwords1.txt\")\n",
    "# print(\"f_stop_seg_list:{0}\".format(f_stop_seg_list))\n",
    "def jieba_cut_word(subject):\n",
    "    seg_list = jieba.cut(subject, cut_all=False)\n",
    "    word_list = list(seg_list)\n",
    "    mywordlist = []\n",
    "    for myword in word_list:\n",
    "#         if not (myword in f_stop_seg_list):\n",
    "        mywordlist.append(myword)\n",
    "\n",
    "    word_list = \" \".join(mywordlist)\n",
    "    return word_list\n",
    "\n",
    "question_cut_list = [jieba_cut_word(question) for question in question_list]\n",
    "print(\"question_cut_list[0:2]:{0}\".format(question_cut_list[0:2]))\n",
    "\n",
    "answer_cut_list = [jieba_cut_word(answer) for answer in answer_list]\n",
    "print(\"answer_cut_list[0:2]:{0}\".format(answer_cut_list[0:2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词典\n",
    "\n",
    "接下来我们需要构建词典然后把问答句对加载到内存里。\n",
    "\n",
    "我们的输入是一个句对，每个句子都是词的序列，但是机器学习只能处理数值，因此我们需要建立词到数字ID的映射。\n",
    "\n",
    "为此，我们会定义一个Voc类，它会保存词到ID的映射，同时也保存反向的从ID到词的映射。除此之外，它还记录每个词出现的次数，以及总共出现的词的个数。这个类提供addWord方法来增加一个词， addSentence方法来增加句子，也提供方法trim来去除低频的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预定义的token\n",
    "PAD_token = 0  # 表示padding \n",
    "SOS_token = 1  # 句子的开始 \n",
    "EOS_token = 2  # 句子的结束 \n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # 目前有SOS, EOS, PAD这3个token。\n",
    "\n",
    "    # 将句子中每个词添加到 Voc 类中\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # 删除频次小于min_count的token \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # 重新构造词典 \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "        \n",
    "        # 重新构造后词频就没有意义了(都是1)\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面的Voc类我们就可以通过问答句对来构建词典了。但是在构建之前我们需要进行一些预处理。\n",
    "\n",
    "首先我们需要使用函数unicodeToAscii来把unicode字符变成ascii，比如把à变成a。注意，这里的代码只是用于处理西方文字，如果是中文，这个函数直接会丢弃掉。接下来把所有字母变成小写同时丢弃掉字母和常见标点(.!?)之外的所有字符。最后为了训练收敛，我们会用函数filterPairs去掉长度超过MAX_LENGTH的句子(句对)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 100 sentence pairs\n",
      "Trimmed to 100 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 1497\n",
      "\n",
      "pairs:\n",
      "len(pairs):100\n",
      "['最近 在 安邦 长青树 中 看到 什么 豁免 ， 这个 是 什么 意思 ？', '您好 ， 这个 是 重疾险 中 给予 投保 者 的 一项 权利 ， 安 * 长青树 保障 责任 规定 ， 投保 者 可以 享受 多次 赔付 ， 豁免 等 权益 。 也就是说 不同 轻症 累计 5 次 赔付 ， 理赔 1 次轻症 豁免 后期 所 交 保费 ， 人性化 的 设计 ， 无需 加 保费 。']\n",
      "['和 老婆 利用 假期 去 澳 * 探亲 ， 但是 第一次 去 不大 熟悉 ， 有没有 相关 保险 呢 ？', '您好 ， HUTS 保险 中 的 乐游 全球 （ 探亲 版 ） - 慧择 旅游 保险 澳新 计划 是 澳 * 新西兰 探亲 专属 保障 ， 承保 年龄 可达 90 周岁 ， 含有 50 万 高额 医疗保障 ， 完全 满足 境外 医疗保障 需求 ， 需要 注意 的 是 这款 产品 仅 承保 出行 目的 为 境外 探亲 的 人群 ， 理赔 时需 提供 相关 签证 或 亲属关系 证明 等']\n",
      "['HUTS 中 有没有 适合 帆船 比赛 的 保险 ， 我 男朋友 这 周 就要 开始 了', '您好 ， 水上运动 比赛 ， 尤其 是 带有 奖金 的 比赛 一般 承保 的 公司 比较 少 。 不过 ， HUTS 保险 中 的 众 行天下 - 水上运动 保险 赛事 版 B 就是 适合 帆船 等 水上 比赛 的 产品 ， 含 户外 溺水 保障 ， 是 水上运动 专属 定制 的 保障 ， 意外 住院 有 津贴 ， 保障 期限 灵活 可 选 ， 还 可以 投保 有 奖金 的 赛事 ， 您 可以 根据 情况 看看 。']\n",
      "['计划 端午节 和 男朋友 自驾 去 九 * 山 ， 买 保险 三天 要 多少 钱 ？', '您好 ， 端午 出行 的 人 比较 多 ， 而且 自驾 存在 一定 风险 ， 所以 有 保险 意识 还是 很 好 的 。 考虑 到 价格 以及 保障 内容 等 相关 因素 ， 您 可以 看看 HUTS 保险 中 的 畅玩 神州 - 慧择 旅游 保险 计划 三 ， 适合 驾驶 私家车 走南闯北 国内 旅游 ， 自驾 意外 累计 赔付 , 承保 的 范围 也 较为 广泛 ， 适合 带 家人 出游 ， 保障 全面 ， 三天 仅 需 75 元 ， 性价比 还是 蛮高 的 。']\n",
      "['计划 端午节 和 男朋友 自驾 去 九 * 山 ， 买 保险 三天 要 多少 钱 ？', '不到 10 块钱 … … … … … …']\n",
      "['端午 我们 准备 要 举行 赛龙舟 ， 说 是 要 份 保险 ， 什么 好', '您好 ， 赛龙舟 是 一项 比较 传统 的 活动 ， 很 有 意义 。 不过 由于 是 在 水上 活动 ， 建议 安全 保障 还要 做足 ， HUTS 保险 中有 针对 水上运动 风险 特别 定制 的 众 行天下 - 水上运动 保险 ， 可以 针对 这种 赛事 进行 保障 ， 含有 意外 住院 有 津贴 以及 一系列 保障 ， 性价比 较 高 ， 关键 是 费用 也 比较 实惠 。']\n",
      "['老婆 买 了 安 * 长 * 树 ， 她 在 网上 投保 的 ， 以后 缴费 怎么办', '您好 ， 这点 是 不用 担心 的 。 投保 后 保险公司 会 在 约定 的 保险费 交纳 日 从 消费者 购买 时 填写 的 银行账号 中划 扣 当期 应交 的 安 * 长青树 重疾险 的 保险费 ， 所以 您 老婆 是 不用 亲自 去 保险公司 缴费 的 。']\n",
      "['听说 huts 可以 申请 医疗 垫付 的 ， 什么 情况 下 可以 批 呢 ？', '您好 ， 是 可以 的 。 该 医疗 垫付 目的 便是 解决 投保 者 的 切实 困难 ， 可以 说 是 很 人性化 的 一项 保障 了 。 在 以下 几个 情况 下 可以 申请 医疗 垫付 ： ① 发生 重大 意外 或 突发 急性病 ； ② 所 投保 保险 涵盖 该 医疗 费用 补偿 责任 ； ③ 医疗 费用 高 客户 无法 承担']\n",
      "['和 团队 去 北极 探险 ， 有没有 针对 这方面 的 HUTS 保险 呢', '您好 ， 去 北极 探险 本身 就 存在 一定 的 风险 ， 建议 选择 专业 的 装备 以及 在 专业人士 的 陪同 下 进行 。 至于 保险 ， 市面上 关于 此类 的 保险 并不多 ， 不过 HUTS 保险 中 却 有 一款 专门 针对 南北极 旅游 的 定制 产品 ， 保障 内容 充足 ， 户外 伤害 、 医疗保障 甚至 的 紧急 救援 都 具备 ， 详情 可以 多 了解 下 。']\n",
      "['去 欧 * 出差 ， 随身 的 物品 比较 多 ， 有没有 保障 这个 的 保险', '您好 ， 去 一个 人生地不熟 的 地方 最怕 的 就是 行李 丢失 ， 很 麻烦 也 很 苦恼 。 目前 关于 境外 的 保险 比较 多 ， 但是 您 可以 看看 HUTS 保险 中 针对 欧 * 商务 出差 必备 ， 高额 随身 财产 保障 定制 的 乐游 全球 （ 境外 直付 版 ） 全球 顶级 款 。 对 出差 人士 的 随身 物品 保障 很 充足 ， 不仅仅 提供 全球 医疗 直付 ， 含有 高额 医疗 补偿 保障 以及 智能手机 及 电脑 保障 ， 还 对 行程 阻碍 、 意外 伤害 等 均 有 保障 。']\n",
      "['老公 喜欢 抽烟 ， 但是 听说 这样 就 投 不了 安 * 长青树 了 ， 是 真的 吗 ， 什么 意思 ？', '您好 ， 由于 重疾险 的 投保 要求 比较 严格 ， 会 对 投保人 的 一些 行为习惯 有所 规定 。 保险公司 一般 会 限制 抽烟 酗酒 的 人 投保 重疾险 的 。 如果 您 老公 每天 抽烟 的 数量 较 多 ， 是 不 建议 投保 安邦 长青树 的 ， 因为 不 符合 产品 健康 告知 中 的 条件 ， 保险公司 核保 通过 不了 ， 出现 拒保 情况 。 建议您 老公 的 情况 可以 选择 意外险 。']\n",
      "['老公 喜欢 抽烟 ， 但是 听说 这样 就 投 不了 安 * 长青树 了 ， 是 真的 吗 ， 什么 意思 ？', '依 哥们儿 的 经历 来看 ， 应该 说 这 是 一个 循续 渐进 的 过程 。 上学时 ， 看到 别人 抽烟 吐 烟圈 貌似 蛮酷 ， 于是 就 开始 跟着 学习 抽烟 … … 时间 一常 ， 想戒 也 难 了 。 另外 ， 不可否认 的 是 ， 特定 情况 下 对于 某些 人 ， 吸烟 有助于 思考 ， 确实 可以 适当 缓解 紧张 情绪 。']\n",
      "['这个 周末 天气 不错 ， 看着 心情 舒坦 ， 准备 和 朋友 户外 拓展 ， 什么 保险 好', '您好 ， HUTS 保险 中 的 众 行天下 - 拓展 训练 保险 针对 各类 户外活动 定制 的 产品 。 它 的 承保 范围 比较 官 ， 33 种 热门 拓展 运动 均 在 列 ， 并且 对 投保 者 的 年龄 规定 也 比较 松 ， 适合 1 - - 65 周岁 的 人群 投保 ， 性价比 高 ， 必要 时候 还 可以 提供 第一 现场 救援 。']\n",
      "['老婆 已经 买 了 其他 的 重疾险 ， 还要 投康惠保 ， 能行 吗', '您好 ， 这个 是 没有 冲突 的 ， 还是 可以 投保 的 。 不过 您 需要 注意 的 是 ， 但康惠 保有 最高 保额 限制 。 被保险人 在 百年 人寿 持有 保单 重疾 风险 保额 ， 出生 满 28 天 - 40 周岁 不 超过 50 万 ， 41 - 50 周岁 不 超过 30 万 ， 51 - 55 周岁 不 超过 10 万 。']\n",
      "['老婆 已经 买 了 其他 的 重疾险 ， 还要 投康惠保 ， 能行 吗', '这种 问题 根本 没法 回答 的 。 问题 好比 你 说 你 爱人 已经 吃 了 一碗 饭 了 ， 还 想 再 吃 一碗 能行 吗 ？ 你 没 说 你 爱人 的 饭量 ， 我 怎么 知道 她 还 能 不能 再 吃 一碗 。 也 就是 我 无从 知道 你 的 家庭 保险 需求 中 你 爱人 的 保险 需求 。 买 保险 是 大事 ， 买 不好 ， 祸害 你 一生 。']\n",
      "['我 和 老婆 单位 都 买 了 社保 ， 现在 她 又 要 买 什么 哆 啦 A 保 ， 有 必要 吗', '您好 ， 社保 是 属于 最为 基础 的 保障 ， 而 它 和 重疾险 两种 保险 的 理赔 方式 不同 ， 功能 相互 补充 。 这 款 产品 是 确诊 患 重大 疾病 就 可以 申请 理赔 ， 经 核定 属于 保险 责任 的 ， 保险公司 会 按照 合同 的 约定 进行 保险金 的 一次性 赔付 ， 不管 您 实际 治疗 费用 是 多少 ， 也 不 需要 提供 发票 和 费用 清单 ， 所 获得 的 保险金 可以 用于 治疗 或 弥补 患病 后 的 收入 损失 。 另外 ， 如果 您 还有 社保 或者 其他 公司 的 医疗保险 ， 还 可以 凭 医疗 单据 进行 费用 报销 。']\n",
      "['我 和 老婆 单位 都 买 了 社保 ， 现在 她 又 要 买 什么 哆 啦 A 保 ， 有 必要 吗', '社保 只是 保险 基础 ， 哆 啦 a 宝 属于 商业保险 ， 很 有 必要 购买 。 至于 买 什么 类型 商业保险 ， 建议 先买 重疾 、 医疗 、 意外 等等']\n",
      "['我 和 老婆 单位 都 买 了 社保 ， 现在 她 又 要 买 什么 哆 啦 A 保 ， 有 必要 吗', '景 生产线 屯阶 上部 狼']\n",
      "['小 * 买 了 份 哆 啦 A 保 ， 说 是 没有 纸质 保单 ， 怎么回事', '您好 ， 这个 不用 担心 的 ， 投保 哆 啦 A 保 完成 后 ， 电子 保单 会 直接 发送至 您 小叔 的 邮箱 的 。 如果 担心 其他 问题 ， 保证 合法权益 ， 可 通过 访问 弘康 人寿保险 网址 或 拨打 弘康 人寿 客户服务 热线 对 所 投保 保单 的 相关 信息 进行 查询 。']\n",
      "['huts 保险 中 有没有 关于 赛事 的 ？ 怎么 好多 保险 户外 赛事 都 不 保障', '您好 ， 是 有 的 。 HUTS 保险 中有 专为 部分 高风险 赛事 进行 定制 的 “ 赛事 安心 ” 保险 计划 综合 运动会 计划 高风险 运动 计划 ， 是 业余 高风险 赛事 专享 保障 ， 特含 紧急 医疗 运送 保障 ， 可 承保 有 奖金 的 赛事 ， 保障 全面 。 至于 目前 很多 保险 不保 户外 赛事 ， 主要 是因为 风险系数 比较 高 ， 而且 多数 保险 是 不 承保 含有 奖金 的 赛事 的 ， 但是 以上 的 那款 保险 是 可以 保障 的 。']\n",
      "['请问 下 ， 乙肝 能 不能 买 投康惠保 ？ 会 被 拒绝 吗 ？', '会 的 ， 买 不了']\n",
      "['请问 下 ， 乙肝 能 不能 买 投康惠保 ？ 会 被 拒绝 吗 ？', '您好 ， 乙肝病毒 携带者 客户 可 申请 核保 。 如果 核保 通过 就 可以 投保 ， 若 是 不 符合 健康 告知 ， 可以 加费 投保 或者 选择 康惠保 以外 的 产品 。']\n",
      "['我 空闲 时间 不 多 ， 老婆 带 着 儿子 两个 人去 周边 玩 一天 ， 有 什么 保险 呢 ?', '您好 ， 一般 情况 下 周边 游玩 风险 并不大 ， 不过 买份 保险 也 是 很 有 必要 的 。 由于 您 妻子 和 孩子 的 游玩 时间 并 不长 ， 只有 一天 ， 可以 看看 HUTS 保险 中 一日游 - 慧择 旅游 保险 ， 主要 针对 1 日游 出行 人群 ， 拥有 10 万 意外 伤害 保障 ， 保费 很 便宜 仅 需 1 元 。 另外 ， 天气 逐渐 转热 ， 注意 防暑 。']\n",
      "['我 空闲 时间 不 多 ， 老婆 带 着 儿子 两个 人去 周边 玩 一天 ， 有 什么 保险 呢 ?', '有 出行 险 的 啊 旅行 险']\n",
      "['这个 周末 儿子 有场 拳击 比赛 ， 很 担心 安全 ， 什么 保险 好', '您好 ， 拳击 比赛 存在 一定 风险 的 ， 所以 对 此类 运动 承保 的 并不多 ， 不过 您 可以 看看 HUTS 保险 有 专为 部分 高风险 赛事 进行 定制 的 “ 赛事 安心 ” 保险 计划 ， 是 业余 高风险 赛事 专享 保障 ， 特含 紧急 医疗 运送 保障 ， 可 承保 有 奖金 的 赛事 。 建议 可以 根据 您 儿子 的 情况 ， 参考 参考 。']\n",
      "['我 女朋友 比较 马虎 ， 她 这次 一个 人去 美 * 玩 ， 想 给 买份 保险', '您好 ， 意外 风险 无处不在 ， 买份 保险 防止 意外 发生 很 重要 。 您 女朋友 一个 人去 美 * ， 路途 比较 遥远 ， 建议 可以 看看 HUTS 保险 中 的 乐游 全球 。 主要 是 ， 可以 承保 部分 高风险 运动 ， 对于 旅途 中 比较 常见 的 行李 丢失 、 意外 伤害 医疗 、 航班 延误 等等 都 有 保障 。']\n",
      "['为什么 要 选择 职业 ， 我 儿子 还 在 上 中学 ， 怎么 填 安邦 长青树 的 职业 ？', '您好 ， 选择 职业 ， 是因为 重疾险 对 职业 有 一定 要求 的 ， 一般 情况 下重 疾会 允许 1 - 4 类 的 职业 投保 ， 安 * 长 * 树 也 是 。 超过 1 - 4 类 的 职业 一般 会 限制 投保 的 。 不过 您 孩子 还 在 上学 ， 可以 按 其 实际上 学 情况 选择 学龄前 儿童 或 学生 。']\n",
      "['哆 啦 A 保对 年龄 有没有 限制 ， 10 岁 的 小女孩 可以 买 吗', '您好 ， 这个 重疾险 对 年龄 是 有 一定 的 限制 的 ， 保险 规定 ， 哆 啦 A 保 适合 出生 满 30 天 - 55 周岁 ( 含 30 天 、 55 周岁 ) 投保 ， 所以 在 这个 年龄段 的 是 可以 买 ， 10 岁 的 小女孩 也 是 能 投保 的 。 另外 ， 哆 啦 A 保 保障 160 种 疾病 ， 不怕 健康 无 守护 ， 覆盖 全 周期 的 隐患 ， 并且 分组 多次 赔付 无 忧虑 ， 而且 智能 核保 更 方便 ， 功能完善 人性化 ， 很 适合 投保 。']\n",
      "['是不是 我 在 犹豫 期 退保 了 安邦 长青树 ， 就 不 给 我 保障 了 ？', '您好 ， 您 在 犹豫 期 申请 解除 保险合同 ， 需要 填写 解除合同 申请书 并 向 保险公司 提供 您 的 有效 身份证件 。 自 保险公司 收到 解除合同 申请书 时起 ， 保险合同 终止 。 因为 是 在 犹豫 期 ， 所以 保险公司 会 无息 退还 您 的 保费 ， 但是 合同 解除 了 ， 您 自然 也 就 不能 享受 保险 的 权利 了 ， 所以 不会 再 对 您 给予 保障 的 。']\n",
      "['是不是 我 在 犹豫 期 退保 了 安邦 长青树 ， 就 不 给 我 保障 了 ？', '是 的']\n",
      "['是不是 我 在 犹豫 期 退保 了 安邦 长青树 ， 就 不 给 我 保障 了 ？', '当然']\n",
      "['想 问下 ， 如果 重疾 理赔 后 身故 ， 哆 啦 A 保 还有 身故 赔偿金 吗', '您好 ， 不 赔 的 。 因为 已经 进行 了 重大 疾病 理赔 ， 保险 责任 规定 ， 重大 疾病 理赔 后 身故 的 ， 不 给付 身故 保险金 ， 所以 是 没有 的 。']\n",
      "['想 问下 ， 如果 重疾 理赔 后 身故 ， 哆 啦 A 保 还有 身故 赔偿金 吗', '没有']\n",
      "['重疾险 的 缴费 期限 有 什么 用处 ， 康惠保 的 缴费 期限 好不好', '一般来说 缴费 期限 越长越 好 的 ， 因为 一般 重疾 都 有 轻症 重疾 豁免权 ， 那么 如果 在 缴费 期限内 出险 ， 就 可以 豁免 后面 的 保费 ， 那 当然 是 缴费 期限 越长 ， 经济 压力 越小嘛']\n",
      "['重疾险 的 缴费 期限 有 什么 用处 ， 康惠保 的 缴费 期限 好不好', '您好 ， 正常 来说 ， 缴费 期限 是 越长越 好 的 ， 因为 缴费 期限 越长 ， 杠杆 率越 高 ， 性价比 也 就 越 高 。 康惠 保能 做到 性价比 最高 的 30 年 缴费 ， 还 可以 保障 终身 ， 在 同类产品 中是 比较 出色 的 。']\n",
      "['安 * 长 * 树 * 保 * 不 * 扣费 ， 怎么 做 呢', '会扣 ， 只能 退 给 你 保单 的 现金 价值 ， 和 你 已交 的 保费 比 ， 差 很多 。']\n",
      "['怎么 儿子 去 留学 前 三个 月 没有 医疗保障 ， 怎么办 ？', '可以 买 国际型 的 医保 啊']\n",
      "['怎么 儿子 去 留学 前 三个 月 没有 医疗保障 ， 怎么办 ？', '您好 ， 留学生 前期 没有 医疗保障 是 一个 比较 常见 的 问题 ， 因为 学校 还 没有 给 学生 办理 相关 医疗保障 ， 需要 学生 提前 补充 ， 加上 国外 的 医疗 费用 一般 比较 高 ， 所以 这个 时候 的 商业保险 显得 尤为重要 了 。 建议 考虑 下 HUTS 保险 中 的 前程似锦 （ 基础 版 ） - 慧择 留学 保险 ， 这 款 产品 是 海外 游学 专属 保障 ， 价格 不 贵 ， 保障 全面 ， 1 - - 90 天 任意 投保 ， 可以 满足 前 三个 月 学校 医疗保障 的 空白 ， 针对性 很强 。']\n",
      "['谁 知道 , 在 海外 务工 , 有 什么 保险 可以 保障 ?', '您好 ， HUTS 保险 中 的 海外 务工 （ 1 - 4 类 ） - 慧择 旅游 保险 计划 C ， 是 海外 工作 派遣 的 专属 选择 ， 保险 期间 1 至 12 月 可选 。 另外 可以 承保 年龄 在 18 - 65 周岁 ， 保障 较为 全面 。']\n",
      "['去年 下半年 大叔 买 了 哆 啦 A 保 ， 现在 原位癌 住院 了 ， 能 不能 理赔 ？', '您好 ， 这 属于 哆 啦 a 保 55 种轻症 的 其中 一种 ， 保险公司 给付 轻症 疾病 保险金 ， 保险合同 继续 有效 。 也 就是 如 后期 罹患 其他 54 种轻症 和 105 种重 疾 仍 可 获得 赔付 的 ， 所以 您 并 不用 担心 。']\n",
      "['当前 有没有 可以 针对 第一 现场 救助 的 旅游 保险 呢', '您好 ， 虽然 目前 市面上 很多 旅行 险中 都 表示 可以 进行 紧急 救援 ， 但是 基本上 提供 的 都 是 第二 现场 救援 ， 而 第一 现场 救援 并不多 。 不过 目前 慧择 就 针对 这部分 人群 专门 定制 这 类 保险 ， 实现 及时 援助 远比 事后 补偿 更 重要 的 切入点 ， 让 人们 旅行 无忧 ， 真正 做到 陪同 服务 和 必须 赶往 现场 的 救援 服务 。']\n",
      "['为什么 我 投保 的 弘康 哆 啦 A 保 没有 发票 呢 ？', '您好 ， 并 不用 担心 ， 它 是 提供 电子 发票 的 。 电子 发票 获取 路径 ： 关注 弘康 人寿 官方 微信 ： 弘康 人寿 — 注册 登录 — 我 的 保单 — 点开 保单 详情 页面 — 点击 左下 小 机器人 按钮 — 选择 “ 电子 发票 ” — 确认 您 的 电子邮箱 无误 — 下载 电子 发票 到 邮箱 中 即可 。 （ 注 ： 电子 发票 自动 生成 条件 为 保单 状态 有效 、 已过 犹豫 期 。 ）']\n",
      "['康惠保 是 选择 终身 的 好 还是 到 70 岁 保障 期好 ， 为什么 ？', '您好 ， 康惠保 的 保障 周期 可以 选择 保障 到 70 岁 或者 终身 ， 如果 投保人 经济 压力 大 的话 可以 选择 定期 70 岁 的 投保 方案 ， 价格 更 低 ， 杠杆 率 更 高 。 考虑 到 我国 人均 寿命 也 就 70 多岁 ， 因此 家庭 不 富裕 的 投保人 完全 可以 选择 这样 的 投保 方式 来 获取 更 高 的 杠杆 收益 。 保障 到 70 岁 ， 如果 70 岁 前 没有 出险 ， 之后 合同 终止 ； 现金 价值 归零 ， 且 70 岁 之后 没有 重疾 保障 ； 保障 到 终身 ， 终身 有 重疾 保障 ， 如果 一辈子 没有 身患 重疾 的话 ， 在 身故 之后 可以 拿到 保单 现金 价值 ， 现金 价值 比较 可观 。']\n",
      "['康惠保 是 选择 终身 的 好 还是 到 70 岁 保障 期好 ， 为什么 ？', '您好 ， 康惠保 重大 疾病 保险 的 产品 形态 ， 包括 主险 和 附加 险 。 主险 （ 康惠保 重大 疾病 保险 ） ； 附加 险 （ 康惠保 特定 疾病 保险 ） 。 主险 （ 必选 ） 包含 重症 责任 ， 附加 险 （ 可 选 ） 包括 轻症 及 轻症 保费 豁免 责任 。 这款 重疾险 是 一款 典型 的 消费型 重疾险 ： 保费 较 低 ， 杠杆 率 较 高 ， 但 无 返还 功能 ， 身故 责任 保障 给 付现金 价值 。']\n",
      "['周末 赔 闺女 体验 下 野外 运动 ， 有没有 亲子 保险', '您好 ， 是 有 的 ， HUTS 保险 中 父母 孩子 共同 参与 亲子 类 活动 可以 参考 慧游游 - 慧择 旅游 保险 计划 一 ， 保障 全面 ， 价格 不高 ， 可以 说 是 性价比 很 高 的 一款 产品 了 。']\n",
      "['康惠保 在 北 * 买 的 ， 现在 人 在 山 * ， 出险 了 是否 可以 理赔 ？', '您好 ， 常住 地 在 大 * 、 湖 * 、 河 * 、 辽 * 、 * * 、 河 * 、 黑 * * 、 安 * 、 山 * 、 * * 、 四 * 、 福 * 、 陕 * 、 内 * 古 、 吉 * 、 * * 、 浙 * 、 山 * 、 广 * 、 重 * 这些 地区 方可 投保 ， 不 影响 理赔 。 如果 迁入地 有 分支 公司 的 支持 保单 迁移 的 保全 操作 ， 到 柜面 申请 即可 。 如果 没有 迁移 ， 您 只是 暂时 在 山 * ， 理赔 的 时候 会 安排 就近 的 分支 公司 提供 服务 。']\n",
      "['康惠保 在 北 * 买 的 ， 现在 人 在 山 * ， 出险 了 是否 可以 理赔 ？', '只要 达到 保险 理赔 要求 就 没 问题 的 。']\n",
      "['出国 玩 比较 担心 护照 等 随身 物品 安全 ， 什么 保险 可以 投 ？', '您好 ， 意外 风险 无处不在 ， 防患于未然 很 有 必要 ， 在 这种 情况 下 建议 可以 了解 下 HUTS 保险 中 的 乐游 全球 （ 境外 直付 版 ） - 慧择 旅游 保险 钻石 计划 承保 部分 高风险 运动 ， 保额 也 很 高 ， 仅 需 100 元 ， 性价比 很 高 。 另外 ， 关于 随身 物品 、 证件 行李 等 都 在 保障 范围 内 。']\n",
      "['出国 玩 比较 担心 护照 等 随身 物品 安全 ， 什么 保险 可以 投 ？', '对于 中 * 游客 而言 ， 出 * 旅游 固然 可以 欣赏 异 * 风情 ， 但是 不同 的 语言 和 不 熟悉 的 环境 会 加大 自身 安全 风险 ， 因此 ， 您 在 出 * 之前 给 自己 上 一份 合适 的 出 * 旅游 保险 是 必要 的 。 出 * 旅游 买 什么 保险 好 建议您 选择 针对性 强 的 出 * 旅游 保险 ， 投保 时 可以 根据 自己 具体 前往 的 * 家 以及 实际 保障 需求 来 选购 。 一般来说 ， 购买 出 * 归属 地 的 出 * 旅游 保险 性价比 更高 。 投保 前 您 需要 注意 以 * 几点 ： 1 . 根据 出行 目的 * 的 消费水平 选择 合适 的 保额 。 2 . 购买 的 境外 旅游 保险 需要 覆盖 您 的 全部 行程 。 使得 自己 的 旅行 全程 在 保障 之中 。 3 . 在 购买 出境 旅游 保险 时 ， 要 注意 是否 包括 * 际 紧急 救援 服务 。 由于 * 内 保险公司 的 网点 很难 铺 到 * 外 ， 所以 出境 旅游 险 产品 通常 是 与 * 际 紧急 救援 公司 合作 。 无论是 游客 在外 遗失 钱包 ， 还是 护照 丢失 等 ， 都 可以 致电 救援 热线 ， 一些 大 的 * 际 保险公司 还 专门 提供 汉语 服务 。 4 . 选择 合适 的 投保 平台 ， 在 慧择 网上 购买 ， 您 可以 对比 多家 保险公司 的 产品 ， 进而 选择 最合适 自己 保障 和 出行 需要 的 产品 。 投保 也 非常简单 方便 ， 在 慧择 网 首页 ， 免费 注册 会员 ， 登陆 后 选择 中意 的 产品 ， 选择 保障 期限 ， 即可 立即 购买 。 价格便宜 ， 整个 投保 过程 清晰 透明 ， 免去 了 中介代理 的 层层 麻烦 ， 保费 更 低 ， 自主性 更强 。 出 * 旅游 买 保险 ， 您 首先 要 要 了解 出 * 旅游 保险 的 种类 ， 然后 再 按 需 选购 ， 慧择 网是 提供 专业 境外 旅游 保险 的 投保 平台 ， 欢迎您 前来 选购 。 美 * “ 乐悠游 ” 境外 旅行 保障 计划 二 华泰 “ 安 * 天 * ” * 际 旅游 保障 计划经济 款 展开']\n",
      "['出国 玩 比较 担心 护照 等 随身 物品 安全 ， 什么 保险 可以 投 ？', '可以 了解 一下 平安 的']\n",
      "['投 了 安 * 长 * 树 几个 月 后 ， 发现 得 了 脑膜炎 ， 这个 可以 报销 吗', '您好 ， 这个 是 可以 报销 的 。 因为 脑膜炎 属于 安邦 长青树 保障 的 38 种轻症 的 其中 一种 ， 保险公司 给付 轻症 疾病 保险金 ， 并且 保险合同 继续 有效 。 意思 就是 ， 如果 后期 罹患 其他 37 种轻症 和 80 种重 疾 仍 可 获得 赔付 。']\n",
      "['什么 是 小额 快 赔 ， 在 huts 保险 中 看到 的', '您好 ， 小额 快 赔 服务 是 HUTS 保险 针对 3000 元 金额 以内 小额 理赔 的 绿色 快速通道 ， 目的 是 为了 让 消费者 更加 快捷 地 获得 理赔 。 优势 主要 体现 在 以下 三个 方面 ： ① 比非 闪 赔 产品 更少 的 材料 ； ② 绿色通道 闪电般 审核 速度 ； ③ 全程 线上 操作 更 便捷 。']\n",
      "['请问 下澳 * 利 * 公差 一个 星期 ， 有 什么 保险 保障 全', '您好 ， 当前 境外 商旅 人士 增多 ， 也 相应 地 对 保险 安全 有 了 要求 。 澳 * 利 * 出差 的话 ， 建议 参考 下澳 * 利 * 商务 出行 必备 ， 财务 保障 充足 的 乐游 全球 （ 境外 直付 版 ） - 慧择 旅游 保险 。 这是 一款 针对 商旅 人士 的 保险 ， 可以 提供 境外 医疗 直付 ， 承保 部分 高风险 运动 ， 含有 既往 症 急救 补偿 ， 保障 1 - - 5 天仅 需 100 元 。']\n",
      "['请问 下澳 * 利 * 公差 一个 星期 ， 有 什么 保险 保障 全', '有个 叫 什么 境外 旅行 什么 的 ， 1 星期 不 贵 ； 也 有 便宜 的 ， 几元 。']\n",
      "['想要 去 欧 * 游玩 ， 不 晓得 什么样 的 保险 比较 合适', '您好 ， 欧 * 相对而言 比较 遥远 ， 出行 买份 保险 保障 安全 是 很 有 必要 的 。 首先 ， 安全 第一 ， 可以 看看 HUTS 保险 的 安 * 天下 - 慧择 旅游 保险 。 30 万 高额 医疗保障 ， 随身 财产 有 保障 ， 特含 紧急 医疗 运送 保障 ， 承保 年龄 也 较为 广泛 ， 基本上 大多数 人 都 是 可以 投保 的 。']\n",
      "['想要 去 欧 * 游玩 ， 不 晓得 什么样 的 保险 比较 合适', '将 赴 吴 * 登乐游 原一绝 ( 杜 * )']\n",
      "['什么 是 轻症 豁免 ， 在 康惠 保中 看到 的 ？', '您好 ， 相对 于 重疾 ， 其实 轻症 大病 更为 常见 和 多发 ， 同时 它 往往 是 重疾 的 潜伏 阶段 ， 需要 早期 发现 和 积极 治疗 ， 才能 防止 往重 疾 转变 。 轻症 是 和 重大 疾病 相对 应 的 ， 简单 来说 ， 就是 重大 疾病 前期 较轻 的 疾病 ， 对人 影响 不 大 ， 通过 接受 治疗 恢复健康 。 所谓 的 保费 豁免 是 指 在 保险合同 规定 的 缴费 期内 ， 投保人 或 被 保人 达到 某些 特定 的 情况 （ 如 身故 、 残疾 、 重疾 或 轻症 疾病 等 ） ， 由 保险公司 获准 ， 同意 投保人 可以 不再 缴纳 后续 保费 ， 保险合同 仍然 有效 。']\n",
      "['安 * 长 * 树对 各个 地区 可以 投保 的 最高 保额 有 限制 吗 ？', '您好 ， 安 * 长 * 树 目前 对 A 类 地区 及 B 类 地区 设有 不同 的 保额 限制 。 针对 A 类 地区 （ * * 、 上 * 、 广 * 、 深 * 、 浙 * 、 * * ） ， 18 - 40 岁 保额 最高 可选 50 万 ， 41 - 50 岁 最高 可选 25 万 ， 51 - 55 岁 最高 可选 8 万 ； B 类 地区 （ 河 * 、 河 * 、 山 * 、 吉 * 、 黑 * * 、 辽 * 、 湖 * 、 湖 * 、 四 * 、 * * 、 山 * 、 安 * 、 天 * ） ， 18 - 40 岁 保额 最高 可选 33 万 ， 41 - 50 岁 最高 可选 16 万 ， 51 - 55 岁 最高 可选 4 万 。']\n",
      "['安 * 长 * 树对 各个 地区 可以 投保 的 最高 保额 有 限制 吗 ？', '在 允许 销售 的 地区 是 一样 的']\n",
      "['本周 公司 团建 ， 去 户外 玩 ， huts 保险 有 什么 比较 好', '您好 ， 一般 公司 团建 安全系数 还是 挺 高 的 ， 但是 由于 人员 比较 多 ， 还是 有 份 保险 比较 好 。 比如说 HUTS 保险 中 的 众 行天下 - 拓展 训练 保险 ， 很 符合 团建 素拓 等 各类 团队 活动 定制 的 产品 。 另外 ， 可以 承保 33 种 热门 拓展 运动 ， 保障 的 年龄 范围 也 广泛 ， 适合 1 - - 65 周岁 的 人群 投保 ， 其 保障 内容 一般 满足 公司 团建 需求 。']\n",
      "['哆 啦 A 保 享有 多次 赔付 ， 但 患重 疾 理赔 后 ， 轻症 和 身故 责任 还有 吗 ？', '您好 ， 在 条款 中有 明确 说明 ， 首次 给付 重大 疾病 保险金 后 ， 身故 保险金 的 保险 责任 与 轻症 疾病 保险金 的 保险 责任 均 终止 。 简言之 ， 重疾 理赔 和 身故 理赔 只 赔付 其中 一种 。']\n",
      "['哆 啦 A 保 享有 多次 赔付 ， 但 患重 疾 理赔 后 ， 轻症 和 身故 责任 还有 吗 ？', '重疾 赔付 后 ， 轻症 ， 高残 ， 身故 责任 终止 ， 只能 等 下次 大病 理赔 了']\n",
      "['和 老婆 去 德 * 旅游 ， 办 签证 要 买 保险 ， 什么 比较 好 ？', '您好 ， HUTS 保险 中有 一款 产品 在 满足 签证 的 基础 上 提供 全面 保障 ， 那 就是 安 * 天下 - 慧择 旅游 保险 欧 * 顶级 款 ， 30 万 高额 医疗保障 ， 随身 财产 有 保障 ， 特含 紧急 医疗 运送 保障 ， 承保 年龄 也 较为 广泛 ， 适合 1 - - 80 周岁 的 人群 投保 ， 服务 的 人群 比较 广泛 ， 可以 看看 。']\n",
      "['有 谁 知道 ， 安 * 长 * 树 出险 了 需要 提供 哪些 医院 证明 ？', '您好 ， 该 保险 对 医院 是 指定 的 ， 需要 到 合同 约定 的 医院 ( 境内 一般 是 二级 或 二级 以上 公立医院 ) 进行 就诊 治疗 。 需要 提醒 投保 者 的 是 ， 提醒 主治医生 使用 医保 范围 内 用药 、 诊疗 项目 及 服务设施 。 另外 ， 就诊 的 同时 请 妥善 保存 病历 ( 包含 首 诊病 历 ) 、 原始 收费 凭证 、 处方 、 诊断 证明 、 检查 化验 报告 、 住院 证明 等 就医 相关 材料 ， 方便 向 保险公司 办理 索赔 申请 。']\n",
      "['姐姐 投 了 安 * 长 * 树 ， 为啥 只有 电子 保单 ？', '您好 ， 网上 投保 保险公司 是 只 提供 电子 保单 的 ， 不过 您 不用 担心 ， 电子 保单 同样 有效 的 。 根据 《 中华人民 共和 * 合同法 》 相关 规定 ， 数据 电文 是 合法 的 合同 表现形式 ， 电子 保单 与 纸质 保单 具有 同等 法律效力 。 另外 ， 根据 中 * 保 * 会 《 人身保险 业务 基本 服务 规定 》 第十五条 规定 ， 安邦 人寿 会 在 保单 生效 后 对 投保人 进行 电话 回访 ， 回访 只是 对于 保险产品 的 风险 进行 提示 确认 的 。']\n",
      "['女儿 去 欧 * 留学 ， 刚 开始 几个 月 医保卡 没有 下来 ， 什么 保险 可以 补充 ？', '您好 ， 欧 * 留学 的话 ， 刚 开始 几个 月 可能 对 留学生 的 保障 是 不够 全面 的 ， 所以 选择 一款 合适 的 保险 进行 补充 是 很 有 必要 的 。 建议您 可以 看看 HUTS 的 前程似锦 （ 留学 ） - 慧择 旅游 保险 。 主要 考虑 到 下面 几点 ： 1 、 高额 医疗 补偿 费用 ， 专 为 发达国家 留学 定制 ， 对于 欧 * 这样 的 高 消费 国家 而言 ， 显得 很 有 必要 ； 2 、 涵盖 个人 责任 及 留学 中断 方方面面 ； 3 、 含 紧急 医疗 运送 服务 ， 孤身 在外 有 照应 。 另外 ， 对于 一些 证件 的 遗失 等 问题 ， 也 是 提供 理赔 的 。']\n",
      "['心肌 桥 可以 投 健康 保险 吗', '您好 ， 大多数 保险公司 对于 心肌 桥 患者 都 是 不予 投保 健康险 的 ， 而且 类似 于 康惠保 这样 的 重疾险 ， 在 消费者 购买 保险 之前 都 会 有 比较 详细 且 严格 的 核保 程序 。 所以 建议您 可以 考虑 意外险 等 相关 保险 的 基础 保障 ， 健康险 的话 一般 会 拒保 的 ， 除非 保险合同 有 规定 ， 除外 情况 可以 投保 。']\n",
      "['我 女朋友 是 个 导游 ， 比较 担心 安全 ， 请问 什么 领队 责任险 可以 考虑 ？', '您好 ， 导游 一般 肩负着 整个 团队 的 安全 责任 ， 买份 保险 还是 很 有 必要 的 。 HUTS 保险 中 充足 的 领队 第三者 责任 保障 的 产品 有众 行天下 - 领队 责任保险 ， 高额 领队 责任 保障 ， 承保 部分 高风险 运动 ， 完全 出于 对于 领队 用户 的 考虑 量身定做 的 ， 可以 参考 参考 。']\n",
      "['请问 下 ， 安 * 长青树 有没有 等待 期 ， 长不长 ？', '您好 ， 一般 重疾险 都 有 一个 等待 期 ， 安 * 长 * 树 也 不 例外 ， 是 有 的 。 它 的 等待 期为 90 天 。 如果 等待 期内 发生 合同 约定 的 重大 疾病 、 达到 疾病 终末期 阶段 、 身故 、 全残 ， 将 无息 返还 已交 保费 ， 合同 终止 。 因 意外 伤害 导致 上述 保险 事故 的 无 等待 期 限制 。']\n",
      "['请问 下 ， 安 * 长青树 有没有 等待 期 ， 长不长 ？', '沈 * 病重 去世 。 臧姑 亦 悔不当初 ， 决定 重新做人 。']\n",
      "['去 韩 * 游玩 ， 有 什么 HUTS 保险 比较 可靠', '您好 ， 可以 考虑 HUTS 保险 中 的 乐 * 日 * - 慧择 旅 * 保险 计划 ， 40 万 医疗 运送 保障 以及 花粉 过敏 急性 发作 ， 产品设计 可以 说 相当 人性化 了 。 5 天仅 需 60 元 ， 性价比 也 是 相当 高 的 了 ， 可以 作为 一个 不错 的 参考 产品 了 。']\n",
      "['打算 周末 和 老公 带 儿子 来场 亲子 游 ， 有 什么 HUTS 保险 比较 可靠 ？', '您好 ， 是 有 的 。 慧游游 - 慧择 旅游 保险 就是 专门 针对 亲子 游 的 保险 。 它 的 承保 范围 涵盖 市 内外 ， 是 亲子 游 首选 ； 另外 ， 还 提供 紧急 医疗 运送 服务 ， 大人 小孩 均 有 保障 ， 它 的 最大 特色 ， 便是 包括 协助 儿童 送返 费用 补偿 的 了 ， 您 可以 多 了解 下 。']\n",
      "['消费型 重疾险 适合 哪些 人群', '（ 1 ） 您好 ， 所谓 消费型 重疾是 相对 于 返还 型 重疾 而言 的 一类 重疾 险种 ， 其 主要 特点 是 到期 后 不会 返还 保费 。 比如说 比较 常见 的 康惠保 、 哆 啦 A 保等 都 是 这 两年 比较 受欢迎 的 消费型 重疾险 。 （ 2 ） 另外 ， 消费型 重疾险 的 特点 是 性价比 高 ， 较 低 的 保费 就 能 获得 很 高 的 保额 。 （ 3 ） 故而 ， 消费型 重疾险 适合 的 人群 ： A 、 希望 把 预算 用 在 风险 保障 上 ， 而 不是 和 理财 、 储蓄 等 功能 混为一谈 的 人群 身上 。 B 、 保险 预算 较少 的 家庭 或 个人 。']\n",
      "['儿子 去 外国 之前 买 了 HUTS ， 这个 在 国外 可以 提供 紧急 救援 吗 ？', 'b']\n",
      "['儿子 去 外国 之前 买 了 HUTS ， 这个 在 国外 可以 提供 紧急 救援 吗 ？', '您好 ， 同样 有 的 。 HUTS 境内 旅行 紧急 援助 服务 已 在 国内 覆盖 各省市 ， 全球 救援 服务 覆盖 达 200 个 国家 。 目前 慧择 已 与 优普 救援 、 美亚 Tr ####### rd 、 SOS 等 知名 救援 机构 达成 合作 。']\n",
      "['下周 儿子 会 有场 小学生 足球比赛 ， 比较 担心 安全 ， 可以 买 什么 保险 ？', '不用 买 ， 踢足球 很 安全 的 ， 哪有 那么 容易 出 意外 。 其实 学校 给 每个 学生 都 买 了 意外险 的 。']\n",
      "['儿子 计划 去 美 * 游学 三个 月 ， 其他 都 准备 好 了 ， 现在 想 找款 合适 的 保险', '您好 ， 去 美 * 游学 的话 保险 还是 很 有 必要 的 。 您 可以 考虑 下 HUTS 是 我 境外游 学 - 慧择 旅游 保险 ， 这 款 产品 特别 适合 医疗 额度 充足 ， 对于 外出 游学 的 朋友 保障 力度 十分 强 。 1 、 全球 留学 的 高性价比 之选 ； 2 、 三个 月 仅 需 190 ， 适合 短期 留学 出行 ； 3 、 紧急 医疗 运送 服务 ， 保障 学子 安全 。 另外 ， 它 的 主要 特色 有 ： 海外 游学 专属 、 高额 意外 身故 保障 、 意外 医疗保障 、 突发 急性病 医疗保障 。']\n",
      "['儿子 计划 去 美 * 游学 三个 月 ， 其他 都 准备 好 了 ， 现在 想 找款 合适 的 保险', '就是 中 * 平安 和 中 * 人寿 ， 比较 合适']\n",
      "['儿子 计划 去 美 * 游学 三个 月 ， 其他 都 准备 好 了 ， 现在 想 找款 合适 的 保险', '买个 极短 险 搞 个 3 个 月 即可 几百块']\n",
      "['请问 huts 可以 承保 30 人 的 野外 跑步 赛事 吗 ？', '您好 ， 是 可以 承保 的 。 HUTS 中有 适合 跑步 运动 ， 包含 猝死 保障 的 产品 - - 众 行天下 - 跑步 运动 保险 运动版 C ， 是 跑步 运动 专属 保障 ， 有 猝死 / 急性病 身故 保障 ， 特含 突发 急性病 医疗 ， 专门 针对 野外 跑步 比赛 的 。']\n",
      "['请问 下康惠保 的 缴费 年限 有 哪些 ？', '您好 ， 百年 康惠保 保障 年限 可以 选择 至 70 岁 和 终身 的 ， 缴费 年限 有 10 年 、 15 年 、 20 年 以及 30 四种 选择 。']\n",
      "['邻居 大叔 买 了 安 * 长 * 树 ， 半年 后 查出 有 原位癌 ， 让 我 查查 赔 吗 ？', '您好 ， 不用 担心 的 ， 原位癌 属于 38 种轻症 的 其中 一种 ， 保险公司 将 按 保额 的 20% 也 就是 6 万元 给付 轻症 疾病 保险金 ， 保险合同 继续 有效 。 也 就是 如 后期 罹患 其他 37 种轻症 和 80 种重 疾 仍 可 获得 赔付 。']\n",
      "['因为 工作 原因 ， 乘坐 飞机 比较 多 ， 有 哪些 HUTS 保险产品 是 适合 坐飞机 投保 的 ?', '您好 ， 像 飞机 这种 交通工具 ， 一旦 出事 ， 后果 将 不堪设想 。 所以 提前 购买 HUTS 保险 还是 很 有 必要 的 ， 以防万一 嘛 。 至于 产品 的 选择 ， 建议 保额 越高越 好 ， 最好 附加 购买 航班 延误 保障 。 具体 的 HUTS 产品 可以 参考 下 “ 慧 飞行 ” 航空 意外 保障 全年 计划 ： 1 、 高性价比 ： 全年 五百万 保障 仅 需 50 元 ； 2 、 高额 保障 ： 保额 叠加 ， 最多 可 购买 2 份 ； 3 、 适用人群 ： 适合 经常 乘飞机 出行 的 商务人士 和 旅行 人士 。 大家 也 可以 去 网站 上 了解 下 。']\n",
      "['最近 看到 HUTS 的 小额 快 赔 ， 有 什么 好处 呢 ？', '您好 ， 小额 快 赔 服务 是 针对 3000 元 金额 以内 小额 理赔 的 绿色 快速通道 ， 非常 快速 便捷 。 优势 主要 体现 在 以下 三个 方面 ： ① 比非 闪 赔 产品 更少 的 材料 ； ② 绿色通道 闪电般 审核 速度 ； ③ 全程 线上 操作 更 便捷 。']\n",
      "['爱好 喝点 小酒 ， 可以 投 长青树 吗', '您好 ， 如果 饮酒 较为 频繁 ， 不 建议 投保 安邦 长青树 的 ， 因为 不 符合 产品 健康 告知 中 的 条件 ， 保险公司 核保 通过 不了 ， 而且 大部分 重疾险 一般 对 爱好 抽烟 、 饮酒 的 人士 都 是 拒保 的 。 不过 ， 您 可以 优先 考虑 意外险 做 基础 的 保障 。']\n",
      "['老板 安排 我 去 美 * 出差 一个月 ， 什么 保险 比较 安全', '您好 ， 具体 可以 考虑 HUTS 保险 中 针对 美 * 展会 和 商务 必备 ， 高额 随身 财产 保障 的 产品 - - - 乐游 全球 （ 商旅 无忧 版 ） - 慧择 旅游 保险 全球 顶级 款 ， 提供 全球 医疗 直付 ， 含有 高额 医疗 补偿 保障 以及 智能手机 及 电脑 保障 。']\n",
      "['请问 下 什么 是 轻症 ， 为什么 原位癌 却 按照 安邦 长青树 的 轻症 理赔 ？', '您好 ， 保险行业 协会 没有 对轻症 有 统一 的 标准 ， 也 造成 了 不同 公司 对轻症 的 数量 、 种类 甚至 定义 都 有 一定 的 不同 。 给 大家 列举 比较 常见 的 几种 轻症 吧 ： 轻微 脑中风 、 不 典型 的 心肌 梗 、 较 小 面积 的 III 度 烧伤 、 视力 严重 受损 、 冠状动脉 介入 手术 、 主动脉 内 手术 、 脑垂体 瘤 、 脑 囊肿 、 脑 动脉瘤 及脑 血管瘤 等等 。 原位癌 虽然 是 癌症 的 早期 阶段 ， 但是 治疗 起来 相对 简单 ， 一般 手术 切除 病灶 即可 ， 5 年 生存率 高 ， 所以 安 * 长 * 树 将 原位癌 纳入 轻症 范围 ， 按照 轻症 理赔 。']\n",
      "['最近 俱乐部 有个 羽毛球 比赛 ， 给 参赛者 选择 什么 huts 保险 好 ？', '您好 ， 现在 户外 爱好者 、 学校 、 相关 单位 、 团体 组织 的 竞赛 越来越 多 ， 竞赛 过程 中 发生意外 风险 的 机率 也 随之 增加 。 另外 ， 目前 大多数 保险 并 不 承保 竞赛 类 ， 特别 是 带 奖金 、 奖品 的 竞赛 性质 活动 。 因此 ， 如果 参加 相关 赛事 活动 ， 一定 要 选择 专门 的 HUTS 业余 赛事 保险 ， 来 承保 业余 普通 体育竞赛 ( 包括 田径 、 游泳 、 球类运动 ) 中 的 意外事故 。 可 根据 具体 比赛 项目风险 特点 ， 有 针对性 的 选择 承保 该项 比赛 的 赛事 保险 ， 以策 万全 。 需要 大家 注意 的 是 ， 只有 赛事 保险 才能 保有 奖金 的 赛事 活动 ， 普通 意外险 是 不保 的 。 如果 投保 普通 意外险 ， 理赔 的 时候 被 发现 ， 可是 会 被 拒赔 的 。']\n",
      "['请问 下 攀岩 徒步 等 户外运动 ， 买 什么 保险 较 好 ？', '您好 ， HUTS 保险 中 承保 攀岩 、 徒步 溯溪 过程 中 各类 风险 的 众 行天下 - 综合 户外运动 保险 计划 二 ， 适合 1 - 65 周岁 的 人群 投保 ， 承保 高风险 活动 ， 可以 保障 溯溪 过程 中 的 各类 风险 ， 保障 期限 灵活 可 选 ， 是 一款 很 不错 的 产品 。']\n",
      "['有 谁 知道 康惠保 申请 得严 不 严格 ？', '您好 ， 对于 重疾险 产品 来说 核保 相对 还是 很 严格 的 ， 但是 现在 很多 重疾险 产品 都 是 智能 核保 ， 很 方便 ， 操作 也 简单 ， 大大 节省 了 用户 的 时间 。 不过 需要 提醒您 的 是 ， 虽然 康惠保 审核 比较 严格 ， 但是 您 既 可以 线上 核保 ， 也 可以 人工 核保 的 ， 比较 便利 。']\n",
      "['刚 投保 的 康 * 保 ， 说 还 在 等待 期 ， 什么 意思', '您好 ， 所谓 的 等待 期 就是 观察期 ， 防止 投保人 逆 选择 投保 。 一般 是 90 天 或者 180 天 ， 如果 在 这个 期限内 发生 风险 ， 保险公司 一般 是 退还 康惠保 的 保费 的 。']\n",
      "['我 是 个 导游 ， 近期 带队 很 频繁 ， 有 什么 保险 可以 投 的 ？', '您好 ， 根据 您 的 职业 角色 ， 您 可以 了解 下 H * T * 保险 中 的 众 行天下 - 领 * 责任保险 计划 一 ， 含有 高额 领 * 责任 保障 ， 承保 部分 高风险 运动 。 另外 ， 购买 这 款 产品 后 即可 免费 开通 H * T * 领 * 会员 ， 自动 获取 专属 服务卡 ， 这么 高额 的 真的 不多见 ， 不妨 参考 下 。']\n",
      "['我 是 个 导游 ， 近期 带队 很 频繁 ， 有 什么 保险 可以 投 的 ？', '1 年期 的 意外险 、 医疗险 都 可以 投 。 需要 出国 的话 ， 记得 选择 在 国外 也 能 保障 的 产品 。']\n",
      "['请问 下 HUTS 旅游 险 和 一般 的 旅游 保险 有 什么 不 一样 的 ？', '您好 ， HUTS 保险 关于 旅游 类 的 保险产品 ， 它 针对 不同 细分 场景 的 风险 特点 ， 设计 出 了 侧重点 不同 的 保障 责任 ， 为 用户 提供 相关 场景 下 最 需要 的 专属 保障 。 HUTS 将 旅行 户外 保险 分为 7 个大类 ， 含 户外运动 、 观光旅游 、 体育运动 、 出境 旅游 、 探亲 、 商务 出行 、 留学 等 ， 并 细分 出 46 个 场景 ， 包括 一日游 、 自驾游 、 欧 * 旅游 、 户外运动 、 高风险 运动 等 。 HUTS 针对 不同 细分 场景 的 风险 特点 ， 设计 出 了 侧重点 不同 的 保障 责任 ， 为 用户 提供 相关 场景 下 最 需要 的 专属 保障 。 和 一般 的 旅游 类产品 最大 的 不同 在于 ， HUTS 保险 可以 提供 第一 现场 紧急 救援 。']\n",
      "['这个 周末 去 周边 玩 ， 只有 一天 的 时间 ， 要 不要 买 保险 的', '您好 ， 进行 周边 游玩 的话 ， 可以 选择 一份 一天 的 保险 ， 既 划算 也 比较 合理 ， 保障 安全 。 目前 HUTS 保险 一日游 ， 就 专门 针对 一日游 的 朋友 设计 的 ， 价格 很 实惠 ， 一天 一份 只 需要 1 元 ， 但是 保障 却 很 足 ， 高达 10 万 。']\n",
      "['这个 周末 去 周边 玩 ， 只有 一天 的 时间 ， 要 不要 买 保险 的', '如果 是 旅行社 组织 的 短途游 ， 一般 合同 里 含 保险 如果 是 自驾游 的话 ， 建议 平时 可以 购买 个 长期 生效 的 意外险 什么 的 。']\n",
      "['这个 周末 去 周边 玩 ， 只有 一天 的 时间 ， 要 不要 买 保险 的', '我 觉得 没有 自杀 倾向 ， 应该 不 需要 ？']\n",
      "['请问 下 哆 啦 A 保重 疾险 , 买 多久 的 比较 好 ?', '您好 ， 哆 啦 A 保 规定 ， 年龄 在 30 天 - 55 周岁 均 支持 30 年 交费 期间 ， 年交 保费 。 且 带有 豁免 功能 的 保障型 产品 交费 期间 越长 ， 豁免 的 作用 就 发挥 得 越 大 。 所以 ， 建议 按照 30 年 缴费 计算 。']\n",
      "['小孩 太小 没来 的 及 买 医疗保险 怎么办', '您好 ， 孩子 的 抵抗力 比较 弱些 ， 建议 可以 先 办理 社区 保险 ， 作为 基础 。 另外 ， 如果 医疗保险 没有 来得及 办 的话 ， 可以 先给 孩子 投保 重疾险 ， 目前 百年 人寿 公司 推出 的 康惠保 ， 比较 适合 孩子 ， 保障 涵盖 100 种重 疾 以及 30 种轻症 ， 孩子 的 年龄 只要 满 了 28 周天 就 可以 办理 了 。']\n",
      "['小孩 太小 没来 的 及 买 医疗保险 怎么办', '我 可以 帮 您']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "MAX_LENGTH = 1000  # 句子最大长度是10个词(包括EOS等特殊词)\n",
    "\n",
    "# 把Unicode字符串变成ASCII\n",
    "# 参考https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# def normalizeString(s):\n",
    "#     # 变成小写、去掉前后空格，然后unicode变成ascii\n",
    "#     #s = unicodeToAscii(s.lower().strip())\n",
    "#     # 在标点前增加空格，这样把标点当成一个词\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     # 字母和标点之外的字符都变成空格\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#     # 因为把不用的字符都变成空格，所以可能存在多个连续空格\n",
    "#     # 下面的正则替换把多个空格变成一个空格，最后去掉前后空格\n",
    "#     s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "#     return s\n",
    "\n",
    "# 读取问答句对并且返回Voc词典对象 \n",
    "def readVocs(question_cut_list,answer_cut_list):\n",
    "    print(\"Reading lines...\")\n",
    "    pairs = [[question,answer] for (question,answer) in zip(question_cut_list,answer_cut_list)]\n",
    "\n",
    "    voc = Voc(\"baidu_qa\")\n",
    "    return voc, pairs\n",
    "\n",
    "def filterPair(p): \n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# 过滤太长的句对 \n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 使用上面的函数进行处理，返回Voc对象和句对的list \n",
    "def loadPrepareData(question_cut_list,answer_cut_list):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(question_cut_list,answer_cut_list)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "# save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(question_cut_list,answer_cut_list)\n",
    "# 输出一些句对\n",
    "print(\"\\npairs:\")\n",
    "print(f\"len(pairs):{len(pairs)}\")\n",
    "for pair in pairs[:100]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，原来共有221282个句对，经过处理后我们只保留了64271个句对。\n",
    "\n",
    "另外为了收敛更快，我们可以去除掉一些低频词。这可以分为两步：\n",
    "\n",
    "1) 使用voc.trim函数去掉频次低于MIN_COUNT 的词。\n",
    "\n",
    "2) 去掉包含低频词的句子(只保留这样的句子——每一个词都是高频的，也就是在voc中出现的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 1494 / 1494 = 1.0000\n",
      "Trimmed from 100 pairs to 100, 1.0000 of total\n",
      "voc:{'最近': 3, '在': 4, '安邦': 5, '长青树': 6, '中': 7, '看到': 8, '什么': 9, '豁免': 10, '，': 11, '这个': 12, '是': 13, '意思': 14, '？': 15, '您好': 16, '重疾险': 17, '给予': 18, '投保': 19, '者': 20, '的': 21, '一项': 22, '权利': 23, '安': 24, '*': 25, '保障': 26, '责任': 27, '规定': 28, '可以': 29, '享受': 30, '多次': 31, '赔付': 32, '等': 33, '权益': 34, '。': 35, '也就是说': 36, '不同': 37, '轻症': 38, '累计': 39, '5': 40, '次': 41, '理赔': 42, '1': 43, '次轻症': 44, '后期': 45, '所': 46, '交': 47, '保费': 48, '人性化': 49, '设计': 50, '无需': 51, '加': 52, '和': 53, '老婆': 54, '利用': 55, '假期': 56, '去': 57, '澳': 58, '探亲': 59, '但是': 60, '第一次': 61, '不大': 62, '熟悉': 63, '有没有': 64, '相关': 65, '保险': 66, '呢': 67, 'HUTS': 68, '乐游': 69, '全球': 70, '（': 71, '版': 72, '）': 73, '-': 74, '慧择': 75, '旅游': 76, '澳新': 77, '计划': 78, '新西兰': 79, '专属': 80, '承保': 81, '年龄': 82, '可达': 83, '90': 84, '周岁': 85, '含有': 86, '50': 87, '万': 88, '高额': 89, '医疗保障': 90, '完全': 91, '满足': 92, '境外': 93, '需求': 94, '需要': 95, '注意': 96, '这款': 97, '产品': 98, '仅': 99, '出行': 100, '目的': 101, '为': 102, '人群': 103, '时需': 104, '提供': 105, '签证': 106, '或': 107, '亲属关系': 108, '证明': 109, '适合': 110, '帆船': 111, '比赛': 112, '我': 113, '男朋友': 114, '这': 115, '周': 116, '就要': 117, '开始': 118, '了': 119, '水上运动': 120, '尤其': 121, '带有': 122, '奖金': 123, '一般': 124, '公司': 125, '比较': 126, '少': 127, '不过': 128, '众': 129, '行天下': 130, '赛事': 131, 'B': 132, '就是': 133, '水上': 134, '含': 135, '户外': 136, '溺水': 137, '定制': 138, '意外': 139, '住院': 140, '有': 141, '津贴': 142, '期限': 143, '灵活': 144, '可': 145, '选': 146, '还': 147, '您': 148, '根据': 149, '情况': 150, '看看': 151, '端午节': 152, '自驾': 153, '九': 154, '山': 155, '买': 156, '三天': 157, '要': 158, '多少': 159, '钱': 160, '端午': 161, '人': 162, '多': 163, '而且': 164, '存在': 165, '一定': 166, '风险': 167, '所以': 168, '意识': 169, '还是': 170, '很': 171, '好': 172, '考虑': 173, '到': 174, '价格': 175, '以及': 176, '内容': 177, '因素': 178, '畅玩': 179, '神州': 180, '三': 181, '驾驶': 182, '私家车': 183, '走南闯北': 184, '国内': 185, ',': 186, '范围': 187, '也': 188, '较为': 189, '广泛': 190, '带': 191, '家人': 192, '出游': 193, '全面': 194, '需': 195, '75': 196, '元': 197, '性价比': 198, '蛮高': 199, '不到': 200, '10': 201, '块钱': 202, '…': 203, '我们': 204, '准备': 205, '举行': 206, '赛龙舟': 207, '说': 208, '份': 209, '传统': 210, '活动': 211, '意义': 212, '由于': 213, '建议': 214, '安全': 215, '还要': 216, '做足': 217, '中有': 218, '针对': 219, '特别': 220, '这种': 221, '进行': 222, '一系列': 223, '较': 224, '高': 225, '关键': 226, '费用': 227, '实惠': 228, '长': 229, '树': 230, '她': 231, '网上': 232, '以后': 233, '缴费': 234, '怎么办': 235, '这点': 236, '不用': 237, '担心': 238, '后': 239, '保险公司': 240, '会': 241, '约定': 242, '保险费': 243, '交纳': 244, '日': 245, '从': 246, '消费者': 247, '购买': 248, '时': 249, '填写': 250, '银行账号': 251, '中划': 252, '扣': 253, '当期': 254, '应交': 255, '亲自': 256, '听说': 257, 'huts': 258, '申请': 259, '医疗': 260, '垫付': 261, '下': 262, '批': 263, '该': 264, '便是': 265, '解决': 266, '切实': 267, '困难': 268, '以下': 269, '几个': 270, '：': 271, '①': 272, '发生': 273, '重大': 274, '突发': 275, '急性病': 276, '；': 277, '②': 278, '涵盖': 279, '补偿': 280, '③': 281, '客户': 282, '无法': 283, '承担': 284, '团队': 285, '北极': 286, '探险': 287, '这方面': 288, '本身': 289, '就': 290, '选择': 291, '专业': 292, '装备': 293, '专业人士': 294, '陪同': 295, '至于': 296, '市面上': 297, '关于': 298, '此类': 299, '并不多': 300, '却': 301, '一款': 302, '专门': 303, '南北极': 304, '充足': 305, '伤害': 306, '、': 307, '甚至': 308, '紧急': 309, '救援': 310, '都': 311, '具备': 312, '详情': 313, '了解': 314, '欧': 315, '出差': 316, '随身': 317, '物品': 318, '一个': 319, '人生地不熟': 320, '地方': 321, '最怕': 322, '行李': 323, '丢失': 324, '麻烦': 325, '苦恼': 326, '目前': 327, '商务': 328, '必备': 329, '财产': 330, '直付': 331, '顶级': 332, '款': 333, '对': 334, '人士': 335, '不仅仅': 336, '智能手机': 337, '及': 338, '电脑': 339, '行程': 340, '阻碍': 341, '均': 342, '老公': 343, '喜欢': 344, '抽烟': 345, '这样': 346, '投': 347, '不了': 348, '真的': 349, '吗': 350, '要求': 351, '严格': 352, '投保人': 353, '一些': 354, '行为习惯': 355, '有所': 356, '限制': 357, '酗酒': 358, '如果': 359, '每天': 360, '数量': 361, '不': 362, '因为': 363, '符合': 364, '健康': 365, '告知': 366, '条件': 367, '核保': 368, '通过': 369, '出现': 370, '拒保': 371, '建议您': 372, '意外险': 373, '依': 374, '哥们儿': 375, '经历': 376, '来看': 377, '应该': 378, '循续': 379, '渐进': 380, '过程': 381, '上学时': 382, '别人': 383, '吐': 384, '烟圈': 385, '貌似': 386, '蛮酷': 387, '于是': 388, '跟着': 389, '学习': 390, '时间': 391, '一常': 392, '想戒': 393, '难': 394, '另外': 395, '不可否认': 396, '特定': 397, '对于': 398, '某些': 399, '吸烟': 400, '有助于': 401, '思考': 402, '确实': 403, '适当': 404, '缓解': 405, '紧张': 406, '情绪': 407, '周末': 408, '天气': 409, '不错': 410, '看着': 411, '心情': 412, '舒坦': 413, '朋友': 414, '拓展': 415, '训练': 416, '各类': 417, '户外活动': 418, '它': 419, '官': 420, '33': 421, '种': 422, '热门': 423, '运动': 424, '列': 425, '并且': 426, '松': 427, '65': 428, '必要': 429, '时候': 430, '第一': 431, '现场': 432, '已经': 433, '其他': 434, '投康惠保': 435, '能行': 436, '没有': 437, '冲突': 438, '但康惠': 439, '保有': 440, '最高': 441, '保额': 442, '被保险人': 443, '百年': 444, '人寿': 445, '持有': 446, '保单': 447, '重疾': 448, '出生': 449, '满': 450, '28': 451, '天': 452, '40': 453, '超过': 454, '41': 455, '30': 456, '51': 457, '55': 458, '问题': 459, '根本': 460, '没法': 461, '回答': 462, '好比': 463, '你': 464, '爱人': 465, '吃': 466, '一碗': 467, '饭': 468, '想': 469, '再': 470, '没': 471, '饭量': 472, '怎么': 473, '知道': 474, '能': 475, '不能': 476, '无从': 477, '家庭': 478, '大事': 479, '不好': 480, '祸害': 481, '一生': 482, '单位': 483, '社保': 484, '现在': 485, '又': 486, '哆': 487, '啦': 488, 'A': 489, '保': 490, '属于': 491, '最为': 492, '基础': 493, '而': 494, '两种': 495, '方式': 496, '功能': 497, '相互': 498, '补充': 499, '确诊': 500, '患': 501, '疾病': 502, '经': 503, '核定': 504, '按照': 505, '合同': 506, '保险金': 507, '一次性': 508, '不管': 509, '实际': 510, '治疗': 511, '发票': 512, '清单': 513, '获得': 514, '用于': 515, '弥补': 516, '患病': 517, '收入': 518, '损失': 519, '还有': 520, '或者': 521, '医疗保险': 522, '凭': 523, '单据': 524, '报销': 525, '只是': 526, 'a': 527, '宝': 528, '商业保险': 529, '类型': 530, '先买': 531, '等等': 532, '景': 533, '生产线': 534, '屯阶': 535, '上部': 536, '狼': 537, '小': 538, '纸质': 539, '怎么回事': 540, '完成': 541, '电子': 542, '直接': 543, '发送至': 544, '小叔': 545, '邮箱': 546, '保证': 547, '合法权益': 548, '访问': 549, '弘康': 550, '人寿保险': 551, '网址': 552, '拨打': 553, '客户服务': 554, '热线': 555, '信息': 556, '查询': 557, '好多': 558, '专为': 559, '部分': 560, '高风险': 561, '“': 562, '安心': 563, '”': 564, '综合': 565, '运动会': 566, '业余': 567, '专享': 568, '特含': 569, '运送': 570, '很多': 571, '不保': 572, '主要': 573, '是因为': 574, '风险系数': 575, '多数': 576, '以上': 577, '那款': 578, '请问': 579, '乙肝': 580, '被': 581, '拒绝': 582, '乙肝病毒': 583, '携带者': 584, '若': 585, '加费': 586, '康惠保': 587, '以外': 588, '空闲': 589, '着': 590, '儿子': 591, '两个': 592, '人去': 593, '周边': 594, '玩': 595, '一天': 596, '?': 597, '游玩': 598, '并不大': 599, '买份': 600, '妻子': 601, '孩子': 602, '并': 603, '不长': 604, '只有': 605, '一日游': 606, '日游': 607, '拥有': 608, '便宜': 609, '逐渐': 610, '转热': 611, '防暑': 612, '险': 613, '啊': 614, '旅行': 615, '有场': 616, '拳击': 617, '参考': 618, '女朋友': 619, '马虎': 620, '这次': 621, '美': 622, '给': 623, '无处不在': 624, '防止': 625, '重要': 626, '路途': 627, '遥远': 628, '旅途': 629, '常见': 630, '航班': 631, '延误': 632, '为什么': 633, '职业': 634, '上': 635, '中学': 636, '填': 637, '下重': 638, '疾会': 639, '允许': 640, '4': 641, '类': 642, '上学': 643, '按': 644, '其': 645, '实际上': 646, '学': 647, '学龄前': 648, '儿童': 649, '学生': 650, '保对': 651, '岁': 652, '小女孩': 653, '(': 654, ')': 655, '年龄段': 656, '160': 657, '不怕': 658, '无': 659, '守护': 660, '覆盖': 661, '全': 662, '周期': 663, '隐患': 664, '分组': 665, '忧虑': 666, '智能': 667, '更': 668, '方便': 669, '功能完善': 670, '是不是': 671, '犹豫': 672, '期': 673, '退保': 674, '解除': 675, '保险合同': 676, '解除合同': 677, '申请书': 678, '向': 679, '有效': 680, '身份证件': 681, '自': 682, '收到': 683, '时起': 684, '终止': 685, '无息': 686, '退还': 687, '自然': 688, '不会': 689, '当然': 690, '问下': 691, '身故': 692, '赔偿金': 693, '赔': 694, '给付': 695, '用处': 696, '好不好': 697, '一般来说': 698, '越长越': 699, '豁免权': 700, '那么': 701, '期限内': 702, '出险': 703, '后面': 704, '那': 705, '越长': 706, '经济': 707, '压力': 708, '越小嘛': 709, '正常': 710, '来说': 711, '杠杆': 712, '率越': 713, '越': 714, '康惠': 715, '保能': 716, '做到': 717, '年': 718, '终身': 719, '同类产品': 720, '中是': 721, '出色': 722, '扣费': 723, '做': 724, '会扣': 725, '只能': 726, '退': 727, '现金': 728, '价值': 729, '已交': 730, '比': 731, '差': 732, '留学': 733, '前': 734, '三个': 735, '月': 736, '国际型': 737, '医保': 738, '留学生': 739, '前期': 740, '学校': 741, '办理': 742, '提前': 743, '加上': 744, '国外': 745, '显得': 746, '尤为重要': 747, '前程似锦': 748, '海外': 749, '游学': 750, '贵': 751, '任意': 752, '空白': 753, '针对性': 754, '很强': 755, '谁': 756, '务工': 757, 'C': 758, '工作': 759, '派遣': 760, '期间': 761, '至': 762, '12': 763, '可选': 764, '18': 765, '去年': 766, '下半年': 767, '大叔': 768, '原位癌': 769, '种轻症': 770, '其中': 771, '一种': 772, '继续': 773, '如': 774, '罹患': 775, '54': 776, '105': 777, '种重': 778, '疾': 779, '仍': 780, '当前': 781, '救助': 782, '虽然': 783, '险中': 784, '表示': 785, '基本上': 786, '第二': 787, '这部分': 788, '实现': 789, '及时': 790, '援助': 791, '远比': 792, '事后': 793, '切入点': 794, '让': 795, '人们': 796, '无忧': 797, '真正': 798, '服务': 799, '必须': 800, '赶往': 801, '获取': 802, '路径': 803, '关注': 804, '官方': 805, '微信': 806, '—': 807, '注册': 808, '登录': 809, '点开': 810, '页面': 811, '点击': 812, '左下': 813, '机器人': 814, '按钮': 815, '确认': 816, '电子邮箱': 817, '无误': 818, '下载': 819, '即可': 820, '注': 821, '自动': 822, '生成': 823, '状态': 824, '已过': 825, '70': 826, '期好': 827, '大': 828, '的话': 829, '定期': 830, '方案': 831, '低': 832, '率': 833, '我国': 834, '人均': 835, '寿命': 836, '多岁': 837, '因此': 838, '富裕': 839, '来': 840, '收益': 841, '之后': 842, '归零': 843, '且': 844, '一辈子': 845, '身患': 846, '拿到': 847, '可观': 848, '形态': 849, '包括': 850, '主险': 851, '附加': 852, '必选': 853, '包含': 854, '重症': 855, '典型': 856, '消费型': 857, '但': 858, '返还': 859, '付现金': 860, '闺女': 861, '体验': 862, '野外': 863, '亲子': 864, '父母': 865, '共同': 866, '参与': 867, '慧游游': 868, '一': 869, '不高': 870, '北': 871, '是否': 872, '常住': 873, '地': 874, '湖': 875, '河': 876, '辽': 877, '黑': 878, '四': 879, '福': 880, '陕': 881, '内': 882, '古': 883, '吉': 884, '浙': 885, '广': 886, '重': 887, '这些': 888, '地区': 889, '方可': 890, '影响': 891, '迁入地': 892, '分支': 893, '支持': 894, '迁移': 895, '保全': 896, '操作': 897, '柜面': 898, '暂时': 899, '安排': 900, '就近': 901, '只要': 902, '达到': 903, '出国': 904, '护照': 905, '防患于未然': 906, '钻石': 907, '100': 908, '证件': 909, '游客': 910, '而言': 911, '出': 912, '固然': 913, '欣赏': 914, '异': 915, '风情': 916, '语言': 917, '环境': 918, '加大': 919, '自身': 920, '之前': 921, '自己': 922, '一份': 923, '合适': 924, '强': 925, '具体': 926, '前往': 927, '家': 928, '选购': 929, '归属': 930, '更高': 931, '以': 932, '几点': 933, '.': 934, '消费水平': 935, '2': 936, '全部': 937, '使得': 938, '全程': 939, '之中': 940, '3': 941, '出境': 942, '际': 943, '网点': 944, '很难': 945, '铺': 946, '外': 947, '通常': 948, '与': 949, '合作': 950, '无论是': 951, '在外': 952, '遗失': 953, '钱包': 954, '致电': 955, '汉语': 956, '平台': 957, '对比': 958, '多家': 959, '进而': 960, '最合适': 961, '非常简单': 962, '网': 963, '首页': 964, '免费': 965, '会员': 966, '登陆': 967, '中意': 968, '立即': 969, '价格便宜': 970, '整个': 971, '清晰': 972, '透明': 973, '免去': 974, '中介代理': 975, '层层': 976, '自主性': 977, '更强': 978, '首先': 979, '种类': 980, '然后': 981, '网是': 982, '欢迎您': 983, '前来': 984, '乐悠游': 985, '二': 986, '华泰': 987, '计划经济': 988, '展开': 989, '一下': 990, '平安': 991, '发现': 992, '得': 993, '脑膜炎': 994, '38': 995, '37': 996, '80': 997, '小额': 998, '快': 999, '3000': 1000, '金额': 1001, '以内': 1002, '绿色': 1003, '快速通道': 1004, '为了': 1005, '更加': 1006, '快捷': 1007, '优势': 1008, '体现': 1009, '方面': 1010, '比非': 1011, '闪': 1012, '更少': 1013, '材料': 1014, '绿色通道': 1015, '闪电般': 1016, '审核': 1017, '速度': 1018, '线上': 1019, '便捷': 1020, '下澳': 1021, '利': 1022, '公差': 1023, '星期': 1024, '商旅': 1025, '增多': 1026, '相应': 1027, '财务': 1028, '这是': 1029, '既往': 1030, '症': 1031, '急救': 1032, '天仅': 1033, '有个': 1034, '叫': 1035, '几元': 1036, '想要': 1037, '晓得': 1038, '什么样': 1039, '相对而言': 1040, '天下': 1041, '大多数': 1042, '将': 1043, '赴': 1044, '吴': 1045, '登乐游': 1046, '原一绝': 1047, '杜': 1048, '保中': 1049, '相对': 1050, '于': 1051, '其实': 1052, '大病': 1053, '更为': 1054, '多发': 1055, '同时': 1056, '往往': 1057, '潜伏': 1058, '阶段': 1059, '早期': 1060, '积极': 1061, '才能': 1062, '往重': 1063, '转变': 1064, '应': 1065, '简单': 1066, '较轻': 1067, '对人': 1068, '接受': 1069, '恢复健康': 1070, '所谓': 1071, '指': 1072, '期内': 1073, '保人': 1074, '残疾': 1075, '由': 1076, '获准': 1077, '同意': 1078, '不再': 1079, '缴纳': 1080, '后续': 1081, '仍然': 1082, '树对': 1083, '各个': 1084, '设有': 1085, '深': 1086, '25': 1087, '8': 1088, '16': 1089, '销售': 1090, '一样': 1091, '本周': 1092, '团建': 1093, '安全系数': 1094, '挺': 1095, '人员': 1096, '比如说': 1097, '素拓': 1098, '享有': 1099, '患重': 1100, '条款': 1101, '明确': 1102, '说明': 1103, '首次': 1104, '简言之': 1105, '只': 1106, '高残': 1107, '下次': 1108, '德': 1109, '办': 1110, '哪些': 1111, '医院': 1112, '指定': 1113, '境内': 1114, '二级': 1115, '公立医院': 1116, '就诊': 1117, '提醒': 1118, '主治医生': 1119, '使用': 1120, '用药': 1121, '诊疗': 1122, '项目': 1123, '服务设施': 1124, '请': 1125, '妥善': 1126, '保存': 1127, '病历': 1128, '首': 1129, '诊病': 1130, '历': 1131, '原始': 1132, '收费': 1133, '凭证': 1134, '处方': 1135, '诊断': 1136, '检查': 1137, '化验': 1138, '报告': 1139, '就医': 1140, '索赔': 1141, '姐姐': 1142, '为啥': 1143, '同样': 1144, '《': 1145, '中华人民': 1146, '共和': 1147, '合同法': 1148, '》': 1149, '数据': 1150, '电文': 1151, '合法': 1152, '表现形式': 1153, '具有': 1154, '同等': 1155, '法律效力': 1156, '人身保险': 1157, '业务': 1158, '基本': 1159, '第十五条': 1160, '生效': 1161, '电话': 1162, '回访': 1163, '保险产品': 1164, '提示': 1165, '女儿': 1166, '刚': 1167, '医保卡': 1168, '下来': 1169, '可能': 1170, '不够': 1171, '下面': 1172, '专': 1173, '发达国家': 1174, '消费': 1175, '国家': 1176, '个人': 1177, '中断': 1178, '方方面面': 1179, '孤身': 1180, '照应': 1181, '心肌': 1182, '桥': 1183, '患者': 1184, '不予': 1185, '健康险': 1186, '类似': 1187, '详细': 1188, '程序': 1189, '除非': 1190, '除外': 1191, '个': 1192, '导游': 1193, '领队': 1194, '责任险': 1195, '肩负着': 1196, '第三者': 1197, '有众': 1198, '责任保险': 1199, '出于': 1200, '用户': 1201, '量身定做': 1202, '等待': 1203, '长不长': 1204, '例外': 1205, '期为': 1206, '终末期': 1207, '全残': 1208, '因': 1209, '导致': 1210, '上述': 1211, '事故': 1212, '沈': 1213, '病重': 1214, '去世': 1215, '臧姑': 1216, '亦': 1217, '悔不当初': 1218, '决定': 1219, '重新做人': 1220, '韩': 1221, '可靠': 1222, '乐': 1223, '旅': 1224, '花粉': 1225, '过敏': 1226, '急性': 1227, '发作': 1228, '产品设计': 1229, '相当': 1230, '60': 1231, '作为': 1232, '打算': 1233, '来场': 1234, '游': 1235, '市': 1236, '内外': 1237, '首选': 1238, '大人': 1239, '小孩': 1240, '最大': 1241, '特色': 1242, '协助': 1243, '送返': 1244, '重疾是': 1245, '型': 1246, '一类': 1247, '险种': 1248, '特点': 1249, '到期': 1250, '保等': 1251, '两年': 1252, '受欢迎': 1253, '故而': 1254, '希望': 1255, '把': 1256, '预算': 1257, '用': 1258, '不是': 1259, '理财': 1260, '储蓄': 1261, '混为一谈': 1262, '身上': 1263, '较少': 1264, '外国': 1265, 'b': 1266, '已': 1267, '各省市': 1268, '达': 1269, '200': 1270, '优普': 1271, '美亚': 1272, 'Tr': 1273, '#######': 1274, 'rd': 1275, 'SOS': 1276, '知名': 1277, '机构': 1278, '达成': 1279, '下周': 1280, '小学生': 1281, '足球比赛': 1282, '踢足球': 1283, '哪有': 1284, '容易': 1285, '每个': 1286, '找款': 1287, '境外游': 1288, '额度': 1289, '外出': 1290, '力度': 1291, '十分': 1292, '高性价比': 1293, '之选': 1294, '190': 1295, '短期': 1296, '学子': 1297, '买个': 1298, '极短': 1299, '搞': 1300, '几百块': 1301, '跑步': 1302, '猝死': 1303, '运动版': 1304, '/': 1305, '下康惠保': 1306, '年限': 1307, '15': 1308, '20': 1309, '四种': 1310, '邻居': 1311, '半年': 1312, '查出': 1313, '查查': 1314, '20%': 1315, '6': 1316, '万元': 1317, '原因': 1318, '乘坐': 1319, '飞机': 1320, '坐飞机': 1321, '像': 1322, '交通工具': 1323, '一旦': 1324, '出事': 1325, '后果': 1326, '不堪设想': 1327, '以防万一': 1328, '嘛': 1329, '越高越': 1330, '最好': 1331, '慧': 1332, '飞行': 1333, '航空': 1334, '全年': 1335, '五百万': 1336, '叠加': 1337, '最多': 1338, '适用人群': 1339, '经常': 1340, '乘飞机': 1341, '商务人士': 1342, '大家': 1343, '网站': 1344, '好处': 1345, '非常': 1346, '快速': 1347, '爱好': 1348, '喝点': 1349, '小酒': 1350, '饮酒': 1351, '频繁': 1352, '大部分': 1353, '优先': 1354, '老板': 1355, '一个月': 1356, '展会': 1357, '保险行业': 1358, '协会': 1359, '对轻症': 1360, '统一': 1361, '标准': 1362, '造成': 1363, '定义': 1364, '列举': 1365, '几种': 1366, '吧': 1367, '轻微': 1368, '脑中风': 1369, '梗': 1370, '面积': 1371, 'III': 1372, '度': 1373, '烧伤': 1374, '视力': 1375, '严重': 1376, '受损': 1377, '冠状动脉': 1378, '介入': 1379, '手术': 1380, '主动脉': 1381, '脑垂体': 1382, '瘤': 1383, '脑': 1384, '囊肿': 1385, '动脉瘤': 1386, '及脑': 1387, '血管瘤': 1388, '癌症': 1389, '起来': 1390, '切除': 1391, '病灶': 1392, '生存率': 1393, '纳入': 1394, '俱乐部': 1395, '羽毛球': 1396, '参赛者': 1397, '爱好者': 1398, '团体': 1399, '组织': 1400, '竞赛': 1401, '越来越': 1402, '发生意外': 1403, '机率': 1404, '随之': 1405, '增加': 1406, '奖品': 1407, '性质': 1408, '参加': 1409, '普通': 1410, '体育竞赛': 1411, '田径': 1412, '游泳': 1413, '球类运动': 1414, '意外事故': 1415, '项目风险': 1416, '该项': 1417, '以策': 1418, '万全': 1419, '可是': 1420, '拒赔': 1421, '攀岩': 1422, '徒步': 1423, '户外运动': 1424, '溯溪': 1425, '得严': 1426, '大大': 1427, '节省': 1428, '提醒您': 1429, '既': 1430, '人工': 1431, '便利': 1432, '康': 1433, '观察期': 1434, '逆': 1435, '180': 1436, '近期': 1437, '带队': 1438, '角色': 1439, 'H': 1440, 'T': 1441, '领': 1442, '开通': 1443, '服务卡': 1444, '这么': 1445, '不多见': 1446, '不妨': 1447, '年期': 1448, '医疗险': 1449, '记得': 1450, '细分': 1451, '场景': 1452, '侧重点': 1453, '最': 1454, '分为': 1455, '7': 1456, '个大类': 1457, '观光旅游': 1458, '体育运动': 1459, '46': 1460, '自驾游': 1461, '类产品': 1462, '在于': 1463, '不要': 1464, '划算': 1465, '合理': 1466, '足': 1467, '高达': 1468, '旅行社': 1469, '短途游': 1470, '里': 1471, '平时': 1472, '长期': 1473, '觉得': 1474, '自杀': 1475, '倾向': 1476, '保重': 1477, '疾险': 1478, '多久': 1479, '交费': 1480, '年交': 1481, '保障型': 1482, '作用': 1483, '发挥': 1484, '计算': 1485, '太小': 1486, '没来': 1487, '抵抗力': 1488, '弱些': 1489, '先': 1490, '社区': 1491, '来得及': 1492, '先给': 1493, '推出': 1494, '周天': 1495, '帮': 1496}\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 1    # 阈值为1\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # 去掉voc中频次小于1的词 \n",
    "    voc.trim(MIN_COUNT)\n",
    "    # 保留的句对 \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # 检查问题\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # 检查答案\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # 如果问题和答案都只包含高频词，我们才保留这个句对\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), \n",
    "\t\tlen(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# 实际进行处理\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "print(\"voc:{0}\".format(voc.word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6305个词之中，频次大于等于1的只有100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  为模型准备数据\n",
    "\n",
    "前面我们构建了词典，并且对训练数据进行预处理并且滤掉一些句对，但是模型最终用到的是Tensor。最简单的办法是一次处理一个句对，那么上面得到的句对直接就可以使用。但是为了加快训练速度，尤其是重复利用GPU的并行能力，我们需要一次处理一个batch的数据。\n",
    "\n",
    "对于某些问题，比如图像来说，输入可能是固定大小的(或者通过预处理缩放成固定大小），但是对于文本来说，我们很难把一个二十个词的句子”缩放”成十个词同时还保持语义不变。\n",
    "\n",
    "但是为了充分利用GPU等计算自由，我们又必须变成固定大小的Tensor，因此我们通常会使用Padding的技巧，把短的句子补充上零使得输入大小是(batch, max_length)，这样通过一次就能实现一个batch数据的forward或者backward计算。当然padding的部分的结果是没有意义的，比如某个句子实际长度是5，而max_length是10，那么最终forward的输出应该是第5个时刻的输出，后面5个时刻计算是无用功。\n",
    "\n",
    "方向计算梯度的时候也是类似的，我们需要从第5个时刻开始反向计算梯度。为了提高效率，我们通常把长度接近的训练数据放到一个batch里面，这样无用的计算是最少的。因此我们通常把全部训练数据根据长度划分成一些组，比如长度小于4的一组，长度4到8的一组，长度8到12的一组，…。然后每次随机的选择一个组，再随机的从一组里选择batch个数据。不过本教程并没有这么做，而是每次随机的从所有pair里随机选择batch个数据。\n",
    "\n",
    "原始的输入通常是batch个list，表示batch个句子，因此自然的表示方法为(batch, max_length)，这种表示方法第一维是batch，每移动一个下标得到的是一个样本的max_length个词(包括padding)。因为RNN的依赖关系，我们在计算t+1时刻必须知道t时刻的结果，因此我们无法用多个核同时计算一个样本的forward。但是不同样本之间是没有依赖关系的，因此我们可以在根据t时刻batch样本的当前状态计算batch个样本的输出和新状态，然后再计算t+2时刻，…。为了便于GPU一次取出t时刻的batch个数据，我们通常把输入从(batch, max_length)变成(max_length, batch)，这样使得t时刻的batch个数据在内存(显存)中是连续的，从而读取效率更高。这个过程如下图所示，原始输入的大小是(batch=6, max_length=4)，转置之后变成(4,6)。这样某个时刻的6个样本数据在内存中是连续的。\n",
    "\n",
    "![avatar](img/seq2seq_batches.png)\n",
    "\n",
    "因此我们会用一些工具函数来实现上述处理。\n",
    "\n",
    "inputVar函数把batch个句子padding后变成一个LongTensor，大小是(max_length, batch)，同时会返回一个大小是batch的list lengths，说明每个句子的实际长度，这个参数后面会传给PyTorch，从而在forward和backward计算的时候使用实际的长度。\n",
    "\n",
    "outputVar函数和inputVar类似，但是它输出的第二个参数不是lengths，而是一个大小为(max_length, batch)的mask矩阵(tensor)，某位是0表示这个位置是padding，1表示不是padding，这样做的目的是后面计算方便。当然这两种表示是等价的，只不过lengths表示更加紧凑，但是计算起来不同方便，而mask矩阵和outputVar直接相乘就可以把padding的位置给mask(变成0)掉，这在计算loss时会非常方便。\n",
    "\n",
    "batch2TrainData 则利用上面的两个函数把一个batch的句对处理成合适的输入和输出Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[ 347,   12,  161,  579,  579],\n",
      "        [ 119,  408,  204,  262,  262],\n",
      "        [  24,   57,  205,   11, 1422],\n",
      "        [  25,  594,  158,  580, 1423],\n",
      "        [ 229,  595,  206,  475,   33],\n",
      "        [  25,   11,  207,  476, 1424],\n",
      "        [ 230,  605,   11,  156,   11],\n",
      "        [ 270,  596,  208,  435,  156],\n",
      "        [ 736,   21,   13,   15,    9],\n",
      "        [ 239,  391,  158,  241,   66],\n",
      "        [  11,   11,  209,  581,  224],\n",
      "        [ 992,  158,   66,  582,  172],\n",
      "        [ 993, 1464,   11,  350,   15],\n",
      "        [ 119,  156,    9,   15,    2],\n",
      "        [ 994,   66,  172,    2,    0],\n",
      "        [  11,   21,    2,    0,    0],\n",
      "        [  12,    2,    0,    0,    0],\n",
      "        [  29,    0,    0,    0,    0],\n",
      "        [ 525,    0,    0,    0,    0],\n",
      "        [ 350,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([21, 17, 16, 15, 14])\n",
      "target_variable: tensor([[  16,  359,   16,  241,   16],\n",
      "        [  11,   13,   11,   21,   11],\n",
      "        [  12, 1469,  207,   11,   68],\n",
      "        [  13, 1400,   13,  156,   66],\n",
      "        [  29,   21,   22,  348,    7],\n",
      "        [ 525, 1470,  126,    2,   81],\n",
      "        [  21,   11,  210,    0, 1422],\n",
      "        [  35,  124,   21,    0,  307],\n",
      "        [ 363,  506,  211,    0, 1423],\n",
      "        [ 994, 1471,   11,    0, 1425],\n",
      "        [ 491,  135,  171,    0,  381],\n",
      "        [   5,   66,  141,    0,    7],\n",
      "        [   6,  359,  212,    0,  417],\n",
      "        [  26,   13,   35,    0,  167],\n",
      "        [  21, 1461,  128,    0,   21],\n",
      "        [ 995,  829,  213,    0,  129],\n",
      "        [ 770,   11,   13,    0,  130],\n",
      "        [  21,  214,    4,    0,   74],\n",
      "        [ 771, 1472,  134,    0,  565],\n",
      "        [ 772,   29,  211,    0, 1424],\n",
      "        [  11,  248,   11,    0,   66],\n",
      "        [ 240, 1192,  214,    0,   78],\n",
      "        [ 695, 1473,  215,    0,  986],\n",
      "        [  38, 1161,   26,    0,   11],\n",
      "        [ 502,   21,  216,    0,  110],\n",
      "        [ 507,  373,  217,    0,   43],\n",
      "        [  11,    9,   11,    0,   74],\n",
      "        [ 426,   21,   68,    0,  428],\n",
      "        [ 676,   35,   66,    0,   85],\n",
      "        [ 773,    2,  218,    0,   21],\n",
      "        [ 680,    0,  219,    0,  103],\n",
      "        [  35,    0,  120,    0,   19],\n",
      "        [  14,    0,  167,    0,   11],\n",
      "        [ 133,    0,  220,    0,   81],\n",
      "        [  11,    0,  138,    0,  561],\n",
      "        [ 359,    0,   21,    0,  211],\n",
      "        [  45,    0,  129,    0,   11],\n",
      "        [ 775,    0,  130,    0,   29],\n",
      "        [ 434,    0,   74,    0,   26],\n",
      "        [ 996,    0,  120,    0, 1425],\n",
      "        [ 770,    0,   66,    0,  381],\n",
      "        [  53,    0,   11,    0,    7],\n",
      "        [ 997,    0,   29,    0,   21],\n",
      "        [ 778,    0,  219,    0,  417],\n",
      "        [ 779,    0,  221,    0,  167],\n",
      "        [ 780,    0,  131,    0,   11],\n",
      "        [ 145,    0,  222,    0,   26],\n",
      "        [ 514,    0,   26,    0,  143],\n",
      "        [  32,    0,   11,    0,  144],\n",
      "        [  35,    0,   86,    0,  145],\n",
      "        [   2,    0,  139,    0,  146],\n",
      "        [   0,    0,  140,    0,   11],\n",
      "        [   0,    0,  141,    0,   13],\n",
      "        [   0,    0,  142,    0,  302],\n",
      "        [   0,    0,  176,    0,  171],\n",
      "        [   0,    0,  223,    0,  410],\n",
      "        [   0,    0,   26,    0,   21],\n",
      "        [   0,    0,   11,    0,   98],\n",
      "        [   0,    0,  198,    0,   35],\n",
      "        [   0,    0,  224,    0,    2],\n",
      "        [   0,    0,  225,    0,    0],\n",
      "        [   0,    0,   11,    0,    0],\n",
      "        [   0,    0,  226,    0,    0],\n",
      "        [   0,    0,   13,    0,    0],\n",
      "        [   0,    0,  227,    0,    0],\n",
      "        [   0,    0,  188,    0,    0],\n",
      "        [   0,    0,  126,    0,    0],\n",
      "        [   0,    0,  228,    0,    0],\n",
      "        [   0,    0,   35,    0,    0],\n",
      "        [   0,    0,    2,    0,    0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 70\n"
     ]
    }
   ],
   "source": [
    "# 把句子的词变成ID\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# l是多个长度不同句子(list)，使用zip_longest padding成定长，长度为最长句子的长度。\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "# l是二维的padding后的list\n",
    "# 返回m和l的大小一样，如果某个位置是padding，那么值为0，否则为1\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 把输入句子变成ID，然后再padding，同时返回lengths这个list，标识实际长度。\n",
    "# 返回的padVar是一个LongTensor，shape是(batch, max_length)，\n",
    "# lengths是一个list，长度为(batch,)，表示每个句子的实际长度。\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# 对输出句子进行padding，然后用binaryMatrix得到每个位置是padding(0)还是非padding，\n",
    "# 同时返回最大最长句子的长度(也就是padding后的长度)\n",
    "# 返回值padVar是LongTensor，shape是(batch, max_target_length)\n",
    "# mask是ByteTensor，shape也是(batch, max_target_length)\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# 处理一个batch的pair句对 \n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    # 按照句子的长度(词数)排序\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# 示例\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到input_variable的每一列表示一个样本，而每一行表示batch(5)个样本在这个时刻的值。而lengths表示真实的长度。类似的target_variable也是每一列表示一个样本，而mask的shape和target_variable一样，如果某个位置是0，则表示padding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "### Seq2Seq 模型\n",
    "我们这个chatbot的核心是一个sequence-to-sequence(seq2seq)模型。 \n",
    "\n",
    "seq2seq模型的输入是一个变长的序列，而输出也是一个变长的序列。而且这两个序列的长度并不相同。\n",
    "\n",
    "一般我们使用RNN来处理变长的序列，Sutskever等人的论文发现通过使用两个RNN可以解决这类问题。\n",
    "\n",
    "这类问题的输入和输出都是变长的而且长度不一样，包括问答系统、机器翻译、自动摘要等等都可以使用seq2seq模型来解决。其中一个RNN叫做Encoder，它把变长的输入序列编码成一个固定长度的context向量，我们一般可以认为这个向量包含了输入句子的语义。而第二个RNN叫做Decoder，初始隐状态是Encoder的输出context向量，输入是(表示句子开始的特殊Token)，然后用RNN计算第一个时刻的输出，接着用第一个时刻的输出和隐状态计算第二个时刻的输出和新的隐状态，...，直到某个时刻输出特殊的(表示句子结束的特殊Token)或者长度超过一个阈值。Seq2Seq模型如下图所示。\n",
    "\n",
    "![avatar](img/seq2seq_ts.png)\n",
    "\n",
    "### Encoder\n",
    "Encoder是个RNN，它会遍历输入的每一个Token(词)，每个时刻的输入是上一个时刻的隐状态和输入，然后会有一个输出和新的隐状态。这个新的隐状态会作为下一个时刻的输入隐状态。每个时刻都有一个输出，对于seq2seq模型来说，我们通常只保留最后一个时刻的隐状态，认为它编码了整个句子的语义，但是后面我们会用到Attention机制，它还会用到Encoder每个时刻的输出。Encoder处理结束后会把最后一个时刻的隐状态作为Decoder的初始隐状态。\n",
    "\n",
    "实际我们通常使用多层的Gated Recurrent Unit(GRU)或者LSTM来作为Encoder，这里使用GRU，读者可以参考Cho等人2014年的[论文]。\n",
    "\n",
    "此外我们会使用双向的RNN，如下图所示。\n",
    "\n",
    "![avatar](img/RNN-bidirectional.png)\n",
    "\n",
    "注意在接入RNN之前会有一个embedding层，用来把每一个词(ID或者one-hot向量)映射成一个连续的稠密的向量，我们可以认为这个向量编码了一个词的语义。在我们的模型里，我们把它的大小定义成和RNN的隐状态大小一样(但是并不是一定要一样)。有了Embedding之后，模型会把相似的词编码成相似的向量(距离比较近)。\n",
    "\n",
    "最后，为了把padding的batch数据传给RNN，我们需要使用下面的两个函数来进行pack和unpack，后面我们会详细介绍它们。这两个函数是：\n",
    "\n",
    "    torch.nn.utils.rnn.pack_padded_sequence\n",
    "    torch.nn.utils.rnn.pad_packed_sequence\n",
    "#### 计算图:\n",
    "\n",
    "1) 把词的ID通过Embedding层变成向量。 \n",
    "\n",
    "2) 把padding后的数据进行pack。 \n",
    "\n",
    "3) 传入GRU进行Forward计算。 \n",
    "\n",
    "4) Unpack计算结果 \n",
    "\n",
    "5) 把双向GRU的结果向量加起来。 \n",
    "\n",
    "6) 返回(所有时刻的)输出和最后时刻的隐状态。\n",
    "\n",
    "##### 输入:\n",
    "\n",
    "    input_seq: 一个batch的输入句子，shape是(max_length, batch_size)\n",
    "    input_lengths: 一个长度为batch的list，表示句子的实际长度。\n",
    "    hidden: 初始化隐状态(通常是零)，shape是(n_layers x num_directions, batch_size, hidden_size)\n",
    "\n",
    "##### 输出:\n",
    "\n",
    "    outputs: 最后一层GRU的输出向量(双向的向量加在了一起)，shape(max_length, batch_size, hidden_size)\n",
    "    hidden: 最后一个时刻的隐状态，shape是(n_layers x num_directions, batch_size, hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderRNN代码如下，请读者详细阅读注释。\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # 初始化GRU，这里输入和hidden大小都是hidden_size，\n",
    "        # 这里假设embedding层的输出大小是hidden_size\n",
    "        # 如果只有一层，那么不进行Dropout，否则使用传入的参数dropout进行GRU的Dropout。\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 输入是(max_length, batch)，Embedding之后变成(max_length, batch, hidden_size)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        # 因为RNN(GRU)要知道实际长度，\n",
    "        # 所以PyTorch提供了函数pack_padded_sequence把输入向量和长度\n",
    "        # pack到一个对象PackedSequence里，这样便于使用。\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # 通过GRU进行forward计算，需要传入输入和隐变量\n",
    "        # 如果传入的输入是一个Tensor (max_length, batch, hidden_size)\n",
    "        # 那么输出outputs是(max_length, batch, hidden_size*num_directions)。\n",
    "        # 第三维是hidden_size和num_directions的混合，\n",
    "        # 它们实际排列顺序是num_directions在前面，\n",
    "        # 因此我们可以使用outputs.view(seq_len, batch, num_directions, hidden_size)\n",
    "        # 得到4维的向量。其中第三维是方向，第四位是隐状态。\n",
    "        \n",
    "        # 而如果输入是PackedSequence对象，那么输出outputs也是一个PackedSequence对象，\n",
    "        # 我们需要用\n",
    "        # 函数pad_packed_sequence把它变成shape为(max_length, batch, hidden*num_directions)\n",
    "        # 的向量以及一个list，表示输出的长度，\n",
    "        # 当然这个list和输入的input_lengths完全一样，因此通常我们不需要它。\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # 参考前面的注释，我们得到outputs为(max_length, batch, hidden*num_directions)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 我们需要把输出的num_directions双向的向量加起来\n",
    "        # 因为outputs的第三维是先放前向的hidden_size个结果，然后再放后向的hidden_size个结果\n",
    "        # 所以outputs[:, :, :self.hidden_size]得到前向的结果\n",
    "        # outputs[:, :, self.hidden_size:]是后向的结果\n",
    "        # 注意，如果bidirectional是False，则outputs第三维的大小就是hidden_size，\n",
    "        # 这时outputs[:, : ,self.hidden_size:]是不存在的，因此也不会加上去。\n",
    "        # 对Python slicing不熟的读者可以看看下面的例子：\n",
    "        \n",
    "        # >>> a=[1,2,3]\n",
    "        # >>> a[:3]\n",
    "        # [1, 2, 3]\n",
    "        # >>> a[3:]\n",
    "        # []\n",
    "        # >>> a[:3]+a[3:]\n",
    "        # [1, 2, 3]\n",
    "        \n",
    "        # 这样就不用写下面的代码了：\n",
    "        # if bidirectional:\n",
    "        #     outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # 返回最终的输出和最后时刻的隐状态。 \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Decoder也是一个RNN，它每个时刻输出一个词。每个时刻的输入是上一个时刻的隐状态和上一个时刻的输出。一开始的隐状态是Encoder最后时刻的隐状态，输入是特殊的。然后使用RNN计算新的隐状态和输出第一个词，接着用新的隐状态和第一个词计算第二个词，...，直到遇到，结束输出。普通的RNN Decoder的问题是它只依赖与Encoder最后一个时刻的隐状态，虽然理论上这个隐状态(context向量)可以编码输入句子的语义，但是实际会比较困难。因此当输入句子很长的时候，效果会很差。\n",
    "\n",
    "为了解决这个问题，Bahdanau等人在论文里提出了注意力机制(attention mechanism)，在Decoder进行t时刻计算的时候，除了t-1时刻的隐状态，当前时刻的输入，注意力机制还可以参考Encoder所有时刻的输入。拿机器翻译来说，我们在翻译以句子的第t个词的时候会把注意力机制在某个词上。当然常见的注意力是一种soft的注意力，假设输入有5个词，注意力可能是一个概率，比如(0.6,0.1,0.1,0.1,0.1)，表示当前最关注的是输入的第一个词。同时我们之前也计算出每个时刻的输出向量，假设5个时刻分别是y1,…,y5，那么我们可以用attention概率加权得到当前时刻的context向量0.6y1+0.1y2+…+0.1y5。\n",
    "\n",
    "注意力有很多方法计算，我们这里介绍Luong等人在论文提出的方法。它是用当前时刻的GRU计算出的新的隐状态来计算注意力得分，\n",
    "\n",
    "首先它用一个score函数计算这个隐状态和Encoder的输出的相似度得分，得分越大，说明越应该注意这个词。\n",
    "\n",
    "然后再用softmax函数把score变成概率。那机器翻译为例，在t时刻，ht表示t时刻的GRU输出的新的隐状态，我们可以认为ht表示当前需要翻译的语义。通过计算ht与y1,…,yn的得分，如果ht与y1的得分很高，那么我们可以认为当前主要翻译词x1的语义。有很多中score函数的计算方法，如下图所示：\n",
    "\n",
    "![avatar](img/scores.png)\n",
    "\n",
    "上式中ht表示t时刻的隐状态，比如第一种计算score的方法，直接计算ht与hs的内积，内积越大，说明这两个向量越相似，因此注意力也更多的放到这个词上。第二种方法也类似，只是引入了一个可以学习的矩阵，我们可以认为它先对ht做一个线性变换，然后在与hs计算内积。而第三种方法把它们拼接起来然后用一个全连接网络来计算score。\n",
    "\n",
    "注意，我们前面介绍的是分别计算ht和y1的内积、ht和y2的内积，…。但是为了效率，可以一次计算ht与hs=[y1,y2,…,yn]的乘积。 计算过程如下图所示。\n",
    "![avatar](img/global_attn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong 注意力layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # 输入hidden的shape是(1, batch=64, hidden_size=500)\n",
    "        # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500)\n",
    "        # hidden * encoder_output得到的shape是(10, 64, 500)，然后对第3维求和就可以计算出score。\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), \n",
    "\t\t\t\t      encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    # 输入是上一个时刻的隐状态hidden和所有时刻的Encoder的输出encoder_outputs\n",
    "    # 输出是注意力的概率，也就是长度为input_lengths的向量，它的和加起来是1。\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # 计算注意力的score，输入hidden的shape是(1, batch=64, hidden_size=500)，\n",
    "        # 表示t时刻batch数据的隐状态\n",
    "        # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500) \n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            # 计算内积，参考dot_score函数\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        # 把attn_energies从(max_length=10, batch=64)转置成(64, 10)\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # 使用softmax函数把score变成概率，shape仍然是(64, 10)，然后用unsqueeze(1)变成\n",
    "        # (64, 1, 10) \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码实现了dot、general和concat三种score计算方法，分别和前面的三个公式对应，我们这里介绍最简单的dot方法。代码里也有一些注释，只有dot_score函数比较难以理解，我们来分析一下。首先这个函数的输入输入hidden的shape是(1, batch=64, hidden_size=500)，encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500)。\n",
    "\n",
    "怎么计算hidden和10个encoder输出向量的内积呢？为了简便，我们先假设batch是1，这样可以把第二维(batch维)去掉，因此hidden是(1, 500)，而encoder_outputs是(10, 500)。内积的定义是两个向量对应位相乘然后相加，但是encoder_outputs是10个500维的向量。当然我们可以写一个for循环来计算，但是效率很低。这里用到一个小的技巧，利用broadcasting，hidden * encoder_outputs可以理解为把hidden从(1,500)复制成(10, 500)（当然实际实现并不会这么做），然后两个(10, 500)的矩阵进行乘法。注意，这里的乘法不是矩阵乘法，而是所谓的Hadamard乘法，其实就是把对应位置的乘起来，比如下面的例子：\n",
    "\n",
    "![Hadamard乘法](img/Hadamard乘法.png)\n",
    "\n",
    "\n",
    "因此hidden * encoder_outputs就可以把hidden向量(500个数)与encoder_outputs的10个向量(500个数)对应的位置相乘。而内积还需要把这500个乘积加起来，因此后面使用torch.sum(hidden * encoder_output, dim=2)，把第2维500个乘积加起来，最终得到10个score值。当然我们实际还有一个batch维度，因此最终得到的attn_energies是(10, 64)。接着在forward函数里把attn_energies转置成(64, 10)，然后使用softmax函数把10个score变成概率，shape仍然是(64, 10)，为了后面使用方便，我们用unsqueeze(1)把它变成(64, 1, 10)。\n",
    "\n",
    "有了注意力的子模块之后，我们就可以实现Decoder了。Encoder可以一次把一个序列输入GRU，得到整个序列的输出。但是Decoder t时刻的输入是t-1时刻的输出，在t-1时刻计算完成之前是未知的，因此只能一次处理一个时刻的数据。因此Encoder的GRU的输入是(max_length, batch, hidden_size)，而Decoder的输入是(1, batch, hidden_size)。此外Decoder只能利用前面的信息，所以只能使用单向(而不是双向)的GRU，而Encoder的GRU是双向的，如果两种的hidden_size是一样的，则Decoder的隐单元个数少了一半，那怎么把Encoder的最后时刻的隐状态作为Decoder的初始隐状态呢？这里是把每个时刻双向结果加起来的，因此它们的大小就能匹配了（请读者参考前面Encoder双向相加的部分代码）。\n",
    "\n",
    "##### 计算图:\n",
    "\n",
    "    1) 把词ID输入Embedding层 \n",
    "\n",
    "    2) 使用单向的GRU继续Forward进行一个时刻的计算。 \n",
    "\n",
    "    3) 使用新的隐状态计算注意力权重 \n",
    "\n",
    "    4) 用注意力权重得到context向量 \n",
    "\n",
    "    5) context向量和GRU的输出拼接起来，然后再进过一个全连接网络，使得输出大小仍然是hidden_size \n",
    "\n",
    "    6) 使用一个投影矩阵把输出从hidden_size变成词典大小，然后用softmax变成概率 \n",
    "    \n",
    "    7) 返回输出和新的隐状态\n",
    "\n",
    "##### 输入:\n",
    "\n",
    "    input_step: shape是(1, batch_size)\n",
    "    last_hidden: 上一个时刻的隐状态， shape是(n_layers x num_directions, batch_size, hidden_size)\n",
    "    encoder_outputs: encoder的输出， shape是(max_length, batch_size, hidden_size)\n",
    "##### 输出:\n",
    "\n",
    "    output: 当前时刻输出每个词的概率，shape是(batch_size, voc.num_words)\n",
    "    hidden: 新的隐状态，shape是(n_layers x num_directions, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 保存到self里，attn_model就是前面定义的Attn类的对象。\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 定义Decoder的layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 注意：decoder每一步只能处理一个时刻的数据，因为t时刻计算完了才能计算t+1时刻。\n",
    "        # input_step的shape是(1, 64)，64是batch，1是当前输入的词ID(来自上一个时刻的输出)\n",
    "        # 通过embedding层变成(1, 64, 500)，然后进行dropout，shape不变。\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # 把embedded传入GRU进行forward计算\n",
    "        # 得到rnn_output的shape是(1, 64, 500)\n",
    "        # hidden是(2, 64, 500)，因为是双向的GRU，所以第一维是2。\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # 计算注意力权重， 根据前面的分析，attn_weights的shape是(64, 1, 10)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "        # encoder_outputs是(10, 64, 500) \n",
    "        # encoder_outputs.transpose(0, 1)后的shape是(64, 10, 500)\n",
    "        # attn_weights.bmm后是(64, 1, 500)\n",
    "        \n",
    "        # bmm是批量的矩阵乘法，第一维是batch，我们可以把attn_weights看成64个(1,10)的矩阵\n",
    "        # 把encoder_outputs.transpose(0, 1)看成64个(10, 500)的矩阵\n",
    "        # 那么bmm就是64个(1, 10)矩阵 x (10, 500)矩阵，最终得到(64, 1, 500)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # 把context向量和GRU的输出拼接起来\n",
    "        # rnn_output从(1, 64, 500)变成(64, 500)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        # context从(64, 1, 500)变成(64, 500)\n",
    "        context = context.squeeze(1)\n",
    "        # 拼接得到(64, 1000)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        # self.concat是一个矩阵(1000, 500)，\n",
    "        # self.concat(concat_input)的输出是(64, 500)\n",
    "        # 然后用tanh把输出返回变成(-1,1)，concat_output的shape是(64, 500)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # out是(500, 词典大小=7826)    \n",
    "        output = self.out(concat_output)\n",
    "        # 用softmax变成概率，表示当前时刻输出每个词的概率。\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # 返回 output和新的隐状态 \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练过程\n",
    "##### Masked损失\n",
    "\n",
    "forward实现之后，我们就需要计算loss。seq2seq有两个RNN，Encoder RNN是没有直接定义损失函数的，它是通过影响Decoder从而影响最终的输出以及loss。Decoder输出一个序列，前面我们介绍的是Decoder在预测时的过程，它的长度是不固定的，只有遇到EOS才结束。给定一个问答句对，我们可以把问题输入Encoder，然后用Decoder得到一个输出序列，但是这个输出序列和”真实”的答案长度并不相同。\n",
    "\n",
    "而且即使长度相同并且语义相似，也很难直接知道预测的答案和真实的答案是否类似。那么我们怎么计算loss呢？比如输入是”What is your name?”，训练数据中的答案是”I am LiLi”。假设模型有两种预测：”I am fine”和”My name is LiLi”。从语义上显然第二种答案更好，但是如果字面上比较的话可能第一种更好。\n",
    "\n",
    "但是让机器知道”I am LiLi”和”My name is LiLi”的语义很接近这是非常困难的，所以实际上我们通常还是通过字面上里进行比较。我们会限制Decoder的输出，使得Decoder的输出长度和”真实”答案一样，然后逐个时刻比较。Decoder输出的是每个词的概率分布，因此可以使用交叉熵损失函数。但是这里还有一个问题，因为是一个batch的数据里有一些是padding的，因此这些位置的预测是没有必要计算loss的，因此我们需要使用前面的mask矩阵把对应位置的loss去掉，我们可以通过下面的函数来实现计算Masked的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    # 计算实际的词的个数，因为padding是0，非padding是1，因此sum就可以得到词的个数\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一次迭代的训练过程\n",
    "函数train实现一个batch数据的训练。前面我们提到过，在训练的时候我们会限制Decoder的输出，使得Decoder的输出长度和”真实”答案一样长。但是我们在训练的时候如果让Decoder自行输出，那么收敛可能会比较慢，因为Decoder在t时刻的输入来自t-1时刻的输出。如果前面预测错了，那么后面很可能都会错下去。另外一种方法叫做teacher forcing，它不管模型在t-1时刻做什么预测都把t-1时刻的正确答案作为t时刻的输入。但是如果只用teacher forcing也有问题，因为在真实的Decoder的是是没有老师来帮它纠正错误的。所以比较好的方法是更加一个teacher_forcing_ratio参数随机的来确定本次训练是否teacher forcing。\n",
    "\n",
    "另外使用到的一个技巧是梯度裁剪(gradient clipping) 。这个技巧通常是为了防止梯度爆炸(exploding gradient)，它把参数限制在一个范围之内，从而可以避免梯度的梯度过大或者出现NaN等问题。注意：虽然它的名字叫梯度裁剪，但实际它是对模型的参数进行裁剪，它把整个参数看成一个向量，如果这个向量的模大于max_norm，那么就把这个向量除以一个值使得模等于max_norm，因此也等价于把这个向量投影到半径为max_norm的球上。它的效果如下图所示。\n",
    "\n",
    "![grad_clip](img/grad_clip.png)\n",
    "\n",
    "#### 操作步骤:\n",
    "\n",
    "    1) 把整个batch的输入传入encoder \n",
    "\n",
    "    2) 把decoder的输入设置为特殊的，初始隐状态设置为encoder最后时刻的隐状态 \n",
    "\n",
    "    3) decoder每次处理一个时刻的forward计算 \n",
    "\n",
    "    4) 如果是teacher forcing，把上个时刻的\"正确的\"词作为当前输入，否则用上一个时刻的输出作为当前时刻的输入 \n",
    "\n",
    "    5) 计算loss \n",
    "\n",
    "    6) 反向计算梯度 \n",
    "\n",
    "    7) 对梯度进行裁剪 \n",
    "\n",
    "    8) 更新模型(包括encoder和decoder)参数\n",
    "\n",
    "注意，PyTorch的RNN模块(RNN, LSTM, GRU)也可以当成普通的非循环的网络来使用。在Encoder部分，我们是直接把所有时刻的数据都传入RNN，让它一次计算出所有的结果，但是在Decoder的时候(非teacher forcing)后一个时刻的输入来自前一个时刻的输出，因此无法一次计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # 梯度清空\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # 设置device，从而支持GPU，当然如果没有GPU也能工作。\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # 初始化变量\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # encoder的Forward计算\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Decoder的初始输入是SOS，我们需要构造(1, batch)的输入，表示第一个时刻batch个输入。\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # 注意：Encoder是双向的，而Decoder是单向的，因此从下往上取n_layers个\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # 确定是否teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # 一次处理一个时刻 \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: 下一个时刻的输入是当前正确答案\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # 计算累计的loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # 不是teacher forcing: 下一个时刻的输入是当前模型预测概率最高的值\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # 计算累计的loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # 反向计算 \n",
    "    loss.backward()\n",
    "\n",
    "    # 对encoder和decoder进行梯度裁剪\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # 更新参数\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练迭代过程\n",
    "\n",
    "最后是把前面的代码组合起来进行训练。函数trainIters用于进行n_iterations次minibatch的训练。\n",
    "\n",
    "值得注意的是我们定期会保存模型，我们会保存一个tar包，包括encoder和decoder的state_dicts(参数),优化器(optimizers)的state_dicts, loss和迭代次数。这样保存模型的好处是从中恢复后我们既可以进行预测也可以进行训练(因为有优化器的参数和迭代的次数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "              embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n",
    "              print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 随机选择n_iteration个batch的数据(pair)\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # 初始化\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # 训练\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # 训练一个batch的数据\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # 进度\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\"\n",
    "\t\t\t.format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # 保存checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'\n",
    "\t\t.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果测试\n",
    "\n",
    "模型训练完成之后，我们需要测试它的效果。最简单直接的方法就是和chatbot来聊天。因此我们需要用Decoder来生成一个响应。\n",
    "\n",
    "## 贪心解码(Greedy decoding)算法\n",
    "\n",
    "最简单的解码算法是贪心算法，也就是每次都选择概率最高的那个词，然后把这个词作为下一个时刻的输入，直到遇到EOS结束解码或者达到一个最大长度。但是贪心算法不一定能得到最优解，因为某个答案可能开始的几个词的概率并不太高，但是后来概率会很大。因此除了贪心算法，我们通常也可以使用Beam-Search算法，也就是每个时刻保留概率最高的Top K个结果，然后下一个时刻尝试把这K个结果输入(当然需要能恢复RNN的状态)，然后再从中选择概率最高的K个。\n",
    "\n",
    "为了实现贪心解码算法，我们定义一个GreedySearchDecoder类。这个类的forwar的方法需要传入一个输入序列(input_seq)，其shape是(input_seq length, 1)， 输入长度input_length和最大输出长度max_length。就是过程如下：\n",
    "\n",
    "1) 把输入传给Encoder，得到所有时刻的输出和最后一个时刻的隐状态。 \n",
    "\n",
    "2) 把Encoder最后时刻的隐状态作为Decoder的初始状态。 \n",
    "\n",
    "3) Decoder的第一输入初始化为SOS。 \n",
    "\n",
    "4) 定义保存解码结果的tensor\n",
    "\n",
    "5) 循环直到最大解码长度 \n",
    "\n",
    "    a) 把当前输入传入Decoder \n",
    "    \n",
    "    b) 得到概率最大的词以及概率 \n",
    "    \n",
    "    c) 把这个词和概率保存下来 \n",
    "    \n",
    "    d) 把当前输出的词作为下一个时刻的输入 \n",
    "    \n",
    "6) 返回所有的词和概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Encoder的Forward计算 \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # 把Encoder最后时刻的隐状态作为Decoder的初始值\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # 因为我们的函数都是要求(time,batch)，因此即使只有一个数据，也要做出二维的。\n",
    "        # Decoder的初始输入是SOS\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # 用于保存解码结果的tensor\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # 循环，这里只使用长度限制，后面处理的时候把EOS去掉了。\n",
    "        for _ in range(max_length):\n",
    "            # Decoder forward一步\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, \n",
    "\t\t\t\t\t\t\t\tencoder_outputs)\n",
    "            # decoder_outputs是(batch=1, vob_size)\n",
    "            # 使用max返回概率最大的词和得分\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # 把解码结果保存到all_tokens和all_scores里\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # decoder_input是当前时刻输出的词的ID，这是个一维的向量，因为max会减少一维。\n",
    "            # 但是decoder要求有一个batch维度，因此用unsqueeze增加batch维度。\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # 返回所有的词和得分。\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试对话函数\n",
    "解码方法完成后，我们写一个函数来测试从终端输入一个句子然后来看看chatbot的回复。我们需要用前面的函数来把句子分词，然后变成ID传入解码器，得到输出的ID后再转换成文字。我们会实现一个evaluate函数，由它来完成这些工作。我们需要把一个句子变成输入需要的格式——shape为(batch, max_length)，即使只有一个输入也需要增加一个batch维度。我们首先把句子分词，然后变成ID的序列，然后转置成合适的格式。此外我们还需要创建一个名为lengths的tensor，虽然只有一个，来表示输入的实际长度。接着我们构造类GreedySearchDecoder的实例searcher，然后用searcher来进行解码得到输出的ID，最后我们把这些ID变成词并且去掉EOS之后的内容。\n",
    "\n",
    "另外一个evaluateInput函数作为chatbot的用户接口，当运行它的时候，它会首先提示用户输入一个句子，然后使用evaluate来生成回复。然后继续对话直到用户输入”q”或者”quit”。如果用户输入的词不在词典里，我们会输出错误信息(当然还有一种办法是忽略这些词)然后提示用户重新输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### 把输入的一个batch句子变成id\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # 创建lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # 转置 \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # 放到合适的设备上(比如GPU)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # 用searcher解码\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # ID变成词。\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        #try:\n",
    "        # 得到用户终端的输入\n",
    "        input_sentence = input('> ')\n",
    "        # 是否退出\n",
    "        if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "        # 句子分词\n",
    "        input_sentence = jieba_cut_word(input_sentence,f_stop_seg_list)\n",
    "        print(\"input_sentence:{0}\".format(input_sentence))\n",
    "        # 生成响应Evaluate sentence\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        # 去掉EOS后面的内容\n",
    "        words = []\n",
    "        for word in output_words:\n",
    "            if word == 'EOS':\n",
    "                break\n",
    "            elif word != 'PAD':\n",
    "                words.append(word)\n",
    "        print('Bot:', ' '.join(words))\n",
    "\n",
    "#         except KeyError:\n",
    "#             print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试模型\n",
    "最后我们可以来训练模型和进行评测了。\n",
    "\n",
    "不论是我们像训练模型还是测试对话，我们都需要初始化encoder和decoder模型参数。在下面的代码，我们从头开始训练模型或者从某个checkpoint加载模型。读者可以尝试不同的超参数配置来进行调优。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# 配置模型\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# 从哪个checkpoint恢复，如果是None，那么从头开始训练。\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "  \n",
    "\n",
    "# 如果loadFilename不空，则从中加载模型 \n",
    "if loadFilename:\n",
    "    # 如果训练和加载是一条机器，那么直接加载 \n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # 否则比如checkpoint是在GPU上得到的，但是我们现在又用CPU来训练或者测试，那么注释掉下面的代码\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# 初始化word embedding\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# 初始化encoder和decoder模型\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, \n",
    "\t\t\t\tdecoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# 使用合适的设备\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "下面的代码进行训练，我们需要设置一些训练的超参数。初始化优化器，最后调用函数trainIters进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 50; Percent complete: 5.0%; Average loss: 6.0840\n",
      "Iteration: 100; Percent complete: 10.0%; Average loss: 4.6358\n"
     ]
    }
   ],
   "source": [
    "# 配置训练的超参数和优化器 \n",
    "import os\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"baidu_qa\"\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 1000\n",
    "print_every = 50\n",
    "save_every = 500\n",
    "\n",
    "# 设置进入训练模式，从而开启dropout \n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# 初始化优化器 \n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# 开始训练\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 请问 乙肝 不能 买 投康惠保 会 被 拒绝\n",
      "input_sentence:请问 乙肝 不能 买 投康惠保 会 被 拒绝\n",
      "Bot: 会 买 不了\n"
     ]
    }
   ],
   "source": [
    "# 进入eval模式，从而去掉dropout。 \n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 构造searcher对象 \n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# 测试\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
